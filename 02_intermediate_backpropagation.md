# 역전파와 경사하강법 (중급 단계)

## 목차
1. [역전파 알고리즘 (Backpropagation)](#1-역전파-알고리즘-backpropagation)
2. [경사하강법 (Gradient Descent)](#2-경사하강법-gradient-descent)
3. [연쇄 법칙 (Chain Rule)](#3-연쇄-법칙-chain-rule)
4. [가중치 업데이트 (Weight Update)](#4-가중치-업데이트-weight-update)

---

## 1. 역전파 알고리즘 (Backpropagation)

역전파는 신경망을 학습시키기 위해 손실 함수의 그래디언트를 효율적으로 계산하는 알고리즘입니다.

### 1.1 기본 개념

#### 목표
손실 함수 `L`에 대한 각 파라미터의 편미분을 계산:
```
∂L/∂W^[l], ∂L/∂b^[l]
```

#### 기호 설명
- `L`: 손실 함수 (loss function)
- `W^[l]`: l번째 층의 가중치 행렬
- `b^[l]`: l번째 층의 편향 벡터
- `∂`: 편미분 기호 (partial derivative)

### 1.2 역전파 공식

**출력층 (L층)의 그래디언트:**
```
δ^[L] = ∂L/∂z^[L] = ∂L/∂a^[L] ⊙ f'(z^[L])
```

**은닉층 (l층)의 그래디언트:**
```
δ^[l] = (W^[l+1])^T δ^[l+1] ⊙ f'(z^[l])
```

**가중치와 편향의 그래디언트:**
```
∂L/∂W^[l] = δ^[l] (a^[l-1])^T
∂L/∂b^[l] = δ^[l]
```

#### 기호 설명
- `δ^[l]`: l번째 층의 오차 신호 (error signal)
- `f'(·)`: 활성화 함수의 미분
- `⊙`: 원소별 곱셈 (element-wise multiplication, Hadamard product)
- `^T`: 전치 (transpose)

### 1.3 수치 예제: 완전한 역전파

앞의 순방향 전파 예제를 계속 사용합니다.

**네트워크 구조:**
- 입력: `a^[0] = [1.0, 2.0]^T`
- 은닉층: 2개 뉴런 (ReLU)
- 출력층: 1개 뉴런 (Sigmoid)

**순방향 전파 결과 (복습):**
```
z^[1] = [1.2, 1.2]^T
a^[1] = [1.2, 1.2]^T
z^[2] = 1.71
a^[2] = 0.8467 (예측값 ŷ)
```

**실제 값:**
```
y = 1.0
```

**손실 함수:** 이진 교차 엔트로피
```
L = -[y log(ŷ) + (1-y) log(1-ŷ)]
  = -[1 × log(0.8467) + 0 × log(0.1533)]
  ≈ 0.1665
```

---

**역전파 계산:**

**Step 1: 출력층 그래디언트**

Sigmoid + Cross-Entropy의 경우:
```
δ^[2] = ∂L/∂z^[2] = a^[2] - y = 0.8467 - 1.0 = -0.1533
```

**Step 2: 출력층 파라미터 그래디언트**

```
∂L/∂W^[2] = δ^[2] (a^[1])^T
           = [-0.1533] × [1.2, 1.2]
           = [-0.1840, -0.1840]

∂L/∂b^[2] = δ^[2] = -0.1533
```

**Step 3: 은닉층 그래디언트**

먼저 그래디언트를 역전파:
```
(W^[2])^T δ^[2] = [0.6, 0.7]^T × (-0.1533)
                 = [-0.0920, -0.1073]^T
```

ReLU 미분 (z^[1] = [1.2, 1.2], 모두 > 0이므로 미분은 1):
```
f'(z^[1]) = [1, 1]^T
```

은닉층 오차:
```
δ^[1] = [-0.0920, -0.1073]^T ⊙ [1, 1]^T
      = [-0.0920, -0.1073]^T
```

**Step 4: 은닉층 파라미터 그래디언트**

```
∂L/∂W^[1] = δ^[1] (a^[0])^T
           = [[-0.0920], [-0.1073]] × [1.0, 2.0]
           = [[-0.0920, -0.1840],
              [-0.1073, -0.2146]]

∂L/∂b^[1] = δ^[1] = [-0.0920, -0.1073]^T
```

---

## 2. 경사하강법 (Gradient Descent)

### 2.1 배치 경사하강법 (Batch Gradient Descent)

#### 수식
```
θ := θ - α ∇J(θ)
```

가중치와 편향에 대해:
```
W^[l] := W^[l] - α ∂L/∂W^[l]
b^[l] := b^[l] - α ∂L/∂b^[l]
```

#### 기호 설명
- `θ`: 모든 파라미터 (all parameters)
- `α`: 학습률 (learning rate)
- `∇J(θ)`: 손실 함수의 그래디언트 벡터 (gradient vector)
- `:=`: 할당 연산자 (assignment operator)

#### 수치 예제

학습률 `α = 0.1`로 위의 예제에서 파라미터 업데이트:

**출력층 업데이트:**
```
W^[2] := [0.6, 0.7] - 0.1 × [-0.1840, -0.1840]
       = [0.6, 0.7] + [0.0184, 0.0184]
       = [0.6184, 0.7184]

b^[2] := 0.15 - 0.1 × (-0.1533)
       = 0.15 + 0.0153
       = 0.1653
```

**은닉층 업데이트:**
```
W^[1] := [[0.5, 0.3],   - 0.1 × [[-0.0920, -0.1840],
          [0.2, 0.4]]              [-0.1073, -0.2146]]
        
       = [[0.5, 0.3],   + [[0.0092, 0.0184],
          [0.2, 0.4]]      [0.0107, 0.0215]]
        
       = [[0.5092, 0.3184],
          [0.2107, 0.4215]]

b^[1] := [0.1, 0.2]^T - 0.1 × [-0.0920, -0.1073]^T
       = [0.1, 0.2]^T + [0.0092, 0.0107]^T
       = [0.1092, 0.2107]^T
```

### 2.2 확률적 경사하강법 (Stochastic Gradient Descent, SGD)

#### 수식
각 샘플 i에 대해:
```
θ := θ - α ∇L^(i)(θ)
```

#### 기호 설명
- `L^(i)`: i번째 샘플의 손실
- 배치 경사하강법과 달리 한 번에 하나의 샘플만 사용

#### 특징
- **장점**: 빠르고 메모리 효율적
- **단점**: 손실 함수가 진동하며 수렴

### 2.3 미니배치 경사하강법 (Mini-batch Gradient Descent)

#### 수식
배치 크기 B에 대해:
```
θ := θ - (α/B) Σᵢ₌₁ᴮ ∇L^(i)(θ)
```

#### 기호 설명
- `B`: 배치 크기 (batch size)
- 배치 내 모든 샘플의 그래디언트 평균을 사용

#### 수치 예제

배치 크기 B = 3, 학습률 α = 0.1

3개 샘플의 그래디언트:
```
∂L^(1)/∂W = [0.2, 0.3]
∂L^(2)/∂W = [0.1, 0.4]
∂L^(3)/∂W = [0.3, 0.2]
```

평균 그래디언트:
```
∂L/∂W = (1/3) × ([0.2, 0.3] + [0.1, 0.4] + [0.3, 0.2])
       = (1/3) × [0.6, 0.9]
       = [0.2, 0.3]
```

가중치 업데이트 (초기값 W = [1.0, 1.0]):
```
W := [1.0, 1.0] - 0.1 × [0.2, 0.3]
   = [1.0, 1.0] - [0.02, 0.03]
   = [0.98, 0.97]
```

---

## 3. 연쇄 법칙 (Chain Rule)

### 3.1 기본 연쇄 법칙

#### 수식
함수 합성 `y = f(g(x))`에 대해:
```
dy/dx = (dy/dg) × (dg/dx)
```

#### 기호 설명
- `y`: 최종 출력
- `g`: 중간 함수
- `x`: 입력
- 미분의 곱으로 계산

### 3.2 다변수 연쇄 법칙

#### 수식
```
∂L/∂x = Σⱼ (∂L/∂zⱼ) × (∂zⱼ/∂x)
```

#### 기호 설명
- `L`: 최종 손실
- `z`: 중간 변수들
- `x`: 관심 있는 변수
- 모든 경로를 통한 미분의 합

### 3.3 신경망에서의 연쇄 법칙

#### 수식
l번째 층의 가중치에 대한 손실의 미분:
```
∂L/∂W^[l] = (∂L/∂z^[l]) × (∂z^[l]/∂W^[l])
           = δ^[l] × (a^[l-1])^T
```

#### 수치 예제

2층 신경망에서 `W₁₁^[1]` (첫 번째 층, 첫 번째 행, 첫 번째 열)의 그래디언트 계산:

**전방향 계산:**
```
z₁^[1] = W₁₁^[1] × a₁^[0] + W₁₂^[1] × a₂^[0] + b₁^[1]
       = W₁₁^[1] × 1.0 + 0.3 × 2.0 + 0.1
```

**편미분:**
```
∂z₁^[1]/∂W₁₁^[1] = a₁^[0] = 1.0
```

**연쇄 법칙 적용:**
```
∂L/∂W₁₁^[1] = (∂L/∂z₁^[1]) × (∂z₁^[1]/∂W₁₁^[1])
             = δ₁^[1] × a₁^[0]
             = -0.0920 × 1.0
             = -0.0920
```

이는 앞서 계산한 행렬 형태의 결과와 일치합니다.

---

## 4. 가중치 업데이트 (Weight Update)

### 4.1 학습률의 중요성

#### 수식
```
W := W - α ∂L/∂W
```

#### 기호 설명
- `α`: 학습률 (learning rate)
  - 너무 크면: 발산 (divergence)
  - 너무 작으면: 느린 수렴 (slow convergence)

#### 수치 예제

초기 가중치 `W = 1.0`, 그래디언트 `∂L/∂W = 0.5`

**α = 0.01 (작은 학습률):**
```
반복 1: W = 1.0 - 0.01 × 0.5 = 0.995
반복 2: W = 0.995 - 0.01 × 0.5 = 0.990
반복 3: W = 0.990 - 0.01 × 0.5 = 0.985
```
느리지만 안정적

**α = 0.5 (적절한 학습률):**
```
반복 1: W = 1.0 - 0.5 × 0.5 = 0.75
반복 2: W = 0.75 - 0.5 × 0.5 = 0.50
반복 3: W = 0.50 - 0.5 × 0.5 = 0.25
```
빠르고 안정적

**α = 2.5 (큰 학습률):**
```
반복 1: W = 1.0 - 2.5 × 0.5 = -0.25
반복 2: W = -0.25 - 2.5 × 0.5 = -1.50
반복 3: W = -1.50 - 2.5 × 0.5 = -2.75
```
발산 가능성

### 4.2 모멘텀 (Momentum)

#### 수식
```
v := β v + (1 - β) ∂L/∂W
W := W - α v
```

#### 기호 설명
- `v`: 속도 (velocity), 이동 평균 그래디언트
- `β`: 모멘텀 계수 (일반적으로 0.9)
- 이전 그래디언트의 방향을 유지하여 진동 감소

#### 수치 예제

초기값: `W = 1.0`, `v = 0`, `α = 0.1`, `β = 0.9`

그래디언트 시퀀스: `[0.5, 0.3, 0.4]`

**반복 1:**
```
v = 0.9 × 0 + 0.1 × 0.5 = 0.05
W = 1.0 - 0.1 × 0.05 = 0.995
```

**반복 2:**
```
v = 0.9 × 0.05 + 0.1 × 0.3 = 0.045 + 0.03 = 0.075
W = 0.995 - 0.1 × 0.075 = 0.9875
```

**반복 3:**
```
v = 0.9 × 0.075 + 0.1 × 0.4 = 0.0675 + 0.04 = 0.1075
W = 0.9875 - 0.1 × 0.1075 = 0.9768
```

### 4.3 완전한 학습 예제

**문제 설정:**
- 단순 선형 회귀: `y = wx + b`
- 데이터: `(x=2, y=5)`, `(x=3, y=7)`
- 초기값: `w=0`, `b=0`
- 학습률: `α=0.1`
- 손실: MSE

**참고:** 실제 관계는 `y = 2x + 1` (w=2, b=1을 학습해야 함)

**반복 1:**

순방향:
```
샘플 1: ŷ = 0 × 2 + 0 = 0, 오차 = 5 - 0 = 5
샘플 2: ŷ = 0 × 3 + 0 = 0, 오차 = 7 - 0 = 7
```

손실:
```
L = (1/2) × [(5)² + (7)²] / 2 = (1/2) × [25 + 49] / 2 = 18.5
```

그래디언트:
```
∂L/∂w = (1/2) × [(-5 × 2) + (-7 × 3)] = (1/2) × [-10 - 21] = -15.5
∂L/∂b = (1/2) × [(-5) + (-7)] = (1/2) × [-12] = -6
```

업데이트:
```
w := 0 - 0.1 × (-15.5) = 1.55
b := 0 - 0.1 × (-6) = 0.6
```

**반복 2:**

순방향:
```
샘플 1: ŷ = 1.55 × 2 + 0.6 = 3.7, 오차 = 5 - 3.7 = 1.3
샘플 2: ŷ = 1.55 × 3 + 0.6 = 5.25, 오차 = 7 - 5.25 = 1.75
```

손실:
```
L = (1/2) × [(1.3)² + (1.75)²] / 2 = (1/2) × [1.69 + 3.0625] / 2 ≈ 1.19
```

그래디언트:
```
∂L/∂w = (1/2) × [(-1.3 × 2) + (-1.75 × 3)] = (1/2) × [-2.6 - 5.25] = -3.925
∂L/∂b = (1/2) × [(-1.3) + (-1.75)] = (1/2) × [-3.05] = -1.525
```

업데이트:
```
w := 1.55 - 0.1 × (-3.925) = 1.9425
b := 0.6 - 0.1 × (-1.525) = 0.7525
```

**진행 상황:**
- 목표: w=2, b=1
- 반복 2 후: w=1.9425, b=0.7525
- 점차 목표에 수렴 중!

---

## 요약

### 핵심 개념
1. **역전파**: 연쇄 법칙을 사용하여 그래디언트를 효율적으로 계산
2. **경사하강법**: 그래디언트의 반대 방향으로 파라미터 업데이트
3. **연쇄 법칙**: 복잡한 함수의 미분을 간단한 미분의 곱으로 분해
4. **학습률**: 업데이트 크기를 조절하는 중요한 하이퍼파라미터

### 수식 체크리스트
- ✓ 역전파: `δ^[l] = (W^[l+1])^T δ^[l+1] ⊙ f'(z^[l])`
- ✓ 가중치 그래디언트: `∂L/∂W^[l] = δ^[l] (a^[l-1])^T`
- ✓ 경사하강법: `W := W - α ∂L/∂W`
- ✓ 모멘텀: `v := βv + (1-β)∂L/∂W`, `W := W - αv`

### 다음 단계
고급 단계에서는 **Adam, RMSprop** 등의 고급 최적화 알고리즘과 **정규화 기법**을 학습합니다.

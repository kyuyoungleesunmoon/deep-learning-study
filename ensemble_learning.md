# ë‹¤ì–‘í•œ ëª¨ë¸ì„ ê²°í•©í•œ ì•™ìƒë¸” í•™ìŠµ (Ensemble Learning)

> ğŸ“ **ê°•ì‚¬**: ë°ì´í„°ì‚¬ì´ì–¸ìŠ¤ ë° ì¸ê³µì§€ëŠ¥ ì „ë¬¸ê°€, í¼ì‹¤ë¦¬í…Œì´ì…˜ ê¸°ë°˜ êµìœ¡ ì „ë¬¸ê°€  
> ğŸ¯ **ëª©í‘œ**: ì•™ìƒë¸” í•™ìŠµì˜ í•µì‹¬ ê°œë…ê³¼ ì•Œê³ ë¦¬ì¦˜ì„ ì´ë¡  + ì‹œê°í™” + ì‹¤ìŠµìœ¼ë¡œ ì™„ë²½ ì´í•´  
> ğŸ•’ **ì˜ˆìƒ í•™ìŠµ ì‹œê°„**: 4ì‹œê°„ 30ë¶„ (ì´ë¡  2ì‹œê°„ + ì‹¤ìŠµ 2ì‹œê°„ 30ë¶„)

---

## ëª©ì°¨
1. [ì•™ìƒë¸” í•™ìŠµ ê°œìš”](#1ï¸âƒ£-ì•™ìƒë¸”-í•™ìŠµ-ê°œìš”)
2. [ë‹¤ìˆ˜ê²° íˆ¬í‘œ ì•™ìƒë¸” (Voting)](#2ï¸âƒ£-ë‹¤ìˆ˜ê²°-íˆ¬í‘œ-ì•™ìƒë¸”-voting)
3. [ë°°ê¹… (Bagging)](#3ï¸âƒ£-ë°°ê¹…-bagging)
4. [ì—ì´ë‹¤ë¶€ìŠ¤íŠ¸ (AdaBoost)](#4ï¸âƒ£-ì—ì´ë‹¤ë¶€ìŠ¤íŠ¸-adaboost)
5. [ê·¸ë ˆì´ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ… & XGBoost](#5ï¸âƒ£-ê·¸ë ˆì´ë””ì–¸íŠ¸-ë¶€ìŠ¤íŒ…--xgboost)
6. [ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ë° ë¹„êµ](#6ï¸âƒ£-ëª¨ë¸-ì„±ëŠ¥-í‰ê°€-ë°-ë¹„êµ)

---

## 1ï¸âƒ£ ì•™ìƒë¸” í•™ìŠµ ê°œìš”

### ğŸ“– ì´ë¡  ì„¤ëª…

#### 1.1 ë‹¨ì¼ ëª¨ë¸ì˜ í•œê³„

ë‹¨ì¼ ëª¨ë¸ì€ ë‹¤ìŒê³¼ ê°™ì€ ë¬¸ì œë¥¼ ê²ªì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤:

- **ê³¼ì í•©(Overfitting)**: í•™ìŠµ ë°ì´í„°ì— ë„ˆë¬´ ë§ì¶°ì ¸ ìƒˆë¡œìš´ ë°ì´í„°ì— ëŒ€í•œ ì„±ëŠ¥ì´ ë–¨ì–´ì§
- **ê³¼ì†Œì í•©(Underfitting)**: ëª¨ë¸ì´ ë„ˆë¬´ ë‹¨ìˆœí•˜ì—¬ ë°ì´í„°ì˜ íŒ¨í„´ì„ ì œëŒ€ë¡œ í•™ìŠµí•˜ì§€ ëª»í•¨
- **ë†’ì€ ë¶„ì‚°(High Variance)**: í•™ìŠµ ë°ì´í„°ê°€ ì¡°ê¸ˆë§Œ ë°”ë€Œì–´ë„ ëª¨ë¸ì´ í¬ê²Œ ë‹¬ë¼ì§
- **ë†’ì€ í¸í–¥(High Bias)**: ëª¨ë¸ì´ ë°ì´í„°ì˜ ì§„ì§œ íŒ¨í„´ì„ í¬ì°©í•˜ì§€ ëª»í•¨

#### 1.2 ì§‘ë‹¨ì§€ì„±(Ensemble)ì˜ ê°œë…

**ì•™ìƒë¸” í•™ìŠµ**ì€ ì—¬ëŸ¬ ê°œì˜ ëª¨ë¸ì„ ê²°í•©í•˜ì—¬ ë‹¨ì¼ ëª¨ë¸ë³´ë‹¤ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ì–»ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.

**í•µì‹¬ ì•„ì´ë””ì–´**: "ì„¸ ì‚¬ëŒì´ ëª¨ì´ë©´ ë¬¸ìˆ˜ì˜ ì§€í˜œë³´ë‹¤ ë‚«ë‹¤"
- ì—¬ëŸ¬ ì „ë¬¸ê°€ì˜ ì˜ê²¬ì„ ëª¨ìœ¼ë©´ í•œ ì‚¬ëŒì˜ ì˜ê²¬ë³´ë‹¤ ì •í™•í•  ê°€ëŠ¥ì„±ì´ ë†’ìŒ
- ê° ëª¨ë¸ì˜ ì‹¤ìˆ˜ê°€ ì„œë¡œ ë‹¤ë¥¸ ë°©í–¥ì´ë¼ë©´, í‰ê· ì„ ë‚´ë©´ ì˜¤ë¥˜ê°€ ì¤„ì–´ë“¦

#### 1.3 ì•™ìƒë¸” í•™ìŠµì˜ ì„¸ ê°€ì§€ ì£¼ìš” ë°©ì‹

| ë°©ì‹ | ì„¤ëª… | ëŒ€í‘œ ì•Œê³ ë¦¬ì¦˜ | ì£¼ìš” ëª©ì  |
|------|------|--------------|----------|
| **ë°°ê¹…(Bagging)** | ê°™ì€ ì•Œê³ ë¦¬ì¦˜, ë‹¤ë¥¸ ë°ì´í„°ì…‹(ë¶€íŠ¸ìŠ¤íŠ¸ë©) | Random Forest | **ë¶„ì‚° ê°ì†Œ** |
| **ë¶€ìŠ¤íŒ…(Boosting)** | ìˆœì°¨ì ìœ¼ë¡œ ì•½í•œ í•™ìŠµê¸°ë¥¼ ê°•í™” | AdaBoost, XGBoost | **í¸í–¥ ê°ì†Œ** |
| **ìŠ¤íƒœí‚¹(Stacking)** | ë‹¤ë¥¸ ì•Œê³ ë¦¬ì¦˜ì˜ ì˜ˆì¸¡ì„ ë©”íƒ€ ëª¨ë¸ë¡œ í•™ìŠµ | Stacked Generalization | **ì¼ë°˜í™” ì„±ëŠ¥** |

#### 1.4 í¸í–¥-ë¶„ì‚° íŠ¸ë ˆì´ë“œì˜¤í”„

ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì˜ ì˜¤ì°¨ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ë¶„í•´ë©ë‹ˆë‹¤:

$$\text{Error} = \text{Bias}^2 + \text{Variance} + \text{Irreducible Error}$$

**í¸í–¥(Bias)**:
- ëª¨ë¸ì´ ì‹¤ì œ ê´€ê³„ë¥¼ ì–¼ë§ˆë‚˜ ì˜ ê·¼ì‚¬í•˜ëŠ”ê°€
- ë†’ì€ í¸í–¥ = ê³¼ì†Œì í•©
- ì˜ˆ: ì„ í˜• ëª¨ë¸ë¡œ ë¹„ì„ í˜• ë°ì´í„° í•™ìŠµ

**ë¶„ì‚°(Variance)**:
- í•™ìŠµ ë°ì´í„°ê°€ ë°”ë€” ë•Œ ëª¨ë¸ì´ ì–¼ë§ˆë‚˜ ë³€í•˜ëŠ”ê°€
- ë†’ì€ ë¶„ì‚° = ê³¼ì í•©
- ì˜ˆ: ê¹Šì€ ê²°ì • íŠ¸ë¦¬

**ì•™ìƒë¸”ì˜ ì—­í• **:
- **ë°°ê¹…**: ì—¬ëŸ¬ ëª¨ë¸ì˜ í‰ê· ìœ¼ë¡œ **ë¶„ì‚° ê°ì†Œ**
- **ë¶€ìŠ¤íŒ…**: ì˜ëª» ë¶„ë¥˜ëœ ìƒ˜í”Œì— ì§‘ì¤‘í•˜ì—¬ **í¸í–¥ ê°ì†Œ**

### ğŸ”¢ ìˆ˜ì‹

#### í‰ê·  ì•™ìƒë¸”ì˜ ë¶„ì‚° ê°ì†Œ

Nê°œì˜ ë…ë¦½ì ì¸ ëª¨ë¸ì´ ìˆê³ , ê° ëª¨ë¸ì˜ ë¶„ì‚°ì´ $\sigma^2$ì¼ ë•Œ:

$$\text{Var}(\text{Ensemble}) = \frac{\sigma^2}{N}$$

**í•´ì„**: ëª¨ë¸ ìˆ˜ê°€ ì¦ê°€í•˜ë©´ ì•™ìƒë¸”ì˜ ë¶„ì‚°ì´ ê°ì†Œí•©ë‹ˆë‹¤!

#### ì‹¤ì œ ìƒí™© (ëª¨ë¸ì´ ì™„ì „íˆ ë…ë¦½ì ì´ì§€ ì•Šì„ ë•Œ)

ìƒê´€ê³„ìˆ˜ê°€ $\rho$ì¼ ë•Œ:

$$\text{Var}(\text{Ensemble}) = \rho\sigma^2 + \frac{1-\rho}{N}\sigma^2$$

**í†µì°°**:
- $\rho = 0$ (ì™„ì „ ë…ë¦½): ë¶„ì‚°ì´ $1/N$ë¡œ ê°ì†Œ
- $\rho = 1$ (ì™„ì „ ìƒê´€): ë¶„ì‚°ì´ ì „í˜€ ê°ì†Œí•˜ì§€ ì•ŠìŒ
- **ë”°ë¼ì„œ ë‹¤ì–‘ì„±(diversity)ì´ ì¤‘ìš”!**

### ğŸ’» ì‹œê°í™” ì½”ë“œ

```python
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import rc
from sklearn.datasets import make_moons
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC

# í•œê¸€ í°íŠ¸ ì„¤ì •
rc('font', family='DejaVu Sans')
plt.rcParams['axes.unicode_minus'] = False

# ë°ì´í„° ìƒì„±
np.random.seed(42)
X, y = make_moons(n_samples=300, noise=0.3, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ë‹¨ì¼ ëª¨ë¸
single_model = DecisionTreeClassifier(max_depth=3, random_state=42)
single_model.fit(X_train, y_train)

# ì•™ìƒë¸” ëª¨ë¸
lr = LogisticRegression(random_state=42)
svm = SVC(kernel='rbf', random_state=42, probability=True)
dt = DecisionTreeClassifier(max_depth=3, random_state=42)
ensemble = VotingClassifier(estimators=[('lr', lr), ('svm', svm), ('dt', dt)], voting='soft')
ensemble.fit(X_train, y_train)

# ê²°ì • ê²½ê³„ ì‹œê°í™” í•¨ìˆ˜
def plot_decision_boundary(model, X, y, title):
    h = 0.02
    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5
    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))
    
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    
    plt.contourf(xx, yy, Z, alpha=0.4, cmap=plt.cm.RdYlBu)
    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdYlBu, edgecolor='black', s=50)
    plt.title(title, fontsize=14, weight='bold')
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')

# ë¹„êµ ì‹œê°í™”
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

plt.sca(axes[0])
plot_decision_boundary(single_model, X_train, y_train, 'Single Model (Decision Tree)')
plt.text(0.5, -1.3, f'Test Accuracy: {single_model.score(X_test, y_test):.3f}', 
         ha='center', fontsize=12, weight='bold')

plt.sca(axes[1])
plot_decision_boundary(ensemble, X_train, y_train, 'Ensemble Model (Voting)')
plt.text(0.5, -1.3, f'Test Accuracy: {ensemble.score(X_test, y_test):.3f}', 
         ha='center', fontsize=12, weight='bold')

plt.tight_layout()
plt.savefig('ensemble_vs_single.png', dpi=150, bbox_inches='tight')
plt.show()

print("=" * 60)
print("Single Model vs Ensemble Comparison")
print("=" * 60)
print(f"Single Model (Decision Tree) - Test Accuracy: {single_model.score(X_test, y_test):.4f}")
print(f"Ensemble Model (Voting)      - Test Accuracy: {ensemble.score(X_test, y_test):.4f}")
print(f"Improvement: {(ensemble.score(X_test, y_test) - single_model.score(X_test, y_test)) * 100:.2f}%")
```

**ê¸°ëŒ€ ì¶œë ¥**:
- ë‹¨ì¼ ëª¨ë¸: ê³¼ì í•© ë˜ëŠ” ê³¼ì†Œì í•©ìœ¼ë¡œ ê²°ì • ê²½ê³„ê°€ ë¶ˆì•ˆì •
- ì•™ìƒë¸” ëª¨ë¸: ë” ë¶€ë“œëŸ½ê³  ì•ˆì •ì ì¸ ê²°ì • ê²½ê³„
- ì •í™•ë„ í–¥ìƒ: ì¼ë°˜ì ìœ¼ë¡œ 3-10% ê°œì„ 

### ğŸ’¬ í¼ì‹¤ë¦¬í…Œì´ì…˜ ì§ˆë¬¸

**ì§ˆë¬¸ 1**: "ì—¬ëŸ¬ ëª¨ë¸ì„ í•©ì¹˜ë©´ ì™œ ë” ë‚˜ì€ ê²°ê³¼ê°€ ë‚˜ì˜¬ê¹Œìš”?"

**ë‹µë³€ ê°€ì´ë“œ**:
- ê° ëª¨ë¸ì´ ë‹¤ë¥¸ ì‹¤ìˆ˜ë¥¼ í•˜ê¸° ë•Œë¬¸
- í‰ê· ì„ ë‚´ë©´ ê·¹ë‹¨ì ì¸ ì˜ˆì¸¡ì´ ì™„í™”ë¨
- ì§‘ë‹¨ì§€ì„±: ì—¬ëŸ¬ ì „ë¬¸ê°€ì˜ ì˜ê²¬ì´ í•œ ì‚¬ëŒë³´ë‹¤ ì •í™•

**ì§ˆë¬¸ 2**: "ëª¨ë“  ê²½ìš°ì— ì•™ìƒë¸”ì´ í•­ìƒ ë” ì¢‹ì„ê¹Œìš”?"

**ë‹µë³€ ê°€ì´ë“œ**:
- ëª¨ë¸ë“¤ì´ ë„ˆë¬´ ë¹„ìŠ·í•˜ë©´ íš¨ê³¼ ê°ì†Œ
- ê³„ì‚° ë¹„ìš© ì¦ê°€
- í•´ì„ë ¥ ê°ì†Œ
- ì ì ˆí•œ ê· í˜•ì´ í•„ìš”

### ğŸ§® ë£¨ë¸Œë¦­ í‰ê°€í‘œ

| í‰ê°€í•­ëª© | ìš°ìˆ˜ (3ì ) | ë³´í†µ (2ì ) | ë¯¸í¡ (1ì ) |
|----------|-----------|-----------|-----------|
| **ê°œë… ì´í•´** | ë°°ê¹…, ë¶€ìŠ¤íŒ…, ìŠ¤íƒœí‚¹ì˜ ì°¨ì´ë¥¼ ëª…í™•íˆ ì„¤ëª…í•˜ê³  ì˜ˆì‹œ ì œì‹œ | 3ê°€ì§€ ë°©ì‹ì˜ ì •ì˜ëŠ” ì•Œì§€ë§Œ ì°¨ì´ì  ì„¤ëª…ì´ ë¶ˆëª…í™• | ê°œë… ì •ì˜ê°€ í˜¼ë€ìŠ¤ëŸ½ê±°ë‚˜ ë¶ˆì™„ì „ |
| **í¸í–¥-ë¶„ì‚° ì´í•´** | í¸í–¥ê³¼ ë¶„ì‚°ì˜ ì˜ë¯¸ë¥¼ ì´í•´í•˜ê³  ì•™ìƒë¸”ê³¼ì˜ ê´€ê³„ ì„¤ëª… | í¸í–¥-ë¶„ì‚° ì •ì˜ëŠ” ì•Œì§€ë§Œ ì•™ìƒë¸”ê³¼ì˜ ì—°ê²°ì´ ì•½í•¨ | í¸í–¥-ë¶„ì‚° ê°œë… ì´í•´ ë¶€ì¡± |
| **ì‹œê°í™” í•´ì„** | ê²°ì • ê²½ê³„ì˜ ì°¨ì´ë¥¼ ì •í™•íˆ í•´ì„í•˜ê³  ì„±ëŠ¥ í–¥ìƒ ì´ìœ  ì„¤ëª… | ì‹œê°í™” ê²°ê³¼ëŠ” ë³´ì§€ë§Œ ê¹Šì€ í•´ì„ ë¶€ì¡± | ì‹œê°í™”ë¥¼ ì œëŒ€ë¡œ ì´í•´í•˜ì§€ ëª»í•¨ |

---

## 2ï¸âƒ£ ë‹¤ìˆ˜ê²° íˆ¬í‘œ ì•™ìƒë¸” (Voting)

### ğŸ“– ì´ë¡  ì„¤ëª…

#### 2.1 íˆ¬í‘œ ì•™ìƒë¸”ì˜ ê°œë…

íˆ¬í‘œ(Voting) ì•™ìƒë¸”ì€ ì—¬ëŸ¬ ê°œì˜ **ì„œë¡œ ë‹¤ë¥¸ ì•Œê³ ë¦¬ì¦˜**ì„ í•™ìŠµì‹œí‚¤ê³ , ê·¸ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ íˆ¬í‘œë¡œ ê²°í•©í•©ë‹ˆë‹¤.

**í•µì‹¬ ì•„ì´ë””ì–´**: ë¯¼ì£¼ì£¼ì˜ íˆ¬í‘œì²˜ëŸ¼ ë‹¤ìˆ˜ê²°ë¡œ ìµœì¢… ê²°ì •

#### 2.2 í•˜ë“œ ë³´íŒ… (Hard Voting)

ê° ëª¨ë¸ì˜ ì˜ˆì¸¡ í´ë˜ìŠ¤ë¥¼ ì§‘ê³„í•˜ì—¬ **ê°€ì¥ ë§ì´ ì˜ˆì¸¡ëœ í´ë˜ìŠ¤**ë¥¼ ì„ íƒí•©ë‹ˆë‹¤.

**ì˜ˆì‹œ**:
- ëª¨ë¸ 1: Class A
- ëª¨ë¸ 2: Class B
- ëª¨ë¸ 3: Class A
- ëª¨ë¸ 4: Class A
- **ìµœì¢… ì˜ˆì¸¡: Class A (3í‘œ)**

#### 2.3 ì†Œí”„íŠ¸ ë³´íŒ… (Soft Voting)

ê° ëª¨ë¸ì˜ **ì˜ˆì¸¡ í™•ë¥ ì„ í‰ê· **í•˜ì—¬ ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ í´ë˜ìŠ¤ë¥¼ ì„ íƒí•©ë‹ˆë‹¤.

**ì˜ˆì‹œ**:
- ëª¨ë¸ 1: [0.7, 0.3] â†’ Class 0
- ëª¨ë¸ 2: [0.4, 0.6] â†’ Class 1
- ëª¨ë¸ 3: [0.6, 0.4] â†’ Class 0
- **í‰ê·  í™•ë¥ : [0.567, 0.433]**
- **ìµœì¢… ì˜ˆì¸¡: Class 0**

**ì¥ì **: í™•ë¥  ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ë” ì •êµí•œ ì˜ˆì¸¡ ê°€ëŠ¥

### ğŸ”¢ ìˆ˜ì‹

#### í•˜ë“œ ë³´íŒ…

$$\hat{y} = \text{mode}\{h_1(x), h_2(x), \ldots, h_M(x)\}$$

ì—¬ê¸°ì„œ:
- $h_i(x)$: ië²ˆì§¸ ëª¨ë¸ì˜ ì˜ˆì¸¡
- $M$: ëª¨ë¸ ê°œìˆ˜
- $\text{mode}$: ìµœë¹ˆê°’ (ê°€ì¥ ë§ì´ ë‚˜ì˜¨ ê°’)

#### ì†Œí”„íŠ¸ ë³´íŒ…

$$\hat{y} = \arg\max_c \frac{1}{M} \sum_{i=1}^{M} P_{h_i}(c|x)$$

ì—¬ê¸°ì„œ:
- $P_{h_i}(c|x)$: ië²ˆì§¸ ëª¨ë¸ì´ ì˜ˆì¸¡í•œ í´ë˜ìŠ¤ cì˜ í™•ë¥ 
- $\arg\max_c$: í™•ë¥ ì´ ê°€ì¥ ë†’ì€ í´ë˜ìŠ¤

#### ë‹¨ê³„ë³„ ê³„ì‚° ì˜ˆì œ

3ê°œì˜ ëª¨ë¸ì´ ìˆê³ , 2ê°œì˜ í´ë˜ìŠ¤(0, 1)ë¥¼ ë¶„ë¥˜í•œë‹¤ê³  ê°€ì •:

**ê° ëª¨ë¸ì˜ í™•ë¥  ì˜ˆì¸¡**:
- ëª¨ë¸ 1: $P(y=0|x) = 0.8, P(y=1|x) = 0.2$
- ëª¨ë¸ 2: $P(y=0|x) = 0.5, P(y=1|x) = 0.5$
- ëª¨ë¸ 3: $P(y=0|x) = 0.6, P(y=1|x) = 0.4$

**í‰ê·  í™•ë¥  ê³„ì‚°**:
$$P_{\text{ensemble}}(y=0|x) = \frac{0.8 + 0.5 + 0.6}{3} = \frac{1.9}{3} = 0.633$$
$$P_{\text{ensemble}}(y=1|x) = \frac{0.2 + 0.5 + 0.4}{3} = \frac{1.1}{3} = 0.367$$

**ìµœì¢… ì˜ˆì¸¡**: Class 0 (0.633 > 0.367)

### ğŸ’» ì‹œê°í™” ì½”ë“œ

```python
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import rc
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import VotingClassifier

# í•œê¸€ í°íŠ¸ ì„¤ì •
rc('font', family='DejaVu Sans')

# ë°ì´í„° ìƒì„±
np.random.seed(42)
X, y = make_classification(n_samples=500, n_features=2, n_redundant=0, 
                          n_informative=2, n_clusters_per_class=1,
                          random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# ê°œë³„ ëª¨ë¸ í•™ìŠµ
lr = LogisticRegression(random_state=42)
svm = SVC(kernel='rbf', probability=True, random_state=42)
knn = KNeighborsClassifier(n_neighbors=5)

lr.fit(X_train, y_train)
svm.fit(X_train, y_train)
knn.fit(X_train, y_train)

# ë³´íŒ… ì•™ìƒë¸”
voting_hard = VotingClassifier(
    estimators=[('lr', lr), ('svm', svm), ('knn', knn)],
    voting='hard'
)
voting_soft = VotingClassifier(
    estimators=[('lr', lr), ('svm', svm), ('knn', knn)],
    voting='soft'
)

voting_hard.fit(X_train, y_train)
voting_soft.fit(X_train, y_train)

# ê²°ì • ê²½ê³„ ì‹œê°í™”
def plot_decision_boundary(model, X, y, title, ax):
    h = 0.02
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))
    
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    
    ax.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.RdYlBu)
    ax.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdYlBu, 
              edgecolor='black', s=30, alpha=0.6)
    ax.set_title(title, fontsize=11, weight='bold')
    ax.set_xlabel('Feature 1', fontsize=9)
    ax.set_ylabel('Feature 2', fontsize=9)

# 5ê°œ ì„œë¸Œí”Œë¡¯ ìƒì„±
fig, axes = plt.subplots(2, 3, figsize=(15, 10))

models = [
    (lr, 'Logistic Regression'),
    (svm, 'SVM (RBF Kernel)'),
    (knn, 'K-Nearest Neighbors'),
    (voting_hard, 'Hard Voting'),
    (voting_soft, 'Soft Voting')
]

for idx, (model, title) in enumerate(models):
    row = idx // 3
    col = idx % 3
    plot_decision_boundary(model, X_train, y_train, title, axes[row, col])
    
    # ì •í™•ë„ í‘œì‹œ
    score = model.score(X_test, y_test)
    axes[row, col].text(0.5, 0.05, f'Test Acc: {score:.3f}', 
                       transform=axes[row, col].transAxes,
                       ha='center', fontsize=10, weight='bold',
                       bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

# ë§ˆì§€ë§‰ ì„œë¸Œí”Œë¡¯ ì œê±°
axes[1, 2].remove()

# ì •í™•ë„ ë¹„êµ ë°” ì°¨íŠ¸ ì¶”ê°€
ax_bar = fig.add_subplot(2, 3, 6)
model_names = ['LR', 'SVM', 'KNN', 'Hard\nVoting', 'Soft\nVoting']
accuracies = [model.score(X_test, y_test) for model, _ in models]

colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A', '#98D8C8']
bars = ax_bar.bar(model_names, accuracies, color=colors, edgecolor='black', linewidth=1.5)

# ê°’ í‘œì‹œ
for bar, acc in zip(bars, accuracies):
    height = bar.get_height()
    ax_bar.text(bar.get_x() + bar.get_width()/2., height,
               f'{acc:.3f}',
               ha='center', va='bottom', fontsize=10, weight='bold')

ax_bar.set_ylabel('Test Accuracy', fontsize=11, weight='bold')
ax_bar.set_title('Model Performance Comparison', fontsize=12, weight='bold')
ax_bar.set_ylim([0.8, 1.0])
ax_bar.grid(axis='y', alpha=0.3, linestyle='--')

plt.tight_layout()
plt.savefig('voting_ensemble.png', dpi=150, bbox_inches='tight')
plt.show()

# ì„±ëŠ¥ ë¹„êµ ì¶œë ¥
print("=" * 70)
print("VOTING ENSEMBLE PERFORMANCE COMPARISON")
print("=" * 70)
print(f"{'Model':<25} {'Train Accuracy':<20} {'Test Accuracy':<20}")
print("-" * 70)
for model, name in models:
    train_acc = model.score(X_train, y_train)
    test_acc = model.score(X_test, y_test)
    print(f"{name:<25} {train_acc:<20.4f} {test_acc:<20.4f}")
print("=" * 70)
```

**ê¸°ëŒ€ ì¶œë ¥**:
- ê° ëª¨ë¸ì˜ ì„œë¡œ ë‹¤ë¥¸ ê²°ì • ê²½ê³„
- ë³´íŒ… ì•™ìƒë¸”ì˜ ë” ì•ˆì •ì ì´ê³  ë¶€ë“œëŸ¬ìš´ ê²½ê³„
- ì†Œí”„íŠ¸ ë³´íŒ…ì´ ì¼ë°˜ì ìœ¼ë¡œ í•˜ë“œ ë³´íŒ…ë³´ë‹¤ ì•½ê°„ ë” ì¢‹ì€ ì„±ëŠ¥

### ğŸ’¬ í¼ì‹¤ë¦¬í…Œì´ì…˜ ì§ˆë¬¸

**ì§ˆë¬¸ 1**: "íˆ¬í‘œ ê·œì¹™ì„ ë°”ê¾¸ë©´ ê²°ê³¼ëŠ” ì–´ë–»ê²Œ ë‹¬ë¼ì§ˆê¹Œìš”?"

**ë‹µë³€ ê°€ì´ë“œ**:
- **í•˜ë“œ ë³´íŒ…**: ê°„ë‹¨í•˜ì§€ë§Œ í™•ë¥  ì •ë³´ ì†ì‹¤
- **ì†Œí”„íŠ¸ ë³´íŒ…**: í™•ë¥ ì„ í™œìš©í•˜ì—¬ ë” ì •êµí•˜ì§€ë§Œ, ëª¨ë“  ëª¨ë¸ì´ `predict_proba` ì§€ì› í•„ìš”
- **ê°€ì¤‘ ë³´íŒ…**: ì„±ëŠ¥ì´ ì¢‹ì€ ëª¨ë¸ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜ ë¶€ì—¬ ê°€ëŠ¥

**ì§ˆë¬¸ 2**: "ì–´ë–¤ ì¢…ë¥˜ì˜ ëª¨ë¸ë“¤ì„ ì¡°í•©í•˜ëŠ” ê²ƒì´ ì¢‹ì„ê¹Œìš”?"

**ë‹µë³€ ê°€ì´ë“œ**:
- **ë‹¤ì–‘ì„±(Diversity)ì´ ì¤‘ìš”**: ì„œë¡œ ë‹¤ë¥¸ ê°€ì •ì„ ê°€ì§„ ëª¨ë¸ ì¡°í•©
- ì˜ˆ: ì„ í˜• ëª¨ë¸(LR) + ë¹„ì„ í˜• ëª¨ë¸(SVM) + ì¸ìŠ¤í„´ìŠ¤ ê¸°ë°˜(KNN)
- ë„ˆë¬´ ë¹„ìŠ·í•œ ëª¨ë¸ë“¤ì€ íš¨ê³¼ ê°ì†Œ

### ğŸ§® ë£¨ë¸Œë¦­ í‰ê°€í‘œ

| í‰ê°€í•­ëª© | ìš°ìˆ˜ (3ì ) | ë³´í†µ (2ì ) | ë¯¸í¡ (1ì ) |
|----------|-----------|-----------|-----------|
| **ë³´íŒ… ì›ë¦¬ ì´í•´** | í•˜ë“œ/ì†Œí”„íŠ¸ ë³´íŒ…ì˜ ì°¨ì´ë¥¼ ìˆ˜ì‹ê³¼ ì˜ˆì‹œë¡œ ëª…í™•íˆ ì„¤ëª… | ë‘ ë°©ì‹ì˜ ì°¨ì´ëŠ” ì•Œì§€ë§Œ êµ¬ì²´ì  ì„¤ëª… ë¶€ì¡± | ê°œë… ì´í•´ê°€ ë¶ˆëª…í™• |
| **ì‹œê°í™” ì •í™•ì„±** | ê²°ì • ê²½ê³„ì™€ ì„±ëŠ¥ ì°¨ì´ë¥¼ ì •í™•íˆ í•´ì„ | ì‹œê°í™” ê²°ê³¼ë¥¼ ë³´ì§€ë§Œ í•´ì„ì´ í”¼ìƒì  | ì‹œê°í™” ì´í•´ ë¶€ì¡± |
| **ì½”ë“œ êµ¬í˜„** | VotingClassifierë¥¼ ì˜¬ë°”ë¥´ê²Œ ì‚¬ìš©í•˜ê³  íŒŒë¼ë¯¸í„° ì´í•´ | ì½”ë“œëŠ” ì‹¤í–‰ë˜ì§€ë§Œ íŒŒë¼ë¯¸í„° ì´í•´ ë¶€ì¡± | ì½”ë“œ ì‹¤í–‰ ì‹¤íŒ¨ ë˜ëŠ” ì´í•´ ë¶€ì¡± |

---

## 3ï¸âƒ£ ë°°ê¹… (Bagging)

### ğŸ“– ì´ë¡  ì„¤ëª…

#### 3.1 ë°°ê¹…ì˜ ê°œë…

**Bagging** = **B**ootstrap **Agg**regat**ing**

ë°°ê¹…ì€ **ê°™ì€ ì•Œê³ ë¦¬ì¦˜**ì„ **ë‹¤ë¥¸ ë°ì´í„°ì…‹(ë¶€íŠ¸ìŠ¤íŠ¸ë© ìƒ˜í”Œ)**ìœ¼ë¡œ ì—¬ëŸ¬ ë²ˆ í•™ìŠµì‹œì¼œ ê²°í•©í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.

**í•µì‹¬ ì•„ì´ë””ì–´**:
- ë°ì´í„°ì˜ ë‹¤ë¥¸ ë¶€ë¶„ ì§‘í•©ìœ¼ë¡œ ì—¬ëŸ¬ ëª¨ë¸ í•™ìŠµ
- ê° ëª¨ë¸ì˜ ì˜ˆì¸¡ì„ í‰ê· (íšŒê·€) ë˜ëŠ” íˆ¬í‘œ(ë¶„ë¥˜)ë¡œ ê²°í•©
- **ë¶„ì‚° ê°ì†Œ**ê°€ ì£¼ìš” ëª©í‘œ

#### 3.2 ë¶€íŠ¸ìŠ¤íŠ¸ë© ìƒ˜í”Œë§ (Bootstrap Sampling)

ë¶€íŠ¸ìŠ¤íŠ¸ë©ì€ **ë³µì› ì¶”ì¶œ(sampling with replacement)**ë¡œ ì›ë³¸ ë°ì´í„°ì™€ ê°™ì€ í¬ê¸°ì˜ ìƒ˜í”Œì„ ë§Œë“œëŠ” ë°©ë²•ì…ë‹ˆë‹¤.

**ê³¼ì •**:
1. ì›ë³¸ ë°ì´í„°ì…‹ì—ì„œ ë¬´ì‘ìœ„ë¡œ í•˜ë‚˜ì˜ ìƒ˜í”Œ ì„ íƒ
2. ì„ íƒí•œ ìƒ˜í”Œì„ ë‹¤ì‹œ ë°ì´í„°ì…‹ì— ë„£ìŒ (ë³µì›)
3. 1-2ë¥¼ ì›ë³¸ ë°ì´í„° í¬ê¸°ë§Œí¼ ë°˜ë³µ

**ê²°ê³¼**:
- ì–´ë–¤ ìƒ˜í”Œì€ ì—¬ëŸ¬ ë²ˆ ì„ íƒë¨
- ì–´ë–¤ ìƒ˜í”Œì€ í•œ ë²ˆë„ ì„ íƒë˜ì§€ ì•ŠìŒ (~36.8%)
- ê° ë¶€íŠ¸ìŠ¤íŠ¸ë© ìƒ˜í”Œì€ ì„œë¡œ ë‹¤ë¦„

#### 3.3 Random Forest

ê°€ì¥ ìœ ëª…í•œ ë°°ê¹… ì•Œê³ ë¦¬ì¦˜ì€ **Random Forest**ì…ë‹ˆë‹¤.

**Random Forest = Bagging + íŠ¹ì„± ëœë¤ ì„ íƒ**

ì¶”ê°€ ê¸°ë²•:
- ê° ë¶„í• ì—ì„œ ì „ì²´ íŠ¹ì„± ì¤‘ ì¼ë¶€ë§Œ ê³ ë ¤
- íŠ¸ë¦¬ ê°„ ìƒê´€ì„± ê°ì†Œ â†’ ë” í° ë‹¤ì–‘ì„±

### ğŸ”¢ ìˆ˜ì‹

#### ë¶€íŠ¸ìŠ¤íŠ¸ë© ìƒ˜í”Œë§ í™•ë¥ 

ì›ë³¸ ë°ì´í„°ì— Nê°œì˜ ìƒ˜í”Œì´ ìˆì„ ë•Œ, íŠ¹ì • ìƒ˜í”Œì´ ë¶€íŠ¸ìŠ¤íŠ¸ë© ìƒ˜í”Œì— **í¬í•¨ë˜ì§€ ì•Šì„ í™•ë¥ **:

$$P(\text{not selected}) = \left(1 - \frac{1}{N}\right)^N$$

Nì´ ì¶©ë¶„íˆ í´ ë•Œ:

$$\lim_{N \to \infty} \left(1 - \frac{1}{N}\right)^N = e^{-1} \approx 0.368$$

**í•´ì„**: ê° ë¶€íŠ¸ìŠ¤íŠ¸ë© ìƒ˜í”Œì€ ì›ë³¸ ë°ì´í„°ì˜ ì•½ 63.2%ë§Œ í¬í•¨

#### ë°°ê¹… ì˜ˆì¸¡

**ë¶„ë¥˜ ë¬¸ì œ**:
$$\hat{y} = \text{mode}\{h_1(x), h_2(x), \ldots, h_M(x)\}$$

**íšŒê·€ ë¬¸ì œ**:
$$\hat{y} = \frac{1}{M} \sum_{i=1}^{M} h_i(x)$$

ì—¬ê¸°ì„œ:
- $h_i(x)$: ië²ˆì§¸ ë¶€íŠ¸ìŠ¤íŠ¸ë© ìƒ˜í”Œë¡œ í•™ìŠµí•œ ëª¨ë¸ì˜ ì˜ˆì¸¡
- $M$: ë¶€íŠ¸ìŠ¤íŠ¸ë© ìƒ˜í”Œ(ëª¨ë¸) ê°œìˆ˜

### ğŸ’» ì‹œê°í™” ì½”ë“œ

```python
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import rc
from sklearn.datasets import load_wine
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn.decomposition import PCA

# í•œê¸€ í°íŠ¸ ì„¤ì •
rc('font', family='DejaVu Sans')

# Wine ë°ì´í„°ì…‹ ë¡œë“œ
wine = load_wine()
X, y = wine.data, wine.target

# PCAë¡œ 2ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œ (ì‹œê°í™”ë¥¼ ìœ„í•´)
pca = PCA(n_components=2, random_state=42)
X_pca = pca.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(
    X_pca, y, test_size=0.3, random_state=42, stratify=y
)

# ëª¨ë¸ ì •ì˜
single_tree = DecisionTreeClassifier(max_depth=None, random_state=42)
bagging = BaggingClassifier(
    estimator=DecisionTreeClassifier(),
    n_estimators=100,
    max_samples=0.8,
    max_features=0.8,
    bootstrap=True,
    random_state=42
)

single_tree.fit(X_train, y_train)
bagging.fit(X_train, y_train)

# ë¶€íŠ¸ìŠ¤íŠ¸ë© ìƒ˜í”Œ ì‹œê°í™”
fig = plt.figure(figsize=(16, 10))

# 1. ì›ë³¸ ë°ì´í„°
ax1 = plt.subplot(2, 3, 1)
scatter = ax1.scatter(X_train[:, 0], X_train[:, 1], c=y_train, 
                     cmap='viridis', s=100, alpha=0.6, edgecolor='black')
ax1.set_title('Original Training Data', fontsize=12, weight='bold')
ax1.set_xlabel('PC1', fontsize=10)
ax1.set_ylabel('PC2', fontsize=10)
plt.colorbar(scatter, ax=ax1, label='Class')

# 2-4. ë¶€íŠ¸ìŠ¤íŠ¸ë© ìƒ˜í”Œ 3ê°œ
np.random.seed(42)
for i in range(3):
    ax = plt.subplot(2, 3, i + 2)
    
    # ë¶€íŠ¸ìŠ¤íŠ¸ë© ìƒ˜í”Œë§
    n_samples = len(X_train)
    bootstrap_indices = np.random.choice(n_samples, size=n_samples, replace=True)
    X_boot = X_train[bootstrap_indices]
    y_boot = y_train[bootstrap_indices]
    
    # ì‹œê°í™”
    scatter = ax.scatter(X_boot[:, 0], X_boot[:, 1], c=y_boot, 
                        cmap='viridis', s=100, alpha=0.6, edgecolor='black')
    ax.set_title(f'Bootstrap Sample {i+1}', fontsize=12, weight='bold')
    ax.set_xlabel('PC1', fontsize=10)
    ax.set_ylabel('PC2', fontsize=10)
    plt.colorbar(scatter, ax=ax, label='Class')

# 5-6. ê²°ì • ê²½ê³„ ë¹„êµ
def plot_decision_boundary(model, X, y, title, ax):
    h = 0.02
    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5
    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))
    
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    
    ax.contourf(xx, yy, Z, alpha=0.4, cmap='viridis')
    ax.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', 
              edgecolor='black', s=50, alpha=0.7)
    ax.set_title(title, fontsize=12, weight='bold')
    ax.set_xlabel('PC1', fontsize=10)
    ax.set_ylabel('PC2', fontsize=10)

ax5 = plt.subplot(2, 3, 5)
plot_decision_boundary(single_tree, X_train, y_train, 
                      f'Single Tree (Test Acc: {single_tree.score(X_test, y_test):.3f})', ax5)

ax6 = plt.subplot(2, 3, 6)
plot_decision_boundary(bagging, X_train, y_train, 
                      f'Bagging (Test Acc: {bagging.score(X_test, y_test):.3f})', ax6)

plt.tight_layout()
plt.savefig('bagging_bootstrap.png', dpi=150, bbox_inches='tight')
plt.show()

# ëª¨ë¸ ìˆ˜ì— ë”°ë¥¸ ì„±ëŠ¥ ë³€í™”
n_estimators_range = range(1, 101, 5)
train_scores = []
test_scores = []

for n_est in n_estimators_range:
    bag = BaggingClassifier(
        estimator=DecisionTreeClassifier(),
        n_estimators=n_est,
        random_state=42
    )
    bag.fit(X_train, y_train)
    train_scores.append(bag.score(X_train, y_train))
    test_scores.append(bag.score(X_test, y_test))

# ì„±ëŠ¥ ê³¡ì„  ê·¸ë¦¬ê¸°
plt.figure(figsize=(10, 6))
plt.plot(n_estimators_range, train_scores, label='Train Accuracy', 
         marker='o', linewidth=2, markersize=6)
plt.plot(n_estimators_range, test_scores, label='Test Accuracy', 
         marker='s', linewidth=2, markersize=6)
plt.axhline(y=single_tree.score(X_test, y_test), color='r', 
            linestyle='--', label='Single Tree (Test)', linewidth=2)
plt.xlabel('Number of Estimators', fontsize=12, weight='bold')
plt.ylabel('Accuracy', fontsize=12, weight='bold')
plt.title('Bagging Performance vs Number of Estimators', fontsize=14, weight='bold')
plt.legend(fontsize=11)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig('bagging_performance.png', dpi=150, bbox_inches='tight')
plt.show()

print("=" * 70)
print("BAGGING ANALYSIS")
print("=" * 70)
print(f"Single Decision Tree:")
print(f"  Train Accuracy: {single_tree.score(X_train, y_train):.4f}")
print(f"  Test Accuracy:  {single_tree.score(X_test, y_test):.4f}")
print(f"\nBagging (100 estimators):")
print(f"  Train Accuracy: {bagging.score(X_train, y_train):.4f}")
print(f"  Test Accuracy:  {bagging.score(X_test, y_test):.4f}")
print(f"\nImprovement: {(bagging.score(X_test, y_test) - single_tree.score(X_test, y_test)) * 100:.2f}%")
print("=" * 70)
```

**ê¸°ëŒ€ ì¶œë ¥**:
- ê° ë¶€íŠ¸ìŠ¤íŠ¸ë© ìƒ˜í”Œì˜ ì„œë¡œ ë‹¤ë¥¸ ë¶„í¬
- ë‹¨ì¼ íŠ¸ë¦¬: ê³¼ì í•©ëœ ë³µì¡í•œ ê²½ê³„
- ë°°ê¹…: ë¶€ë“œëŸ½ê³  ì¼ë°˜í™”ëœ ê²½ê³„
- ëª¨ë¸ ìˆ˜ ì¦ê°€ â†’ ì„±ëŠ¥ í–¥ìƒ â†’ í¬í™”

### ğŸ’¬ í¼ì‹¤ë¦¬í…Œì´ì…˜ ì§ˆë¬¸

**ì§ˆë¬¸ 1**: "ëª¨ë¸ ìˆ˜ë¥¼ ëŠ˜ë¦¬ë©´ ì™œ ë¶„ì‚°ì´ ì¤„ì–´ë“¤ê¹Œìš”?"

**ë‹µë³€ ê°€ì´ë“œ**:
- ê° ëª¨ë¸ì˜ ë¬´ì‘ìœ„ì„±ì´ í‰ê· ìœ¼ë¡œ ìƒì‡„ë¨
- ì¤‘ì‹¬ê·¹í•œì •ë¦¬: í‰ê· ì˜ ë¶„ì‚°ì€ $\sigma^2/N$ë¡œ ê°ì†Œ
- ê·¸ë˜í”„ì—ì„œ í…ŒìŠ¤íŠ¸ ì •í™•ë„ê°€ ì•ˆì •í™”ë˜ëŠ” ê²ƒ í™•ì¸

**ì§ˆë¬¸ 2**: "ë¶€íŠ¸ìŠ¤íŠ¸ë© ìƒ˜í”Œì´ ì›ë³¸ê³¼ ë‹¤ë¥¸ ì´ìœ ëŠ” ë¬´ì—‡ì¼ê¹Œìš”?"

**ë‹µë³€ ê°€ì´ë“œ**:
- ë³µì› ì¶”ì¶œë¡œ ì¼ë¶€ ìƒ˜í”Œì€ ì¤‘ë³µ, ì¼ë¶€ëŠ” ëˆ„ë½
- ê° ìƒ˜í”Œì€ ì›ë³¸ì˜ ì•½ 63.2%ë§Œ í¬í•¨
- ì´ ì°¨ì´ê°€ ëª¨ë¸ì˜ ë‹¤ì–‘ì„±ì„ ë§Œë“¦

### ğŸ§® ë£¨ë¸Œë¦­ í‰ê°€í‘œ

| í‰ê°€í•­ëª© | ìš°ìˆ˜ (3ì ) | ë³´í†µ (2ì ) | ë¯¸í¡ (1ì ) |
|----------|-----------|-----------|-----------|
| **ë¶€íŠ¸ìŠ¤íŠ¸ë© ì´í•´** | ë¶€íŠ¸ìŠ¤íŠ¸ë© ìƒ˜í”Œë§ ì›ë¦¬ì™€ í™•ë¥  ê³„ì‚°ì„ ì •í™•íˆ ì„¤ëª… | ê°œë…ì€ ì´í•´í•˜ì§€ë§Œ ìˆ˜í•™ì  ì„¤ëª… ë¶€ì¡± | ë¶€íŠ¸ìŠ¤íŠ¸ë© ê°œë… ì´í•´ ë¶€ì¡± |
| **ì‹¤í—˜ ì„¤ê³„** | ë‹¤ì–‘í•œ íŒŒë¼ë¯¸í„°ë¡œ ì²´ê³„ì ì¸ ì‹¤í—˜ ìˆ˜í–‰ | ê¸°ë³¸ ì‹¤í—˜ì€ í•˜ì§€ë§Œ ì‹¬í™” ë¶„ì„ ë¶€ì¡± | ì‹¤í—˜ ì„¤ê³„ ë¯¸í¡ |
| **ì‹œê°í™” ëª…í™•ì„±** | ë¶€íŠ¸ìŠ¤íŠ¸ë© ìƒ˜í”Œê³¼ ì„±ëŠ¥ ë³€í™”ë¥¼ ëª…í™•íˆ ì‹œê°í™” | ì‹œê°í™”ëŠ” ìˆì§€ë§Œ í•´ì„ ë¶€ì¡± | ì‹œê°í™” í’ˆì§ˆ ë‚®ìŒ |

---

## 4ï¸âƒ£ ì—ì´ë‹¤ë¶€ìŠ¤íŠ¸ (AdaBoost)

### ğŸ“– ì´ë¡  ì„¤ëª…

#### 4.1 ë¶€ìŠ¤íŒ…ì˜ ê°œë…

**Boosting**ì€ **ì•½í•œ í•™ìŠµê¸°(weak learner)**ë“¤ì„ **ìˆœì°¨ì ìœ¼ë¡œ** ê²°í•©í•˜ì—¬ ê°•í•œ í•™ìŠµê¸°ë¥¼ ë§Œë“œëŠ” ë°©ë²•ì…ë‹ˆë‹¤.

**í•µì‹¬ ì•„ì´ë””ì–´**:
- ì´ì „ ëª¨ë¸ì´ í‹€ë¦° ìƒ˜í”Œì— ë” ì§‘ì¤‘
- ê° ëª¨ë¸ì€ ì´ì „ ëª¨ë¸ì˜ ì‹¤ìˆ˜ë¥¼ ë³´ì™„
- **í¸í–¥ ê°ì†Œ**ê°€ ì£¼ìš” ëª©í‘œ

**ë°°ê¹… vs ë¶€ìŠ¤íŒ…**:
| íŠ¹ì„± | ë°°ê¹… | ë¶€ìŠ¤íŒ… |
|------|------|--------|
| í•™ìŠµ ë°©ì‹ | ë³‘ë ¬ (ë…ë¦½ì ) | ìˆœì°¨ì  (ì˜ì¡´ì ) |
| ë°ì´í„° ìƒ˜í”Œë§ | ë¶€íŠ¸ìŠ¤íŠ¸ë© | ê°€ì¤‘ì¹˜ ì¡°ì • |
| ì£¼ìš” ëª©í‘œ | ë¶„ì‚° ê°ì†Œ | í¸í–¥ ê°ì†Œ |
| ëŒ€í‘œ ì•Œê³ ë¦¬ì¦˜ | Random Forest | AdaBoost, Gradient Boosting |

#### 4.2 AdaBoost ì•Œê³ ë¦¬ì¦˜

**AdaBoost** = **Ada**ptive **Boost**ing

AdaBoostëŠ” ê°€ì¥ ëŒ€í‘œì ì¸ ë¶€ìŠ¤íŒ… ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤.

**ì‘ë™ ì›ë¦¬**:
1. ëª¨ë“  ìƒ˜í”Œì— ë™ì¼í•œ ê°€ì¤‘ì¹˜ ë¶€ì—¬
2. ì•½í•œ í•™ìŠµê¸° í•™ìŠµ
3. ì˜ëª» ë¶„ë¥˜ëœ ìƒ˜í”Œì˜ ê°€ì¤‘ì¹˜ ì¦ê°€
4. 2-3 ë°˜ë³µ
5. ìµœì¢…ì ìœ¼ë¡œ ëª¨ë“  í•™ìŠµê¸°ë¥¼ ê°€ì¤‘ íˆ¬í‘œë¡œ ê²°í•©

#### 4.3 ì•½í•œ í•™ìŠµê¸° (Weak Learner)

**ì•½í•œ í•™ìŠµê¸°**: ë¬´ì‘ìœ„ ì¶”ì¸¡ë³´ë‹¤ ì¡°ê¸ˆ ë‚˜ì€ ëª¨ë¸
- ì¼ë°˜ì ìœ¼ë¡œ ê¹Šì´ê°€ 1ì¸ ê²°ì • íŠ¸ë¦¬ (decision stump) ì‚¬ìš©
- ì •í™•ë„ 50% ì´ìƒì´ë©´ ì¶©ë¶„

**ì™œ ì•½í•œ í•™ìŠµê¸°ë¥¼ ì‚¬ìš©í• ê¹Œ?**
- ê³¼ì í•© ë°©ì§€
- ë¹ ë¥¸ í•™ìŠµ
- ìˆœì°¨ì  ê²°í•©ìœ¼ë¡œ ì ì§„ì  ê°œì„ 

### ğŸ”¢ ìˆ˜ì‹

#### AdaBoost ì•Œê³ ë¦¬ì¦˜ ìƒì„¸

**ì´ˆê¸°í™”** (t=0):
$$w_i^{(0)} = \frac{1}{N}, \quad i = 1, 2, \ldots, N$$

ì—¬ê¸°ì„œ $w_i$ëŠ” ië²ˆì§¸ ìƒ˜í”Œì˜ ê°€ì¤‘ì¹˜, Nì€ ì „ì²´ ìƒ˜í”Œ ìˆ˜

**ë°˜ë³µ** (t = 1, 2, ..., T):

**1ë‹¨ê³„**: ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì•½í•œ í•™ìŠµê¸° $h_t$ í•™ìŠµ

**2ë‹¨ê³„**: ê°€ì¤‘ ì˜¤ì°¨ìœ¨ ê³„ì‚°
$$\epsilon_t = \frac{\sum_{i=1}^{N} w_i^{(t-1)} \mathbb{1}(h_t(x_i) \neq y_i)}{\sum_{i=1}^{N} w_i^{(t-1)}}$$

ì—¬ê¸°ì„œ $\mathbb{1}$ì€ ì§€ì‹œí•¨ìˆ˜ (ì¡°ê±´ì´ ì°¸ì´ë©´ 1, ê±°ì§“ì´ë©´ 0)

**3ë‹¨ê³„**: í•™ìŠµê¸° ê°€ì¤‘ì¹˜ ê³„ì‚°
$$\alpha_t = \frac{1}{2} \ln\left(\frac{1 - \epsilon_t}{\epsilon_t}\right)$$

**í†µì°°**:
- $\epsilon_t < 0.5$: $\alpha_t > 0$ (ì¢‹ì€ ëª¨ë¸ì€ ë†’ì€ ê°€ì¤‘ì¹˜)
- $\epsilon_t = 0.5$: $\alpha_t = 0$ (ë¬´ì‘ìœ„ ì¶”ì¸¡ ìˆ˜ì¤€)
- $\epsilon_t > 0.5$: $\alpha_t < 0$ (ë‚˜ìœ ëª¨ë¸ì€ ìŒì˜ ê°€ì¤‘ì¹˜)

**4ë‹¨ê³„**: ìƒ˜í”Œ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
$$w_i^{(t)} = w_i^{(t-1)} \exp\left(-\alpha_t y_i h_t(x_i)\right)$$

**ì •ê·œí™”**:
$$w_i^{(t)} = \frac{w_i^{(t)}}{\sum_{j=1}^{N} w_j^{(t)}}$$

**ìµœì¢… ì˜ˆì¸¡**:
$$H(x) = \text{sign}\left(\sum_{t=1}^{T} \alpha_t h_t(x)\right)$$

#### ë‹¨ê³„ë³„ ê³„ì‚° ì˜ˆì œ

ê°„ë‹¨í•œ ì˜ˆì œë¡œ AdaBoostë¥¼ ì´í•´í•´ë´…ì‹œë‹¤.

**ë°ì´í„°**: 5ê°œ ìƒ˜í”Œ
| ìƒ˜í”Œ | íŠ¹ì„± | ì‹¤ì œ ë ˆì´ë¸” |
|------|------|------------|
| 1 | xâ‚ | +1 |
| 2 | xâ‚‚ | +1 |
| 3 | xâ‚ƒ | -1 |
| 4 | xâ‚„ | -1 |
| 5 | xâ‚… | +1 |

**ë¼ìš´ë“œ 1**:
- ì´ˆê¸° ê°€ì¤‘ì¹˜: $w_1 = w_2 = w_3 = w_4 = w_5 = 0.2$
- í•™ìŠµê¸° $h_1$ ì˜ˆì¸¡: [+1, +1, +1, -1, -1]
- ì˜¤ë¶„ë¥˜: ìƒ˜í”Œ 3, 5
- ì˜¤ì°¨ìœ¨: $\epsilon_1 = (0.2 + 0.2) / 1.0 = 0.4$
- ëª¨ë¸ ê°€ì¤‘ì¹˜: $\alpha_1 = 0.5 \ln(0.6/0.4) = 0.203$
- ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸:
  - ìƒ˜í”Œ 3: $w_3 = 0.2 \times \exp(0.203) = 0.245$
  - ìƒ˜í”Œ 5: $w_5 = 0.2 \times \exp(0.203) = 0.245$
  - ë‚˜ë¨¸ì§€: $w = 0.2 \times \exp(-0.203) = 0.163$
- ì •ê·œí™” í›„: [0.163, 0.163, 0.245, 0.163, 0.245]

**ë¼ìš´ë“œ 2**:
- ì´ì œ ìƒ˜í”Œ 3, 5ì— ë” ì§‘ì¤‘í•˜ì—¬ í•™ìŠµ
- ê³¼ì • ë°˜ë³µ...

### ğŸ’» ì‹œê°í™” ì½”ë“œ

```python
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import rc
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import AdaBoostClassifier

# í•œê¸€ í°íŠ¸ ì„¤ì •
rc('font', family='DejaVu Sans')

# ë°ì´í„° ìƒì„±
np.random.seed(42)
X, y = make_classification(n_samples=300, n_features=2, n_redundant=0,
                          n_informative=2, n_clusters_per_class=1,
                          flip_y=0.1, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# AdaBoost í•™ìŠµ
ada = AdaBoostClassifier(
    estimator=DecisionTreeClassifier(max_depth=1),
    n_estimators=50,
    learning_rate=1.0,
    random_state=42
)
ada.fit(X_train, y_train)

# ë‹¨ê³„ë³„ í•™ìŠµ ê³¼ì • ì‹œê°í™”
n_estimators_list = [1, 5, 10, 20, 50]
fig, axes = plt.subplots(2, 3, figsize=(15, 10))

for idx, n_est in enumerate(n_estimators_list):
    row = idx // 3
    col = idx % 3
    ax = axes[row, col]
    
    # í•´ë‹¹ ë‹¨ê³„ê¹Œì§€ì˜ ëª¨ë¸
    ada_partial = AdaBoostClassifier(
        estimator=DecisionTreeClassifier(max_depth=1),
        n_estimators=n_est,
        learning_rate=1.0,
        random_state=42
    )
    ada_partial.fit(X_train, y_train)
    
    # ê²°ì • ê²½ê³„
    h = 0.02
    x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1
    y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))
    
    Z = ada_partial.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    
    ax.contourf(xx, yy, Z, alpha=0.4, cmap=plt.cm.RdYlBu)
    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, 
              cmap=plt.cm.RdYlBu, edgecolor='black', s=50, alpha=0.7)
    
    train_acc = ada_partial.score(X_train, y_train)
    test_acc = ada_partial.score(X_test, y_test)
    
    ax.set_title(f'n_estimators = {n_est}', fontsize=12, weight='bold')
    ax.set_xlabel('Feature 1', fontsize=10)
    ax.set_ylabel('Feature 2', fontsize=10)
    ax.text(0.5, 0.05, f'Test Acc: {test_acc:.3f}', 
           transform=ax.transAxes, ha='center', fontsize=10, weight='bold',
           bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.7))

# ë§ˆì§€ë§‰ ì„œë¸Œí”Œë¡¯: í•™ìŠµ ê³¡ì„ 
ax_curve = axes[1, 2]
n_range = range(1, 51)
train_scores = []
test_scores = []

for n in n_range:
    ada_temp = AdaBoostClassifier(
        estimator=DecisionTreeClassifier(max_depth=1),
        n_estimators=n,
        learning_rate=1.0,
        random_state=42
    )
    ada_temp.fit(X_train, y_train)
    train_scores.append(ada_temp.score(X_train, y_train))
    test_scores.append(ada_temp.score(X_test, y_test))

ax_curve.plot(n_range, train_scores, label='Train Accuracy', 
             marker='o', linewidth=2, markersize=4)
ax_curve.plot(n_range, test_scores, label='Test Accuracy', 
             marker='s', linewidth=2, markersize=4)
ax_curve.set_xlabel('Number of Estimators', fontsize=10, weight='bold')
ax_curve.set_ylabel('Accuracy', fontsize=10, weight='bold')
ax_curve.set_title('Learning Curve', fontsize=12, weight='bold')
ax_curve.legend(fontsize=9)
ax_curve.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('adaboost_stages.png', dpi=150, bbox_inches='tight')
plt.show()

# ì˜¤ì°¨ ê°ì†Œ ì‹œê°í™”
plt.figure(figsize=(12, 5))

# ì™¼ìª½: ë‹¨ê³„ë³„ ì˜¤ì°¨ìœ¨
plt.subplot(1, 2, 1)
estimator_errors = ada.estimator_errors_[:30]  # ì²˜ìŒ 30ê°œë§Œ
plt.plot(range(1, len(estimator_errors) + 1), estimator_errors, 
         marker='o', linewidth=2, markersize=6, color='coral')
plt.axhline(y=0.5, color='r', linestyle='--', label='Random Guess', linewidth=2)
plt.xlabel('Iteration', fontsize=12, weight='bold')
plt.ylabel('Weighted Error Rate', fontsize=12, weight='bold')
plt.title('AdaBoost: Error Rate per Iteration', fontsize=14, weight='bold')
plt.legend(fontsize=11)
plt.grid(True, alpha=0.3)

# ì˜¤ë¥¸ìª½: í•™ìŠµê¸° ê°€ì¤‘ì¹˜
plt.subplot(1, 2, 2)
estimator_weights = ada.estimator_weights_[:30]
plt.bar(range(1, len(estimator_weights) + 1), estimator_weights, 
        color='skyblue', edgecolor='black', linewidth=1.5)
plt.xlabel('Iteration', fontsize=12, weight='bold')
plt.ylabel('Estimator Weight (Î±)', fontsize=12, weight='bold')
plt.title('AdaBoost: Learner Weights per Iteration', fontsize=14, weight='bold')
plt.grid(axis='y', alpha=0.3)

plt.tight_layout()
plt.savefig('adaboost_errors_weights.png', dpi=150, bbox_inches='tight')
plt.show()

print("=" * 70)
print("ADABOOST ANALYSIS")
print("=" * 70)
print(f"Number of Estimators: {ada.n_estimators}")
print(f"Training Accuracy: {ada.score(X_train, y_train):.4f}")
print(f"Test Accuracy: {ada.score(X_test, y_test):.4f}")
print(f"\nFirst 10 Estimator Errors:")
for i, error in enumerate(ada.estimator_errors_[:10], 1):
    print(f"  Round {i:2d}: {error:.4f}")
print(f"\nFirst 10 Estimator Weights:")
for i, weight in enumerate(ada.estimator_weights_[:10], 1):
    print(f"  Round {i:2d}: {weight:.4f}")
print("=" * 70)
```

**ê¸°ëŒ€ ì¶œë ¥**:
- ì´ˆê¸°ì—ëŠ” ë‹¨ìˆœí•œ ê²°ì • ê²½ê³„
- ë°˜ë³µì´ ì§„í–‰ë˜ë©´ì„œ ë³µì¡í•´ì§€ê³  ì •í™•í•´ì§
- ì˜¤ì°¨ìœ¨ì´ ì ì§„ì ìœ¼ë¡œ ê°ì†Œ
- ì¢‹ì€ í•™ìŠµê¸°ëŠ” ë†’ì€ ê°€ì¤‘ì¹˜ë¥¼ ë°›ìŒ

### ğŸ’¬ í¼ì‹¤ë¦¬í…Œì´ì…˜ ì§ˆë¬¸

**ì§ˆë¬¸ 1**: "ì™œ ì˜¤ì°¨ê°€ í° ìƒ˜í”Œì˜ ê°€ì¤‘ì¹˜ë¥¼ ë†’ì´ëŠ” ê±¸ê¹Œìš”?"

**ë‹µë³€ ê°€ì´ë“œ**:
- ì˜ëª» ë¶„ë¥˜ëœ ìƒ˜í”Œì€ ì–´ë ¤ìš´ ì¼€ì´ìŠ¤
- ë‹¤ìŒ ëª¨ë¸ì´ ì´ ìƒ˜í”Œì— ì§‘ì¤‘í•˜ë„ë¡ ìœ ë„
- ì ì§„ì ìœ¼ë¡œ ì–´ë ¤ìš´ ë¬¸ì œ í•´ê²°
- ì „ì²´ì ì¸ ì„±ëŠ¥ í–¥ìƒ

**ì§ˆë¬¸ 2**: "AdaBoostê°€ ê³¼ì í•©ë  ìˆ˜ ìˆì„ê¹Œìš”?"

**ë‹µë³€ ê°€ì´ë“œ**:
- ì´ë¡ ì ìœ¼ë¡œ ë¶€ìŠ¤íŒ…ì€ ê³¼ì í•©ì— ê°•í•¨
- í•˜ì§€ë§Œ ë…¸ì´ì¦ˆê°€ ë§ìœ¼ë©´ ê³¼ì í•© ê°€ëŠ¥
- í•™ìŠµë¥ (learning_rate) ì¡°ì •ìœ¼ë¡œ ì™„í™”
- ë„ˆë¬´ ë§ì€ ë°˜ë³µì€ í”¼í•´ì•¼ í•¨

### ğŸ§® ë£¨ë¸Œë¦­ í‰ê°€í‘œ

| í‰ê°€í•­ëª© | ìš°ìˆ˜ (3ì ) | ë³´í†µ (2ì ) | ë¯¸í¡ (1ì ) |
|----------|-----------|-----------|-----------|
| **ì•Œê³ ë¦¬ì¦˜ ì´í•´** | ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ ê³¼ì •ì„ ìˆ˜ì‹ê³¼ í•¨ê»˜ ëª…í™•íˆ ì„¤ëª… | ê°œë…ì€ ì´í•´í•˜ì§€ë§Œ ìˆ˜ì‹ ì„¤ëª… ë¶€ì¡± | ì•Œê³ ë¦¬ì¦˜ ì´í•´ ë¶€ì¡± |
| **í•´ì„ ì •í™•ë„** | ì˜¤ì°¨ìœ¨ê³¼ ê°€ì¤‘ì¹˜ ë³€í™”ë¥¼ ì •í™•íˆ í•´ì„ | ê·¸ë˜í”„ëŠ” ë³´ì§€ë§Œ í•´ì„ì´ í”¼ìƒì  | ê²°ê³¼ í•´ì„ ë¶ˆê°€ |
| **ì½”ë“œ êµ¬í˜„** | AdaBoostClassifierë¥¼ ì˜¬ë°”ë¥´ê²Œ ì‚¬ìš©í•˜ê³  íŒŒë¼ë¯¸í„° ì´í•´ | ì½”ë“œ ì‹¤í–‰ì€ ë˜ì§€ë§Œ íŒŒë¼ë¯¸í„° ì´í•´ ë¶€ì¡± | ì½”ë“œ ì‹¤í–‰ ì‹¤íŒ¨ |

---

## 5ï¸âƒ£ ê·¸ë ˆì´ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ… & XGBoost

### ğŸ“– ì´ë¡  ì„¤ëª…

#### 5.1 ê·¸ë ˆì´ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ…ì˜ ê°œë…

**Gradient Boosting**ì€ ì†ì‹¤ í•¨ìˆ˜ì˜ **ê·¸ë ˆì´ë””ì–¸íŠ¸(ê¸°ìš¸ê¸°)**ë¥¼ ì´ìš©í•˜ì—¬ ë¶€ìŠ¤íŒ…í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.

**í•µì‹¬ ì•„ì´ë””ì–´**:
- ì´ì „ ëª¨ë¸ì˜ **ì”ì°¨(residual)**ë¥¼ ë‹¤ìŒ ëª¨ë¸ì´ í•™ìŠµ
- ê²½ì‚¬í•˜ê°•ë²•ì²˜ëŸ¼ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ìµœì†Œí™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ í•™ìŠµ
- ë§¤ìš° ê°•ë ¥í•˜ì§€ë§Œ ê³¼ì í•© ìœ„í—˜

**AdaBoost vs Gradient Boosting**:
| íŠ¹ì„± | AdaBoost | Gradient Boosting |
|------|----------|------------------|
| ê°€ì¤‘ì¹˜ ì¡°ì • | ìƒ˜í”Œ ê°€ì¤‘ì¹˜ | ì”ì°¨ í•™ìŠµ |
| ì†ì‹¤ í•¨ìˆ˜ | ì§€ìˆ˜ ì†ì‹¤ | ì„ì˜ì˜ ë¯¸ë¶„ ê°€ëŠ¥ ì†ì‹¤ |
| ìœ ì—°ì„± | ë¶„ë¥˜ì— íŠ¹í™” | ë¶„ë¥˜/íšŒê·€ ëª¨ë‘ ê°€ëŠ¥ |

#### 5.2 ê·¸ë ˆì´ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ… ì•Œê³ ë¦¬ì¦˜

**ê³¼ì •**:
1. ì´ˆê¸° ì˜ˆì¸¡ $F_0(x)$ ì„¤ì • (ë³´í†µ í‰ê· ê°’)
2. í˜„ì¬ ëª¨ë¸ì˜ ì”ì°¨ ê³„ì‚°: $r_i = y_i - F_{m-1}(x_i)$
3. ì”ì°¨ë¥¼ íƒ€ê²Ÿìœ¼ë¡œ ìƒˆë¡œìš´ ëª¨ë¸ $h_m$ í•™ìŠµ
4. ëª¨ë¸ ì—…ë°ì´íŠ¸: $F_m(x) = F_{m-1}(x) + \nu \cdot h_m(x)$
5. 2-4 ë°˜ë³µ

ì—¬ê¸°ì„œ $\nu$ëŠ” í•™ìŠµë¥ (learning rate)

#### 5.3 XGBoost (eXtreme Gradient Boosting)

XGBoostëŠ” Gradient Boostingì˜ **ìµœì í™”ëœ êµ¬í˜„**ì…ë‹ˆë‹¤.

**ì£¼ìš” ê°œì„ ì **:
- **ì •ê·œí™”**: L1, L2 ì •ê·œí™”ë¡œ ê³¼ì í•© ë°©ì§€
- **ë³‘ë ¬ ì²˜ë¦¬**: íŠ¸ë¦¬ êµ¬ì¶• ì‹œ ë³‘ë ¬í™”ë¡œ ì†ë„ í–¥ìƒ
- **ê²°ì¸¡ì¹˜ ì²˜ë¦¬**: ìë™ìœ¼ë¡œ ìµœì ì˜ ë°©í–¥ í•™ìŠµ
- **ê°€ì§€ì¹˜ê¸°**: ì†ì‹¤ ê°ì†Œê°€ ì—†ìœ¼ë©´ ë¶„í•  ì¤‘ë‹¨
- **ì¡°ê¸° ì¢…ë£Œ**: ê²€ì¦ ì„±ëŠ¥ì´ ì•…í™”ë˜ë©´ í•™ìŠµ ì¤‘ë‹¨

#### 5.4 í•™ìŠµë¥ ì˜ ì—­í• 

**í•™ìŠµë¥ (Learning Rate, $\nu$)**:
- ê° íŠ¸ë¦¬ì˜ ê¸°ì—¬ë„ ì¡°ì ˆ
- ë‚®ì€ í•™ìŠµë¥ : ì²œì²œíˆ í•™ìŠµ, ë” ë§ì€ íŠ¸ë¦¬ í•„ìš”, ê³¼ì í•© ë°©ì§€
- ë†’ì€ í•™ìŠµë¥ : ë¹ ë¥´ê²Œ í•™ìŠµ, ì ì€ íŠ¸ë¦¬, ê³¼ì í•© ìœ„í—˜

**ì¼ë°˜ì ì¸ ì „ëµ**:
- $\nu = 0.1$: ì¤‘ê°„ ì†ë„, ê· í˜•ì¡íŒ ì„±ëŠ¥
- $\nu = 0.01$: ëŠë¦° í•™ìŠµ, ë†’ì€ ì •í™•ë„
- íŠ¸ë¦¬ ìˆ˜ì™€ ë°˜ë¹„ë¡€: í•™ìŠµë¥  â†“ â†’ íŠ¸ë¦¬ ìˆ˜ â†‘

### ğŸ”¢ ìˆ˜ì‹

#### ê·¸ë ˆì´ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ… ìˆ˜ì‹

**ëª©í‘œ**: ì†ì‹¤ í•¨ìˆ˜ $L$ì„ ìµœì†Œí™”í•˜ëŠ” í•¨ìˆ˜ $F$ë¥¼ ì°¾ê¸°

$$F^* = \arg\min_F \sum_{i=1}^{N} L(y_i, F(x_i))$$

**ì´ˆê¸°í™”**:
$$F_0(x) = \arg\min_\gamma \sum_{i=1}^{N} L(y_i, \gamma)$$

íšŒê·€ ë¬¸ì œì—ì„œ í‰ê· ì œê³±ì˜¤ì°¨ ì‚¬ìš© ì‹œ: $F_0(x) = \bar{y}$

**ë°˜ë³µ** (m = 1, 2, ..., M):

**1ë‹¨ê³„**: ìŒì˜ ê·¸ë ˆì´ë””ì–¸íŠ¸ (ì˜ì‚¬ ì”ì°¨) ê³„ì‚°
$$r_{im} = -\left[\frac{\partial L(y_i, F(x_i))}{\partial F(x_i)}\right]_{F=F_{m-1}}$$

í‰ê· ì œê³±ì˜¤ì°¨ì˜ ê²½ìš°: $r_{im} = y_i - F_{m-1}(x_i)$

**2ë‹¨ê³„**: ì”ì°¨ì— ëŒ€í•´ íšŒê·€ íŠ¸ë¦¬ $h_m$ í•™ìŠµ
$$h_m = \arg\min_h \sum_{i=1}^{N} (r_{im} - h(x_i))^2$$

**3ë‹¨ê³„**: ëª¨ë¸ ì—…ë°ì´íŠ¸
$$F_m(x) = F_{m-1}(x) + \nu \cdot h_m(x)$$

**ìµœì¢… ëª¨ë¸**:
$$F_M(x) = F_0(x) + \nu \sum_{m=1}^{M} h_m(x)$$

#### XGBoost ëª©ì  í•¨ìˆ˜

XGBoostëŠ” ë‹¤ìŒì„ ìµœì†Œí™”:

$$\text{Obj}^{(t)} = \sum_{i=1}^{N} L(y_i, \hat{y}_i^{(t)}) + \sum_{k=1}^{t} \Omega(f_k)$$

ì—¬ê¸°ì„œ:
- $L$: ì†ì‹¤ í•¨ìˆ˜
- $\Omega(f)$: ì •ê·œí™” í•­ (ë³µì¡ë„ ì œì–´)
- $\Omega(f) = \gamma T + \frac{1}{2}\lambda \sum_{j=1}^{T} w_j^2$
  - $T$: ë¦¬í”„ ë…¸ë“œ ìˆ˜
  - $w_j$: ë¦¬í”„ jì˜ ê°€ì¤‘ì¹˜
  - $\gamma$, $\lambda$: ì •ê·œí™” íŒŒë¼ë¯¸í„°

### ğŸ’» ì‹œê°í™” ì½”ë“œ

```python
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import rc
from sklearn.datasets import make_regression, make_classification
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor
from sklearn.tree import DecisionTreeRegressor
try:
    from xgboost import XGBClassifier
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False
    print("Warning: XGBoost not installed. Some visualizations will be skipped.")

# í•œê¸€ í°íŠ¸ ì„¤ì •
rc('font', family='DejaVu Sans')

# ===== íšŒê·€ ë¬¸ì œ: ì”ì°¨ í•™ìŠµ ì‹œê°í™” =====
np.random.seed(42)
X_reg = np.sort(5 * np.random.rand(100, 1), axis=0)
y_reg = np.sin(X_reg).ravel() + np.random.normal(0, 0.1, X_reg.shape[0])

# ê°„ë‹¨í•œ ê·¸ë ˆì´ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ… êµ¬í˜„ (ì„¤ëª…ìš©)
n_estimators = 5
learning_rate = 0.5

# ì´ˆê¸° ì˜ˆì¸¡: í‰ê· 
predictions = np.full(len(y_reg), np.mean(y_reg))

fig, axes = plt.subplots(2, 3, figsize=(15, 10))

for i in range(n_estimators):
    row = i // 3
    col = i % 3
    ax = axes[row, col]
    
    # ì”ì°¨ ê³„ì‚°
    residuals = y_reg - predictions
    
    # ì”ì°¨ì— ëŒ€í•´ íŠ¸ë¦¬ í•™ìŠµ
    tree = DecisionTreeRegressor(max_depth=3)
    tree.fit(X_reg, residuals)
    
    # ì˜ˆì¸¡ ì—…ë°ì´íŠ¸
    predictions += learning_rate * tree.predict(X_reg)
    
    # ì‹œê°í™”
    X_plot = np.linspace(0, 5, 100).reshape(-1, 1)
    y_plot = np.full(len(X_plot), np.mean(y_reg))
    
    # í˜„ì¬ê¹Œì§€ì˜ ì˜ˆì¸¡
    temp_pred = np.full(len(X_plot), np.mean(y_reg))
    for j in range(i + 1):
        if j == 0:
            residuals_temp = y_reg - temp_pred[:len(y_reg)]
            tree_temp = DecisionTreeRegressor(max_depth=3, random_state=42+j)
            tree_temp.fit(X_reg, residuals_temp)
            temp_pred += learning_rate * tree_temp.predict(X_plot)
        else:
            # ì¬í•™ìŠµ ê³¼ì •
            pass
    
    ax.scatter(X_reg, y_reg, alpha=0.5, s=30, label='Data')
    ax.plot(X_plot, temp_pred, 'r-', linewidth=2, label=f'Prediction (step {i+1})')
    ax.plot(X_reg, predictions, 'g.', markersize=8, label='Current fit', alpha=0.7)
    ax.set_title(f'Boosting Round {i+1}', fontsize=12, weight='bold')
    ax.set_xlabel('X', fontsize=10)
    ax.set_ylabel('y', fontsize=10)
    ax.legend(fontsize=8)
    ax.grid(True, alpha=0.3)

# ë§ˆì§€ë§‰ ì„œë¸Œí”Œë¡¯: ìµœì¢… ê²°ê³¼
ax = axes[1, 2]
gb_reg = GradientBoostingRegressor(n_estimators=50, learning_rate=0.1, 
                                   max_depth=3, random_state=42)
gb_reg.fit(X_reg, y_reg)
X_plot = np.linspace(0, 5, 100).reshape(-1, 1)
y_plot = gb_reg.predict(X_plot)

ax.scatter(X_reg, y_reg, alpha=0.5, s=30, label='Data')
ax.plot(X_plot, y_plot, 'r-', linewidth=2, label='GB (50 estimators)')
ax.plot(X_plot, np.sin(X_plot), 'g--', linewidth=2, label='True function', alpha=0.7)
ax.set_title('Final Gradient Boosting Model', fontsize=12, weight='bold')
ax.set_xlabel('X', fontsize=10)
ax.set_ylabel('y', fontsize=10)
ax.legend(fontsize=9)
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('gradient_boosting_residuals.png', dpi=150, bbox_inches='tight')
plt.show()

# ===== ë¶„ë¥˜ ë¬¸ì œ: GB vs XGBoost ë¹„êµ =====
X_clf, y_clf = make_classification(n_samples=1000, n_features=20, 
                                   n_informative=15, n_redundant=5,
                                   random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X_clf, y_clf, 
                                                    test_size=0.3, random_state=42)

# í•™ìŠµë¥  íš¨ê³¼ ë¶„ì„
learning_rates = [0.01, 0.05, 0.1, 0.5, 1.0]
colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A', '#98D8C8']

plt.figure(figsize=(14, 6))

# ì™¼ìª½: í•™ìŠµë¥ ë³„ ì„±ëŠ¥
plt.subplot(1, 2, 1)
for lr, color in zip(learning_rates, colors):
    gb = GradientBoostingClassifier(n_estimators=100, learning_rate=lr, 
                                   max_depth=3, random_state=42)
    gb.fit(X_train, y_train)
    
    # í•™ìŠµ ê³¼ì •ì˜ ëˆ„ì  ì ìˆ˜
    train_scores = []
    test_scores = []
    for i, (train_pred, test_pred) in enumerate(zip(
        gb.staged_predict(X_train), gb.staged_predict(X_test))):
        train_scores.append(np.mean(train_pred == y_train))
        test_scores.append(np.mean(test_pred == y_test))
    
    plt.plot(range(1, len(test_scores) + 1), test_scores, 
            label=f'LR = {lr}', linewidth=2, color=color)

plt.xlabel('Number of Estimators', fontsize=12, weight='bold')
plt.ylabel('Test Accuracy', fontsize=12, weight='bold')
plt.title('Effect of Learning Rate on GB Performance', fontsize=14, weight='bold')
plt.legend(fontsize=10)
plt.grid(True, alpha=0.3)

# ì˜¤ë¥¸ìª½: GB vs XGBoost ë¹„êµ
plt.subplot(1, 2, 2)

gb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, 
                               max_depth=3, random_state=42)
gb.fit(X_train, y_train)

models = [('Gradient Boosting', gb)]
model_names = ['GB']
train_accs = [gb.score(X_train, y_train)]
test_accs = [gb.score(X_test, y_test)]

if XGBOOST_AVAILABLE:
    xgb = XGBClassifier(n_estimators=100, learning_rate=0.1, 
                       max_depth=3, random_state=42, use_label_encoder=False,
                       eval_metric='logloss')
    xgb.fit(X_train, y_train)
    models.append(('XGBoost', xgb))
    model_names.append('XGB')
    train_accs.append(xgb.score(X_train, y_train))
    test_accs.append(xgb.score(X_test, y_test))

x = np.arange(len(model_names))
width = 0.35

bars1 = plt.bar(x - width/2, train_accs, width, label='Train', 
               color='skyblue', edgecolor='black', linewidth=1.5)
bars2 = plt.bar(x + width/2, test_accs, width, label='Test', 
               color='coral', edgecolor='black', linewidth=1.5)

# ê°’ í‘œì‹œ
for bars in [bars1, bars2]:
    for bar in bars:
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.3f}',
                ha='center', va='bottom', fontsize=10, weight='bold')

plt.xlabel('Model', fontsize=12, weight='bold')
plt.ylabel('Accuracy', fontsize=12, weight='bold')
plt.title('Gradient Boosting vs XGBoost', fontsize=14, weight='bold')
plt.xticks(x, model_names)
plt.legend(fontsize=10)
plt.ylim([0.85, 1.0])
plt.grid(axis='y', alpha=0.3)

plt.tight_layout()
plt.savefig('gradient_boosting_xgboost.png', dpi=150, bbox_inches='tight')
plt.show()

# íŠ¹ì„± ì¤‘ìš”ë„
plt.figure(figsize=(10, 6))
feature_importance = gb.feature_importances_
sorted_idx = np.argsort(feature_importance)[-10:]  # ìƒìœ„ 10ê°œ

plt.barh(range(len(sorted_idx)), feature_importance[sorted_idx], 
        color='teal', edgecolor='black', linewidth=1.5)
plt.yticks(range(len(sorted_idx)), [f'Feature {i}' for i in sorted_idx])
plt.xlabel('Importance', fontsize=12, weight='bold')
plt.title('Top 10 Feature Importances (Gradient Boosting)', fontsize=14, weight='bold')
plt.grid(axis='x', alpha=0.3)
plt.tight_layout()
plt.savefig('feature_importance.png', dpi=150, bbox_inches='tight')
plt.show()

print("=" * 70)
print("GRADIENT BOOSTING & XGBOOST COMPARISON")
print("=" * 70)
print(f"Gradient Boosting:")
print(f"  Train Accuracy: {gb.score(X_train, y_train):.4f}")
print(f"  Test Accuracy:  {gb.score(X_test, y_test):.4f}")
if XGBOOST_AVAILABLE:
    print(f"\nXGBoost:")
    print(f"  Train Accuracy: {xgb.score(X_train, y_train):.4f}")
    print(f"  Test Accuracy:  {xgb.score(X_test, y_test):.4f}")
print("\nLearning Rate Effects:")
for lr in learning_rates:
    gb_temp = GradientBoostingClassifier(n_estimators=100, learning_rate=lr, 
                                        max_depth=3, random_state=42)
    gb_temp.fit(X_train, y_train)
    print(f"  LR = {lr:4.2f}: Test Acc = {gb_temp.score(X_test, y_test):.4f}")
print("=" * 70)
```

**ê¸°ëŒ€ ì¶œë ¥**:
- ì”ì°¨ í•™ìŠµ ê³¼ì •ì˜ ë‹¨ê³„ë³„ ì‹œê°í™”
- í•™ìŠµë¥ ì— ë”°ë¥¸ ì„±ëŠ¥ ë³€í™” ê³¡ì„ 
- GBì™€ XGBoostì˜ ì„±ëŠ¥ ë¹„êµ
- íŠ¹ì„± ì¤‘ìš”ë„ ì‹œê°í™”

### ğŸ’¬ í¼ì‹¤ë¦¬í…Œì´ì…˜ ì§ˆë¬¸

**ì§ˆë¬¸ 1**: "í•™ìŠµë¥ ì´ ë„ˆë¬´ í¬ê±°ë‚˜ ì‘ìœ¼ë©´ ì–´ë–¤ ë¬¸ì œê°€ ìƒê¸¸ê¹Œìš”?"

**ë‹µë³€ ê°€ì´ë“œ**:
- **ë„ˆë¬´ í° í•™ìŠµë¥  (ì˜ˆ: 1.0)**:
  - ë¹ ë¥´ê²Œ ìˆ˜ë ´í•˜ì§€ë§Œ ê³¼ì í•© ìœ„í—˜
  - ìµœì ì ì„ ë„˜ì–´ì„¤ ìˆ˜ ìˆìŒ
  - ë¶ˆì•ˆì •í•œ í•™ìŠµ
- **ë„ˆë¬´ ì‘ì€ í•™ìŠµë¥  (ì˜ˆ: 0.001)**:
  - ì²œì²œíˆ ìˆ˜ë ´, ë§¤ìš° ë§ì€ íŠ¸ë¦¬ í•„ìš”
  - í•™ìŠµ ì‹œê°„ ì¦ê°€
  - ê³¼ì†Œì í•© ìœ„í—˜
- **ì ì ˆí•œ ê· í˜•** (ì˜ˆ: 0.1): ì•ˆì •ì ì´ê³  íš¨ìœ¨ì 

**ì§ˆë¬¸ 2**: "XGBoostê°€ ì¼ë°˜ Gradient Boostingë³´ë‹¤ ì¢‹ì€ ì´ìœ ëŠ”?"

**ë‹µë³€ ê°€ì´ë“œ**:
- ì •ê·œí™”ë¡œ ê³¼ì í•© ë°©ì§€
- ë³‘ë ¬ ì²˜ë¦¬ë¡œ ì†ë„ í–¥ìƒ
- ê²°ì¸¡ì¹˜ ìë™ ì²˜ë¦¬
- ì¡°ê¸° ì¢…ë£Œë¡œ íš¨ìœ¨ì„± ì¦ê°€
- ëŒ€ê·œëª¨ ë°ì´í„°ì— ì í•©

### ğŸ§® ë£¨ë¸Œë¦­ í‰ê°€í‘œ

| í‰ê°€í•­ëª© | ìš°ìˆ˜ (3ì ) | ë³´í†µ (2ì ) | ë¯¸í¡ (1ì ) |
|----------|-----------|-----------|-----------|
| **ëª¨ë¸ ë¹„êµ ë¶„ì„** | GBì™€ XGBoostì˜ ì°¨ì´ë¥¼ ì´ë¡ ê³¼ ì‹¤í—˜ìœ¼ë¡œ ëª…í™•íˆ ì„¤ëª… | ê¸°ë³¸ ì°¨ì´ëŠ” ì•Œì§€ë§Œ ê¹Šì€ ë¶„ì„ ë¶€ì¡± | ì°¨ì´ì  ì´í•´ ë¶€ì¡± |
| **ì„¤ëª… ëª…í™•ì„±** | ì”ì°¨ í•™ìŠµê³¼ í•™ìŠµë¥ ì˜ ì—­í• ì„ ëª…í™•íˆ ì„¤ëª… | ê°œë…ì€ ì´í•´í•˜ì§€ë§Œ ì„¤ëª…ì´ ë¶ˆì™„ì „ | í•µì‹¬ ê°œë… ì´í•´ ë¶€ì¡± |
| **íŒŒë¼ë¯¸í„° íŠœë‹** | í•™ìŠµë¥ ê³¼ íŠ¸ë¦¬ ìˆ˜ì˜ ê´€ê³„ë¥¼ ì´í•´í•˜ê³  ìµœì ê°’ íƒìƒ‰ | ê¸°ë³¸ íŒŒë¼ë¯¸í„°ë§Œ ì‚¬ìš© | íŒŒë¼ë¯¸í„° ì˜ë¯¸ ëª¨ë¦„ |

---

## 6ï¸âƒ£ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ë° ë¹„êµ

### ğŸ“– ì´ë¡  ì„¤ëª…

#### 6.1 ì•™ìƒë¸” ë°©ì‹ë³„ íŠ¹ì§• ìš”ì•½

ê° ì•™ìƒë¸” ë°©ì‹ì€ ì„œë¡œ ë‹¤ë¥¸ ê°•ì ê³¼ ì•½ì ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.

| ì•™ìƒë¸” ë°©ì‹ | ì£¼ìš” ëª©ì  | ì¥ì  | ë‹¨ì  | ì í•©í•œ ìƒí™© |
|------------|----------|------|------|-----------|
| **Voting** | ë‹¤ì–‘ì„± í™œìš© | ê°„ë‹¨, í•´ì„ ê°€ëŠ¥ | ëª¨ë¸ ì„ íƒì— ì˜ì¡´ | ì„œë¡œ ë‹¤ë¥¸ ì•Œê³ ë¦¬ì¦˜ ì¡°í•© |
| **Bagging** | ë¶„ì‚° ê°ì†Œ | ê³¼ì í•© ë°©ì§€, ë³‘ë ¬ ê°€ëŠ¥ | í¸í–¥ ê°ì†Œ ì–´ë ¤ì›€ | ê³ ë¶„ì‚° ëª¨ë¸(ê¹Šì€ íŠ¸ë¦¬) |
| **Random Forest** | ë¶„ì‚° ê°ì†Œ + ë‹¤ì–‘ì„± | ì•ˆì •ì , íŠ¹ì„± ì¤‘ìš”ë„ | í•´ì„ë ¥ ë‚®ìŒ | ë²”ìš©ì  ì‚¬ìš© |
| **AdaBoost** | í¸í–¥ ê°ì†Œ | ì•½í•œ í•™ìŠµê¸° ê°•í™” | ë…¸ì´ì¦ˆì— ë¯¼ê° | ë‹¨ìˆœ ëª¨ë¸ ê°œì„  |
| **Gradient Boosting** | í¸í–¥ ê°ì†Œ | ë†’ì€ ì •í™•ë„ | ê³¼ì í•© ìœ„í—˜, ëŠë¦¼ | ì •í™•ë„ ìµœìš°ì„  |
| **XGBoost** | í¸í–¥ ê°ì†Œ + ì •ê·œí™” | ë§¤ìš° ë†’ì€ ì„±ëŠ¥, ë¹ ë¦„ | í•˜ì´í¼íŒŒë¼ë¯¸í„° ë§ìŒ | ëŒ€ê·œëª¨ ë°ì´í„°, ê²½ì§„ëŒ€íšŒ |

#### 6.2 í‰ê°€ ì§€í‘œ

**ë¶„ë¥˜ ë¬¸ì œ**:
- **ì •í™•ë„(Accuracy)**: ì „ì²´ ì¤‘ ë§ì¶˜ ë¹„ìœ¨
- **ì •ë°€ë„(Precision)**: ì–‘ì„± ì˜ˆì¸¡ ì¤‘ ì‹¤ì œ ì–‘ì„± ë¹„ìœ¨
- **ì¬í˜„ìœ¨(Recall)**: ì‹¤ì œ ì–‘ì„± ì¤‘ ì–‘ì„±ìœ¼ë¡œ ì˜ˆì¸¡í•œ ë¹„ìœ¨
- **F1 ì ìˆ˜**: ì •ë°€ë„ì™€ ì¬í˜„ìœ¨ì˜ ì¡°í™”í‰ê· 
- **ROC-AUC**: ê±°ì§“ì–‘ì„±ë¥  ëŒ€ë¹„ ì°¸ì–‘ì„±ë¥ 

**íšŒê·€ ë¬¸ì œ**:
- **MSE (Mean Squared Error)**: í‰ê·  ì œê³± ì˜¤ì°¨
- **RMSE (Root Mean Squared Error)**: MSEì˜ ì œê³±ê·¼
- **MAE (Mean Absolute Error)**: í‰ê·  ì ˆëŒ€ ì˜¤ì°¨
- **RÂ² ì ìˆ˜**: ê²°ì •ê³„ìˆ˜ (1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ìŒ)

#### 6.3 ëª¨ë¸ ì„ íƒ ê°€ì´ë“œ

**í”„ë¡œì íŠ¸ ìš”êµ¬ì‚¬í•­ì— ë”°ë¥¸ ì„ íƒ**:

1. **í•´ì„ ê°€ëŠ¥ì„±ì´ ì¤‘ìš”**: Voting, Single Tree
2. **ë†’ì€ ì •í™•ë„ í•„ìš”**: XGBoost, Gradient Boosting
3. **ë¹ ë¥¸ í•™ìŠµ í•„ìš”**: Random Forest (ë³‘ë ¬), Voting
4. **ì ì€ ë°ì´í„°**: Bagging, Random Forest
5. **ë§ì€ ë°ì´í„°**: XGBoost, Gradient Boosting
6. **ë…¸ì´ì¦ˆê°€ ë§ìŒ**: Random Forest, Bagging
7. **ë¶ˆê· í˜• ë°ì´í„°**: ê°€ì¤‘ì¹˜ ì¡°ì • + XGBoost

### ğŸ”¢ ìˆ˜ì‹

#### êµì°¨ ê²€ì¦ ì ìˆ˜

K-Fold êµì°¨ ê²€ì¦ì˜ í‰ê·  ì ìˆ˜:

$$\text{CV Score} = \frac{1}{K} \sum_{k=1}^{K} \text{Score}_k$$

ì—¬ê¸°ì„œ:
- $K$: í´ë“œ ìˆ˜ (ì¼ë°˜ì ìœ¼ë¡œ 5 ë˜ëŠ” 10)
- $\text{Score}_k$: kë²ˆì§¸ í´ë“œì˜ ì„±ëŠ¥ ì ìˆ˜

#### F1 ì ìˆ˜

ì •ë°€ë„ì™€ ì¬í˜„ìœ¨ì˜ ì¡°í™”í‰ê· :

$$F1 = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}$$

$$\text{Precision} = \frac{TP}{TP + FP}, \quad \text{Recall} = \frac{TP}{TP + FN}$$

ì—¬ê¸°ì„œ:
- TP (True Positive): ì°¸ ì–‘ì„±
- FP (False Positive): ê±°ì§“ ì–‘ì„±
- FN (False Negative): ê±°ì§“ ìŒì„±

### ğŸ’» ì‹œê°í™” ì½”ë“œ

```python
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import rc
import pandas as pd
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.ensemble import (VotingClassifier, BaggingClassifier, 
                             RandomForestClassifier, AdaBoostClassifier, 
                             GradientBoostingClassifier)
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import (accuracy_score, precision_score, recall_score, 
                            f1_score, roc_auc_score, classification_report,
                            confusion_matrix)
import time

try:
    from xgboost import XGBClassifier
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False

# í•œê¸€ í°íŠ¸ ì„¤ì •
rc('font', family='DejaVu Sans')

# ë°ì´í„° ë¡œë“œ
data = load_breast_cancer()
X, y = data.data, data.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, 
                                                    random_state=42, stratify=y)

# ëª¨ë¸ ì •ì˜
models = {
    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),
    'Decision Tree': DecisionTreeClassifier(random_state=42),
    'Voting': VotingClassifier(
        estimators=[
            ('lr', LogisticRegression(random_state=42, max_iter=1000)),
            ('dt', DecisionTreeClassifier(random_state=42)),
            ('svm', SVC(probability=True, random_state=42))
        ],
        voting='soft'
    ),
    'Bagging': BaggingClassifier(
        estimator=DecisionTreeClassifier(),
        n_estimators=50,
        random_state=42
    ),
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
    'AdaBoost': AdaBoostClassifier(n_estimators=50, random_state=42),
    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, 
                                                    random_state=42)
}

if XGBOOST_AVAILABLE:
    models['XGBoost'] = XGBClassifier(n_estimators=100, random_state=42,
                                     use_label_encoder=False, eval_metric='logloss')

# ì„±ëŠ¥ í‰ê°€
results = {
    'Model': [],
    'Train Acc': [],
    'Test Acc': [],
    'Precision': [],
    'Recall': [],
    'F1': [],
    'ROC-AUC': [],
    'Train Time (s)': []
}

for name, model in models.items():
    print(f"Training {name}...")
    
    # í•™ìŠµ ì‹œê°„ ì¸¡ì •
    start_time = time.time()
    model.fit(X_train, y_train)
    train_time = time.time() - start_time
    
    # ì˜ˆì¸¡
    y_pred = model.predict(X_test)
    y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None
    
    # ë©”íŠ¸ë¦­ ê³„ì‚°
    results['Model'].append(name)
    results['Train Acc'].append(accuracy_score(y_train, model.predict(X_train)))
    results['Test Acc'].append(accuracy_score(y_test, y_pred))
    results['Precision'].append(precision_score(y_test, y_pred))
    results['Recall'].append(recall_score(y_test, y_pred))
    results['F1'].append(f1_score(y_test, y_pred))
    results['ROC-AUC'].append(roc_auc_score(y_test, y_proba) if y_proba is not None else 0)
    results['Train Time (s)'].append(train_time)

# ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ
df_results = pd.DataFrame(results)
print("\n" + "=" * 100)
print("MODEL PERFORMANCE COMPARISON")
print("=" * 100)
print(df_results.to_string(index=False))
print("=" * 100)

# ì‹œê°í™” 1: ì„±ëŠ¥ ë¹„êµ ë°” ì°¨íŠ¸
fig, axes = plt.subplots(2, 2, figsize=(16, 12))

metrics = ['Test Acc', 'F1', 'ROC-AUC', 'Train Time (s)']
titles = ['Test Accuracy', 'F1 Score', 'ROC-AUC Score', 'Training Time (seconds)']
colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A']

for idx, (metric, title, color) in enumerate(zip(metrics, titles, colors)):
    row = idx // 2
    col = idx % 2
    ax = axes[row, col]
    
    sorted_df = df_results.sort_values(metric, ascending=(metric == 'Train Time (s)'))
    
    bars = ax.barh(sorted_df['Model'], sorted_df[metric], color=color, 
                   edgecolor='black', linewidth=1.5)
    
    # ê°’ í‘œì‹œ
    for bar, value in zip(bars, sorted_df[metric]):
        width = bar.get_width()
        ax.text(width, bar.get_y() + bar.get_height()/2,
               f'{value:.3f}' if metric != 'Train Time (s)' else f'{value:.2f}s',
               ha='left', va='center', fontsize=9, weight='bold', 
               bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.7))
    
    ax.set_xlabel(title, fontsize=11, weight='bold')
    ax.set_title(f'{title} Comparison', fontsize=12, weight='bold')
    ax.grid(axis='x', alpha=0.3)

plt.tight_layout()
plt.savefig('ensemble_performance_comparison.png', dpi=150, bbox_inches='tight')
plt.show()

# ì‹œê°í™” 2: ì •í™•ë„-ì‹œê°„ íŠ¸ë ˆì´ë“œì˜¤í”„
plt.figure(figsize=(10, 8))
scatter = plt.scatter(df_results['Train Time (s)'], df_results['Test Acc'], 
                     s=200, c=df_results['F1'], cmap='viridis', 
                     edgecolor='black', linewidth=2, alpha=0.7)

# ëª¨ë¸ ì´ë¦„ í‘œì‹œ
for idx, row in df_results.iterrows():
    plt.annotate(row['Model'], 
                (row['Train Time (s)'], row['Test Acc']),
                xytext=(5, 5), textcoords='offset points',
                fontsize=10, weight='bold',
                bbox=dict(boxstyle='round,pad=0.5', facecolor='yellow', alpha=0.5))

plt.colorbar(scatter, label='F1 Score')
plt.xlabel('Training Time (seconds)', fontsize=12, weight='bold')
plt.ylabel('Test Accuracy', fontsize=12, weight='bold')
plt.title('Accuracy vs Training Time Trade-off', fontsize=14, weight='bold')
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig('accuracy_time_tradeoff.png', dpi=150, bbox_inches='tight')
plt.show()

# ì‹œê°í™” 3: ë ˆì´ë” ì°¨íŠ¸ (ìƒìœ„ 5ê°œ ëª¨ë¸)
from math import pi

top_models = df_results.nlargest(5, 'Test Acc')

categories = ['Test Acc', 'Precision', 'Recall', 'F1', 'ROC-AUC']
N = len(categories)

angles = [n / float(N) * 2 * pi for n in range(N)]
angles += angles[:1]

fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))

for idx, row in top_models.iterrows():
    values = [row['Test Acc'], row['Precision'], row['Recall'], 
             row['F1'], row['ROC-AUC']]
    values += values[:1]
    
    ax.plot(angles, values, 'o-', linewidth=2, label=row['Model'])
    ax.fill(angles, values, alpha=0.15)

ax.set_xticks(angles[:-1])
ax.set_xticklabels(categories, fontsize=11, weight='bold')
ax.set_ylim(0, 1)
ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])
ax.set_yticklabels(['0.2', '0.4', '0.6', '0.8', '1.0'])
ax.grid(True)
ax.set_title('Top 5 Models - Multi-Metric Comparison', 
            fontsize=14, weight='bold', pad=20)
ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=10)

plt.tight_layout()
plt.savefig('radar_chart_comparison.png', dpi=150, bbox_inches='tight')
plt.show()

# GridSearchCVë¥¼ í™œìš©í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì˜ˆì œ
print("\n" + "=" * 70)
print("HYPERPARAMETER TUNING EXAMPLE (Random Forest)")
print("=" * 70)

param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10]
}

rf = RandomForestClassifier(random_state=42)
grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='accuracy', 
                          n_jobs=-1, verbose=1)
grid_search.fit(X_train, y_train)

print(f"\nBest Parameters: {grid_search.best_params_}")
print(f"Best CV Score: {grid_search.best_score_:.4f}")
print(f"Test Score: {grid_search.score(X_test, y_test):.4f}")

# GridSearch ê²°ê³¼ ì‹œê°í™”
cv_results = pd.DataFrame(grid_search.cv_results_)
plt.figure(figsize=(12, 6))

# n_estimators íš¨ê³¼
for depth in [None, 10, 20, 30]:
    mask = cv_results['param_max_depth'] == depth
    subset = cv_results[mask]
    plt.plot(subset['param_n_estimators'], subset['mean_test_score'], 
            marker='o', label=f'max_depth={depth}', linewidth=2)

plt.xlabel('Number of Estimators', fontsize=12, weight='bold')
plt.ylabel('Mean CV Accuracy', fontsize=12, weight='bold')
plt.title('GridSearch Results: n_estimators vs max_depth', fontsize=14, weight='bold')
plt.legend(fontsize=10)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig('gridsearch_results.png', dpi=150, bbox_inches='tight')
plt.show()

print("=" * 70)
```

**ê¸°ëŒ€ ì¶œë ¥**:
- ëª¨ë“  ì•™ìƒë¸” ë°©ë²•ì˜ ìƒì„¸ ì„±ëŠ¥ ë¹„êµ í‘œ
- ì„±ëŠ¥ ì§€í‘œë³„ ë°” ì°¨íŠ¸
- ì •í™•ë„-ì‹œê°„ íŠ¸ë ˆì´ë“œì˜¤í”„ ì‚°ì ë„
- ìƒìœ„ ëª¨ë¸ì˜ ë ˆì´ë” ì°¨íŠ¸
- GridSearch í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ê²°ê³¼

### ğŸ’¬ í¼ì‹¤ë¦¬í…Œì´ì…˜ ì§ˆë¬¸

**ì§ˆë¬¸**: "ìš°ë¦¬ íŒ€ì´ ì„ íƒí•œ ìµœì ì˜ ì•™ìƒë¸” ëª¨ë¸ì€ ë¬´ì—‡ì´ë©°, ê·¸ ì´ìœ ëŠ”?"

**í† ë¡  ê°€ì´ë“œ**:
1. **í”„ë¡œì íŠ¸ ìš”êµ¬ì‚¬í•­ ë¶„ì„**:
   - ì •í™•ë„ê°€ ìµœìš°ì„ ì¸ê°€?
   - í•´ì„ ê°€ëŠ¥ì„±ì´ í•„ìš”í•œê°€?
   - ì‹¤ì‹œê°„ ì˜ˆì¸¡ì´ í•„ìš”í•œê°€?
   - í•™ìŠµ ì‹œê°„ ì œì•½ì´ ìˆëŠ”ê°€?

2. **ë°ì´í„° íŠ¹ì„± ê³ ë ¤**:
   - ë°ì´í„° í¬ê¸°ëŠ”?
   - ë…¸ì´ì¦ˆ ìˆ˜ì¤€ì€?
   - í´ë˜ìŠ¤ ë¶ˆê· í˜• ì—¬ë¶€?
   - íŠ¹ì„± ê°œìˆ˜ëŠ”?

3. **ì„±ëŠ¥-ë³µì¡ë„ íŠ¸ë ˆì´ë“œì˜¤í”„**:
   - ì•½ê°„ì˜ ì •í™•ë„ í–¥ìƒì„ ìœ„í•´ ë³µì¡ë„ ì¦ê°€ê°€ ì •ë‹¹í™”ë˜ëŠ”ê°€?
   - ìœ ì§€ë³´ìˆ˜ ë¹„ìš©ì„ ê³ ë ¤í–ˆëŠ”ê°€?

4. **ì‹¤í—˜ ê²°ê³¼ ê¸°ë°˜ ì˜ì‚¬ê²°ì •**:
   - êµì°¨ ê²€ì¦ ì ìˆ˜ê°€ ì•ˆì •ì ì¸ê°€?
   - í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ì´ í•™ìŠµ ì„±ëŠ¥ê³¼ ë¹„ìŠ·í•œê°€? (ê³¼ì í•© í™•ì¸)

### ğŸ§® ë£¨ë¸Œë¦­ í‰ê°€í‘œ

| í‰ê°€í•­ëª© | ìš°ìˆ˜ (3ì ) | ë³´í†µ (2ì ) | ë¯¸í¡ (1ì ) |
|----------|-----------|-----------|-----------|
| **ì‹¤í—˜ ì„¤ê³„** | ì²´ê³„ì ì¸ ë¹„êµ ì‹¤í—˜ê³¼ ëª…í™•í•œ í‰ê°€ ê¸°ì¤€ ì œì‹œ | ê¸°ë³¸ ì‹¤í—˜ì€ í–ˆìœ¼ë‚˜ ì¼ë¶€ ë©”íŠ¸ë¦­ ëˆ„ë½ | ì‹¤í—˜ ì„¤ê³„ê°€ ë¹„ì²´ê³„ì ì´ê±°ë‚˜ ë¶ˆì™„ì „ |
| **ê²°ê³¼ í•´ì„** | ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œ ëª¨ë¸ ì„ íƒ ê·¼ê±°ë¥¼ ëª…í™•íˆ ì„¤ëª… | ê²°ê³¼ë¥¼ ë³´ê³ í•˜ì§€ë§Œ í•´ì„ì´ í”¼ìƒì  | ê²°ê³¼ í•´ì„ ì—†ì´ ìˆ˜ì¹˜ë§Œ ë‚˜ì—´ |
| **ëª¨ë¸ ì„ íƒ ì •ë‹¹í™”** | í”„ë¡œì íŠ¸ ìš”êµ¬ì‚¬í•­ê³¼ ì‹¤í—˜ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ìµœì  ëª¨ë¸ ì„ íƒ | ì¼ë¶€ ìš”ì†Œë§Œ ê³ ë ¤í•˜ì—¬ ëª¨ë¸ ì„ íƒ | ê·¼ê±° ì—†ëŠ” ì£¼ê´€ì  ì„ íƒ |
| **í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹** | GridSearch/RandomSearchë¡œ ì²´ê³„ì  íŠœë‹ ìˆ˜í–‰ | ê¸°ë³¸ íŒŒë¼ë¯¸í„°ë§Œ ì‚¬ìš©í•˜ê±°ë‚˜ ìˆ˜ë™ ì¡°ì • | íŒŒë¼ë¯¸í„° íŠœë‹ ì—†ìŒ |
| **ì‹œê°í™” í’ˆì§ˆ** | ë‹¤ì–‘í•œ ê´€ì ì˜ ëª…í™•í•˜ê³  ìœ ìš©í•œ ì‹œê°í™” | ê¸°ë³¸ ì‹œê°í™”ëŠ” ìˆìœ¼ë‚˜ í†µì°°ë ¥ ë¶€ì¡± | ì‹œê°í™” ë¶€ì¡±í•˜ê±°ë‚˜ ë¶€ì ì ˆ |

---

## ğŸ“š í•™ìŠµ ìš”ì•½ ë° ì‹¤ì „ ê°€ì´ë“œ

### âœ… í•µì‹¬ ê°œë… ì²´í¬ë¦¬ìŠ¤íŠ¸

**ê¸°ë³¸ ê°œë…**:
- [ ] ì•™ìƒë¸” í•™ìŠµì˜ ê¸°ë³¸ ì›ë¦¬ ì´í•´
- [ ] í¸í–¥-ë¶„ì‚° íŠ¸ë ˆì´ë“œì˜¤í”„ ê°œë… ì´í•´
- [ ] ë°°ê¹…, ë¶€ìŠ¤íŒ…, ìŠ¤íƒœí‚¹ì˜ ì°¨ì´ì  ì„¤ëª… ê°€ëŠ¥

**ê°œë³„ ì•Œê³ ë¦¬ì¦˜**:
- [ ] Voting: í•˜ë“œ/ì†Œí”„íŠ¸ ë³´íŒ… ì°¨ì´ ì´í•´
- [ ] Bagging: ë¶€íŠ¸ìŠ¤íŠ¸ë© ìƒ˜í”Œë§ ì›ë¦¬ ì´í•´
- [ ] Random Forest: íŠ¹ì„± ëœë¤ ì„ íƒì˜ íš¨ê³¼ ì´í•´
- [ ] AdaBoost: ìƒ˜í”Œ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ ê³¼ì • ì´í•´
- [ ] Gradient Boosting: ì”ì°¨ í•™ìŠµ ê°œë… ì´í•´
- [ ] XGBoost: ì •ê·œí™”ì™€ ìµœì í™” ê¸°ë²• ì´í•´

**ì‹¤ì „ ìŠ¤í‚¬**:
- [ ] scikit-learn ì•™ìƒë¸” ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥
- [ ] í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ìˆ˜í–‰ ê°€ëŠ¥
- [ ] êµì°¨ ê²€ì¦ìœ¼ë¡œ ëª¨ë¸ í‰ê°€ ê°€ëŠ¥
- [ ] ì ì ˆí•œ í‰ê°€ ì§€í‘œ ì„ íƒ ê°€ëŠ¥
- [ ] ì‹œê°í™”ë¥¼ í†µí•œ ê²°ê³¼ í•´ì„ ê°€ëŠ¥

### ğŸ¯ ì‹¤ì „ í”„ë¡œì íŠ¸ ê°€ì´ë“œ

#### ë‹¨ê³„ 1: ë¬¸ì œ ì •ì˜
```python
# 1. ë¬¸ì œ ìœ í˜• í™•ì¸ (ë¶„ë¥˜ vs íšŒê·€)
# 2. í‰ê°€ ì§€í‘œ ê²°ì • (ì •í™•ë„, F1, MSE ë“±)
# 3. ì„±ëŠ¥ ëª©í‘œ ì„¤ì •
```

#### ë‹¨ê³„ 2: ë°ì´í„° ì¤€ë¹„
```python
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)
```

#### ë‹¨ê³„ 3: ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸
```python
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression

# ë‹¨ìˆœ ëª¨ë¸ë¡œ ì‹œì‘
baseline = LogisticRegression()
baseline.fit(X_train, y_train)
baseline_score = baseline.score(X_test, y_test)
print(f"Baseline: {baseline_score:.4f}")
```

#### ë‹¨ê³„ 4: ì•™ìƒë¸” ëª¨ë¸ ì‹¤í—˜
```python
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier

models = {
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42)
}

for name, model in models.items():
    model.fit(X_train, y_train)
    score = model.score(X_test, y_test)
    print(f"{name}: {score:.4f}")
```

#### ë‹¨ê³„ 5: í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
```python
from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 5, 10],
    'learning_rate': [0.01, 0.1, 0.5]
}

grid = GridSearchCV(
    GradientBoostingClassifier(random_state=42),
    param_grid,
    cv=5,
    scoring='accuracy'
)
grid.fit(X_train, y_train)
print(f"Best params: {grid.best_params_}")
print(f"Best score: {grid.best_score_:.4f}")
```

#### ë‹¨ê³„ 6: ìµœì¢… ëª¨ë¸ í‰ê°€
```python
from sklearn.metrics import classification_report, confusion_matrix

best_model = grid.best_estimator_
y_pred = best_model.predict(X_test)

print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))
```

### ğŸ’¡ ì‹¤ë¬´ íŒ

1. **í•­ìƒ ë² ì´ìŠ¤ë¼ì¸ë¶€í„°**:
   - ë‹¨ìˆœ ëª¨ë¸ë¡œ ì‹œì‘
   - ë³µì¡í•œ ëª¨ë¸ì€ ë‚˜ì¤‘ì—

2. **êµì°¨ ê²€ì¦ í•„ìˆ˜**:
   - ë‹¨ì¼ train/test splitì€ ë¶ˆì•ˆì •
   - ìµœì†Œ 5-fold CV ì‚¬ìš©

3. **ê³¼ì í•© ê²½ê³„**:
   - Train/Test ì„±ëŠ¥ ì°¨ì´ ëª¨ë‹ˆí„°ë§
   - ì •ê·œí™” íŒŒë¼ë¯¸í„° ì¡°ì •

4. **íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ìš°ì„ **:
   - ì¢‹ì€ íŠ¹ì„± > ë³µì¡í•œ ëª¨ë¸
   - ë„ë©”ì¸ ì§€ì‹ í™œìš©

5. **ì•™ìƒë¸”ì˜ ì•™ìƒë¸”**:
   - Votingìœ¼ë¡œ ì—¬ëŸ¬ ì•™ìƒë¸” ê²°í•© ê°€ëŠ¥
   - Stackingìœ¼ë¡œ ë©”íƒ€ í•™ìŠµ ê°€ëŠ¥

### ğŸ“– ì¶”ê°€ í•™ìŠµ ìë£Œ

**ì˜¨ë¼ì¸ ë¦¬ì†ŒìŠ¤**:
- [scikit-learn ì•™ìƒë¸” ê°€ì´ë“œ](https://scikit-learn.org/stable/modules/ensemble.html)
- [XGBoost ê³µì‹ ë¬¸ì„œ](https://xgboost.readthedocs.io/)
- [Kaggle ì•™ìƒë¸” íŠœí† ë¦¬ì–¼](https://www.kaggle.com/learn/intro-to-machine-learning)

**ì¶”ì²œ ë…¼ë¬¸**:
- Breiman, L. (1996). "Bagging Predictors"
- Freund, Y., & Schapire, R. E. (1997). "A Decision-Theoretic Generalization of On-Line Learning"
- Friedman, J. H. (2001). "Greedy Function Approximation: A Gradient Boosting Machine"
- Chen, T., & Guestrin, C. (2016). "XGBoost: A Scalable Tree Boosting System"

**ë„ì„œ**:
- "Ensemble Methods: Foundations and Algorithms" - Zhi-Hua Zhou
- "Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow" - AurÃ©lien GÃ©ron

---

## ğŸ“ ì¢…í•© í‰ê°€ ê³¼ì œ

### í”„ë¡œì íŠ¸: ì™€ì¸ í’ˆì§ˆ ì˜ˆì¸¡ ì•™ìƒë¸” ì‹œìŠ¤í…œ

**ëª©í‘œ**: ì™€ì¸ì˜ í™”í•™ì  íŠ¹ì„±ìœ¼ë¡œ í’ˆì§ˆì„ ì˜ˆì¸¡í•˜ëŠ” ìµœì ì˜ ì•™ìƒë¸” ëª¨ë¸ êµ¬ì¶•

**ë°ì´í„°ì…‹**: UCI Wine Quality Dataset (Red Wine)

**ìš”êµ¬ì‚¬í•­**:
1. ìµœì†Œ 5ê°€ì§€ ì•™ìƒë¸” ë°©ë²• ë¹„êµ
2. GridSearchë¡œ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰
3. êµì°¨ ê²€ì¦ìœ¼ë¡œ ì„±ëŠ¥ í‰ê°€
4. ê²°ê³¼ë¥¼ ì‹œê°í™”ë¡œ í‘œí˜„
5. ìµœì¢… ëª¨ë¸ ì„ íƒ ê·¼ê±° ì‘ì„±

**í‰ê°€ ê¸°ì¤€**:
- ì‹¤í—˜ ì„¤ê³„ ì²´ê³„ì„± (30%)
- ì½”ë“œ í’ˆì§ˆ ë° ì™„ì„±ë„ (25%)
- ì‹œê°í™” ëª…í™•ì„± (20%)
- ê²°ê³¼ ë¶„ì„ ë° í•´ì„ (25%)

**ì œì¶œë¬¼**:
- Python ì½”ë“œ (.py ë˜ëŠ” .ipynb)
- ê²°ê³¼ ë¦¬í¬íŠ¸ (Markdown ë˜ëŠ” PDF)
- ì‹œê°í™” ì´ë¯¸ì§€

---

## ğŸ™ ë§ˆë¬´ë¦¬

ì¶•í•˜í•©ë‹ˆë‹¤! ì•™ìƒë¸” í•™ìŠµì˜ í•µì‹¬ ê°œë…ê³¼ ì‹¤ì „ í™œìš©ë²•ì„ ëª¨ë‘ í•™ìŠµí•˜ì…¨ìŠµë‹ˆë‹¤.

**ì—¬ëŸ¬ë¶„ì´ ë°°ìš´ ë‚´ìš©**:
- âœ… ì•™ìƒë¸” í•™ìŠµì˜ ì´ë¡ ì  ë°°ê²½ (í¸í–¥-ë¶„ì‚° íŠ¸ë ˆì´ë“œì˜¤í”„)
- âœ… 5ê°€ì§€ ì£¼ìš” ì•™ìƒë¸” ì•Œê³ ë¦¬ì¦˜ (Voting, Bagging, AdaBoost, GB, XGBoost)
- âœ… ê° ì•Œê³ ë¦¬ì¦˜ì˜ ìˆ˜í•™ì  ì›ë¦¬ì™€ ì‘ë™ ë°©ì‹
- âœ… Pythonìœ¼ë¡œ ì•™ìƒë¸” ëª¨ë¸ êµ¬í˜„ ë° í‰ê°€
- âœ… í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ë° ëª¨ë¸ ì„ íƒ ì „ëµ

**ë‹¤ìŒ ë‹¨ê³„**:
1. ì‹¤ì œ ë°ì´í„°ì…‹ìœ¼ë¡œ í”„ë¡œì íŠ¸ ì§„í–‰
2. Kaggle ê²½ì§„ëŒ€íšŒ ì°¸ì—¬
3. ì•™ìƒë¸”ì˜ ì•™ìƒë¸”(Stacking) í•™ìŠµ
4. ë”¥ëŸ¬ë‹ê³¼ ì•™ìƒë¸” ê²°í•©

**ë§ˆì§€ë§‰ ì¡°ì–¸**:
> "ì™„ë²½í•œ ëª¨ë¸ì€ ì—†ì§€ë§Œ, ë” ë‚˜ì€ ëª¨ë¸ì€ í•­ìƒ ìˆìŠµë‹ˆë‹¤.  
> ê³„ì† ì‹¤í—˜í•˜ê³ , í•™ìŠµí•˜ê³ , ê°œì„ í•˜ì„¸ìš”!"

í–‰ìš´ì„ ë¹•ë‹ˆë‹¤! ğŸš€

---

**ë¬¸ì„œ ì‘ì„± ì •ë³´**:
- ì‘ì„±ì¼: 2024
- ë²„ì „: 1.0
- Python ë²„ì „: 3.10+
- ì£¼ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬: scikit-learn 1.3.0+, matplotlib 3.7.0+, xgboost 2.0.0+
- ì˜ˆìƒ í•™ìŠµ ì‹œê°„: 4ì‹œê°„ 30ë¶„

**ë¼ì´ì„ ìŠ¤**: MIT License
**ê¸°ì—¬**: ê°œì„  ì œì•ˆ ë° ì˜¤ë¥˜ ë³´ê³  í™˜ì˜í•©ë‹ˆë‹¤!

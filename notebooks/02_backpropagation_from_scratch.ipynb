{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 역전파 알고리즘 구현 - 실습 노트북 (중급 단계)\n",
    "\n",
    "이 노트북은 역전파(Backpropagation) 알고리즘을 처음부터 구현하고 검증합니다.\n",
    "\n",
    "## 목차\n",
    "1. 역전파 알고리즘 구현\n",
    "2. 경사하강법 구현\n",
    "3. 완전한 학습 예제\n",
    "4. 그래디언트 검증 (Gradient Checking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 시드 설정 (재현성)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"NumPy version:\", np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 역전파 알고리즘 구현\n",
    "\n",
    "### 핵심 수식\n",
    "\n",
    "**출력층 그래디언트:**\n",
    "$$\\delta^{[L]} = \\frac{\\partial L}{\\partial z^{[L]}}$$\n",
    "\n",
    "**은닉층 그래디언트:**\n",
    "$$\\delta^{[l]} = (W^{[l+1]})^T \\delta^{[l+1]} \\odot f'(z^{[l]})$$\n",
    "\n",
    "**파라미터 그래디언트:**\n",
    "$$\\frac{\\partial L}{\\partial W^{[l]}} = \\delta^{[l]} (a^{[l-1]})^T$$\n",
    "$$\\frac{\\partial L}{\\partial b^{[l]}} = \\delta^{[l]}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 활성화 함수와 미분\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-np.clip(z, -500, 500)))  # 오버플로우 방지\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    s = sigmoid(z)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def relu_derivative(z):\n",
    "    return np.where(z > 0, 1, 0)\n",
    "\n",
    "# 손실 함수\n",
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    \"\"\"이진 교차 엔트로피 손실\"\"\"\n",
    "    epsilon = 1e-10\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return -(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "def binary_cross_entropy_derivative(y_true, y_pred):\n",
    "    \"\"\"이진 교차 엔트로피 손실의 미분 (sigmoid 활성화 함수와 결합시)\"\"\"\n",
    "    return y_pred - y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, W1, b1, W2, b2):\n",
    "    \"\"\"\n",
    "    2층 신경망의 순방향 전파\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : (n_features, n_samples) 입력 데이터\n",
    "    W1 : (n_hidden, n_features) 1층 가중치\n",
    "    b1 : (n_hidden, 1) 1층 편향\n",
    "    W2 : (n_output, n_hidden) 2층 가중치\n",
    "    b2 : (n_output, 1) 2층 편향\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    cache : dict, 중간 계산 값들\n",
    "    \"\"\"\n",
    "    # 1층 (은닉층, ReLU)\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = relu(Z1)\n",
    "    \n",
    "    # 2층 (출력층, Sigmoid)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    \n",
    "    cache = {\n",
    "        'X': X,\n",
    "        'Z1': Z1,\n",
    "        'A1': A1,\n",
    "        'Z2': Z2,\n",
    "        'A2': A2\n",
    "    }\n",
    "    \n",
    "    return A2, cache\n",
    "\n",
    "def backward_propagation(Y, cache, W1, W2):\n",
    "    \"\"\"\n",
    "    2층 신경망의 역전파\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    Y : (1, n_samples) 실제 레이블\n",
    "    cache : dict, 순방향 전파의 중간 값들\n",
    "    W1, W2 : 가중치 행렬\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    grads : dict, 그래디언트들\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]  # 샘플 개수\n",
    "    \n",
    "    X = cache['X']\n",
    "    A1 = cache['A1']\n",
    "    A2 = cache['A2']\n",
    "    Z1 = cache['Z1']\n",
    "    \n",
    "    # 출력층 그래디언트 (Sigmoid + Cross-Entropy)\n",
    "    dZ2 = A2 - Y\n",
    "    \n",
    "    # 출력층 파라미터 그래디언트\n",
    "    dW2 = (1/m) * np.dot(dZ2, A1.T)\n",
    "    db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    \n",
    "    # 은닉층 그래디언트\n",
    "    dZ1 = np.dot(W2.T, dZ2) * relu_derivative(Z1)\n",
    "    \n",
    "    # 은닉층 파라미터 그래디언트\n",
    "    dW1 = (1/m) * np.dot(dZ1, X.T)\n",
    "    db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    \n",
    "    grads = {\n",
    "        'dW1': dW1,\n",
    "        'db1': db1,\n",
    "        'dW2': dW2,\n",
    "        'db2': db2\n",
    "    }\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 수치 예제 검증\n",
    "\n",
    "문서의 예제와 동일한 값으로 역전파를 검증합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서의 예제와 동일한 설정\n",
    "X = np.array([[1.0], [2.0]])\n",
    "Y = np.array([[1.0]])\n",
    "\n",
    "W1 = np.array([[0.5, 0.3],\n",
    "               [0.2, 0.4]])\n",
    "b1 = np.array([[0.1], [0.2]])\n",
    "\n",
    "W2 = np.array([[0.6, 0.7]])\n",
    "b2 = np.array([[0.15]])\n",
    "\n",
    "# 순방향 전파\n",
    "A2, cache = forward_propagation(X, W1, b1, W2, b2)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"순방향 전파 결과\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Z1 = {cache['Z1'].flatten()}\")\n",
    "print(f\"A1 = {cache['A1'].flatten()}\")\n",
    "print(f\"Z2 = {cache['Z2'].flatten()}\")\n",
    "print(f\"A2 (예측) = {A2.flatten()}\")\n",
    "print()\n",
    "\n",
    "# 손실 계산\n",
    "loss = binary_cross_entropy(Y, A2)\n",
    "print(f\"손실 = {loss.flatten()[0]:.4f}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 역전파\n",
    "grads = backward_propagation(Y, cache, W1, W2)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"역전파 결과 (그래디언트)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"출력층:\")\n",
    "print(f\"  dW2 = {grads['dW2']}\")\n",
    "print(f\"  db2 = {grads['db2'].flatten()}\")\n",
    "print()\n",
    "print(\"은닉층:\")\n",
    "print(f\"  dW1 = \\n{grads['dW1']}\")\n",
    "print(f\"  db1 = {grads['db1'].flatten()}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 경사하강법 구현\n",
    "\n",
    "### 수식\n",
    "$$W := W - \\alpha \\frac{\\partial L}{\\partial W}$$\n",
    "$$b := b - \\alpha \\frac{\\partial L}{\\partial b}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(W1, b1, W2, b2, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    경사하강법으로 파라미터 업데이트\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    W1, b1, W2, b2 : 현재 파라미터\n",
    "    grads : dict, 그래디언트\n",
    "    learning_rate : float, 학습률\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    W1, b1, W2, b2 : 업데이트된 파라미터\n",
    "    \"\"\"\n",
    "    W1 = W1 - learning_rate * grads['dW1']\n",
    "    b1 = b1 - learning_rate * grads['db1']\n",
    "    W2 = W2 - learning_rate * grads['dW2']\n",
    "    b2 = b2 - learning_rate * grads['db2']\n",
    "    \n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "# 파라미터 업데이트 예제\n",
    "learning_rate = 0.1\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"파라미터 업데이트 (학습률 = 0.1)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"업데이트 전:\")\n",
    "print(f\"  W2 = {W2}\")\n",
    "print(f\"  b2 = {b2.flatten()}\")\n",
    "\n",
    "W1_new, b1_new, W2_new, b2_new = update_parameters(\n",
    "    W1.copy(), b1.copy(), W2.copy(), b2.copy(), grads, learning_rate\n",
    ")\n",
    "\n",
    "print(\"\\n업데이트 후:\")\n",
    "print(f\"  W2 = {W2_new}\")\n",
    "print(f\"  b2 = {b2_new.flatten()}\")\n",
    "\n",
    "print(\"\\n수동 계산 (W2):\")\n",
    "print(f\"  W2_new = {W2} - {learning_rate} × {grads['dW2']}\")\n",
    "print(f\"         = {W2 - learning_rate * grads['dW2']}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 완전한 학습 예제\n",
    "\n",
    "단순 선형 분류 문제를 학습합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 생성\n",
    "np.random.seed(42)\n",
    "\n",
    "# 클래스 0: 원점 주변\n",
    "X0 = np.random.randn(2, 50) * 0.5 - 1\n",
    "Y0 = np.zeros((1, 50))\n",
    "\n",
    "# 클래스 1: (2, 2) 주변\n",
    "X1 = np.random.randn(2, 50) * 0.5 + 1\n",
    "Y1 = np.ones((1, 50))\n",
    "\n",
    "# 데이터 결합\n",
    "X_train = np.hstack([X0, X1])\n",
    "Y_train = np.hstack([Y0, Y1])\n",
    "\n",
    "# 데이터 섞기\n",
    "shuffle_idx = np.random.permutation(X_train.shape[1])\n",
    "X_train = X_train[:, shuffle_idx]\n",
    "Y_train = Y_train[:, shuffle_idx]\n",
    "\n",
    "print(f\"훈련 데이터 크기: {X_train.shape}\")\n",
    "print(f\"레이블 크기: {Y_train.shape}\")\n",
    "\n",
    "# 데이터 시각화\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_train[0, Y_train[0]==0], X_train[1, Y_train[0]==0], \n",
    "            c='blue', label='Class 0', alpha=0.6)\n",
    "plt.scatter(X_train[0, Y_train[0]==1], X_train[1, Y_train[0]==1], \n",
    "            c='red', label='Class 1', alpha=0.6)\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Training Data')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network(X, Y, n_hidden=4, learning_rate=0.1, num_iterations=1000):\n",
    "    \"\"\"\n",
    "    신경망 학습\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : (n_features, n_samples)\n",
    "    Y : (1, n_samples)\n",
    "    n_hidden : int, 은닉층 뉴런 개수\n",
    "    learning_rate : float\n",
    "    num_iterations : int\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    parameters : dict\n",
    "    losses : list\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    n_features = X.shape[0]\n",
    "    \n",
    "    # 파라미터 초기화 (Xavier initialization)\n",
    "    W1 = np.random.randn(n_hidden, n_features) * np.sqrt(2.0 / n_features)\n",
    "    b1 = np.zeros((n_hidden, 1))\n",
    "    W2 = np.random.randn(1, n_hidden) * np.sqrt(2.0 / n_hidden)\n",
    "    b2 = np.zeros((1, 1))\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        # 순방향 전파\n",
    "        A2, cache = forward_propagation(X, W1, b1, W2, b2)\n",
    "        \n",
    "        # 손실 계산\n",
    "        loss = np.mean(binary_cross_entropy(Y, A2))\n",
    "        losses.append(loss)\n",
    "        \n",
    "        # 역전파\n",
    "        grads = backward_propagation(Y, cache, W1, W2)\n",
    "        \n",
    "        # 파라미터 업데이트\n",
    "        W1, b1, W2, b2 = update_parameters(W1, b1, W2, b2, grads, learning_rate)\n",
    "        \n",
    "        # 진행 상황 출력\n",
    "        if i % 100 == 0:\n",
    "            # 정확도 계산\n",
    "            predictions = (A2 > 0.5).astype(float)\n",
    "            accuracy = np.mean(predictions == Y) * 100\n",
    "            print(f\"반복 {i:4d}: 손실 = {loss:.4f}, 정확도 = {accuracy:.2f}%\")\n",
    "    \n",
    "    parameters = {\n",
    "        'W1': W1, 'b1': b1,\n",
    "        'W2': W2, 'b2': b2\n",
    "    }\n",
    "    \n",
    "    return parameters, losses\n",
    "\n",
    "# 학습 실행\n",
    "print(\"=\" * 70)\n",
    "print(\"신경망 학습 시작\")\n",
    "print(\"=\" * 70)\n",
    "parameters, losses = train_neural_network(X_train, Y_train, \n",
    "                                         n_hidden=4, \n",
    "                                         learning_rate=0.5, \n",
    "                                         num_iterations=1000)\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 곡선 시각화\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses, linewidth=2)\n",
    "plt.xlabel('Iteration', fontsize=12)\n",
    "plt.ylabel('Loss (Cross-Entropy)', fontsize=12)\n",
    "plt.title('Training Loss Over Time', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n초기 손실: {losses[0]:.4f}\")\n",
    "print(f\"최종 손실: {losses[-1]:.4f}\")\n",
    "print(f\"손실 감소: {(losses[0] - losses[-1]) / losses[0] * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결정 경계 시각화\n",
    "def plot_decision_boundary(X, Y, parameters):\n",
    "    \"\"\"결정 경계 시각화\"\"\"\n",
    "    # 그리드 생성\n",
    "    x_min, x_max = X[0, :].min() - 1, X[0, :].max() + 1\n",
    "    y_min, y_max = X[1, :].min() - 1, X[1, :].max() + 1\n",
    "    \n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                         np.linspace(y_min, y_max, 200))\n",
    "    \n",
    "    # 그리드 포인트 예측\n",
    "    X_grid = np.c_[xx.ravel(), yy.ravel()].T\n",
    "    A2, _ = forward_propagation(X_grid, \n",
    "                                parameters['W1'], parameters['b1'],\n",
    "                                parameters['W2'], parameters['b2'])\n",
    "    Z = A2.reshape(xx.shape)\n",
    "    \n",
    "    # 플롯\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.contourf(xx, yy, Z, levels=20, cmap='RdBu', alpha=0.6)\n",
    "    plt.colorbar(label='Probability of Class 1')\n",
    "    \n",
    "    # 데이터 포인트\n",
    "    plt.scatter(X[0, Y[0]==0], X[1, Y[0]==0], \n",
    "               c='blue', edgecolors='k', s=80, label='Class 0')\n",
    "    plt.scatter(X[0, Y[0]==1], X[1, Y[0]==1], \n",
    "               c='red', edgecolors='k', s=80, label='Class 1')\n",
    "    \n",
    "    plt.xlabel('Feature 1', fontsize=12)\n",
    "    plt.ylabel('Feature 2', fontsize=12)\n",
    "    plt.title('Decision Boundary', fontsize=14)\n",
    "    plt.legend(fontsize=11)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "plot_decision_boundary(X_train, Y_train, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 그래디언트 검증 (Gradient Checking)\n",
    "\n",
    "수치적 그래디언트와 역전파 그래디언트를 비교하여 구현이 올바른지 검증합니다.\n",
    "\n",
    "### 수식\n",
    "수치적 그래디언트:\n",
    "$$\\frac{\\partial L}{\\partial \\theta} \\approx \\frac{L(\\theta + \\epsilon) - L(\\theta - \\epsilon)}{2\\epsilon}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check(X, Y, parameters, grads, epsilon=1e-7):\n",
    "    \"\"\"\n",
    "    그래디언트 검증\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X, Y : 입력 데이터와 레이블\n",
    "    parameters : dict, 파라미터\n",
    "    grads : dict, 역전파로 계산된 그래디언트\n",
    "    epsilon : float, 수치 미분을 위한 작은 값\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    difference : float, 상대 오차\n",
    "    \"\"\"\n",
    "    # 파라미터를 벡터로 변환\n",
    "    params_values = []\n",
    "    grads_values = []\n",
    "    \n",
    "    for key in ['W1', 'b1', 'W2', 'b2']:\n",
    "        params_values.append(parameters[key].flatten())\n",
    "        grads_values.append(grads['d' + key].flatten())\n",
    "    \n",
    "    theta = np.concatenate(params_values)\n",
    "    grad_bp = np.concatenate(grads_values)\n",
    "    \n",
    "    # 수치적 그래디언트 계산\n",
    "    num_grad = np.zeros_like(theta)\n",
    "    \n",
    "    for i in range(len(theta)):\n",
    "        # theta + epsilon\n",
    "        theta_plus = theta.copy()\n",
    "        theta_plus[i] += epsilon\n",
    "        \n",
    "        # 파라미터 복원\n",
    "        idx = 0\n",
    "        params_plus = {}\n",
    "        for key in ['W1', 'b1', 'W2', 'b2']:\n",
    "            shape = parameters[key].shape\n",
    "            size = np.prod(shape)\n",
    "            params_plus[key] = theta_plus[idx:idx+size].reshape(shape)\n",
    "            idx += size\n",
    "        \n",
    "        # 손실 계산\n",
    "        A2_plus, _ = forward_propagation(X, params_plus['W1'], params_plus['b1'],\n",
    "                                        params_plus['W2'], params_plus['b2'])\n",
    "        loss_plus = np.mean(binary_cross_entropy(Y, A2_plus))\n",
    "        \n",
    "        # theta - epsilon\n",
    "        theta_minus = theta.copy()\n",
    "        theta_minus[i] -= epsilon\n",
    "        \n",
    "        idx = 0\n",
    "        params_minus = {}\n",
    "        for key in ['W1', 'b1', 'W2', 'b2']:\n",
    "            shape = parameters[key].shape\n",
    "            size = np.prod(shape)\n",
    "            params_minus[key] = theta_minus[idx:idx+size].reshape(shape)\n",
    "            idx += size\n",
    "        \n",
    "        A2_minus, _ = forward_propagation(X, params_minus['W1'], params_minus['b1'],\n",
    "                                         params_minus['W2'], params_minus['b2'])\n",
    "        loss_minus = np.mean(binary_cross_entropy(Y, A2_minus))\n",
    "        \n",
    "        # 수치적 그래디언트\n",
    "        num_grad[i] = (loss_plus - loss_minus) / (2 * epsilon)\n",
    "    \n",
    "    # 상대 오차 계산\n",
    "    numerator = np.linalg.norm(grad_bp - num_grad)\n",
    "    denominator = np.linalg.norm(grad_bp) + np.linalg.norm(num_grad)\n",
    "    difference = numerator / denominator\n",
    "    \n",
    "    return difference, grad_bp, num_grad\n",
    "\n",
    "# 작은 데이터로 테스트\n",
    "X_test = X_train[:, :5]\n",
    "Y_test = Y_train[:, :5]\n",
    "\n",
    "# 순방향 전파\n",
    "A2, cache = forward_propagation(X_test, \n",
    "                                parameters['W1'], parameters['b1'],\n",
    "                                parameters['W2'], parameters['b2'])\n",
    "\n",
    "# 역전파\n",
    "grads = backward_propagation(Y_test, cache, \n",
    "                            parameters['W1'], parameters['W2'])\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"그래디언트 검증 (샘플 몇 개만 사용)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"계산 중... (시간이 걸릴 수 있습니다)\")\n",
    "\n",
    "difference, grad_bp, num_grad = gradient_check(X_test, Y_test, parameters, grads)\n",
    "\n",
    "print(f\"\\n상대 오차: {difference:.10f}\")\n",
    "print()\n",
    "if difference < 1e-7:\n",
    "    print(\"✓ 역전파 구현이 정확합니다!\")\n",
    "elif difference < 1e-5:\n",
    "    print(\"⚠ 역전파가 대체로 정확하지만 확인이 필요할 수 있습니다.\")\n",
    "else:\n",
    "    print(\"✗ 역전파 구현에 문제가 있을 수 있습니다.\")\n",
    "\n",
    "print(\"\\n그래디언트 비교 (처음 5개 값):\")\n",
    "print(f\"  역전파:     {grad_bp[:5]}\")\n",
    "print(f\"  수치적:     {num_grad[:5]}\")\n",
    "print(f\"  차이:       {grad_bp[:5] - num_grad[:5]}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 요약\n",
    "\n",
    "이 노트북에서 다룬 내용:\n",
    "\n",
    "1. **역전파 알고리즘**: 처음부터 구현하고 수치적으로 검증\n",
    "2. **경사하강법**: 파라미터 업데이트 메커니즘\n",
    "3. **완전한 학습**: 실제 데이터로 신경망 학습\n",
    "4. **그래디언트 검증**: 수치적 방법으로 구현 정확도 확인\n",
    "\n",
    "### 주요 결과\n",
    "- 역전파 구현이 정확함을 수치적으로 검증\n",
    "- 신경망이 데이터를 성공적으로 학습\n",
    "- 결정 경계를 시각화하여 학습 결과 확인\n",
    "\n",
    "### 다음 단계\n",
    "다음 노트북에서는 **Adam, RMSprop** 등의 고급 최적화 기법과 **정규화** 기법을 구현합니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

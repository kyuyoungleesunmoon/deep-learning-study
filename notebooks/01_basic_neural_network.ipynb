{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 신경망 학습의 수학적 기초 - 실습 노트북 (기본 단계)\n",
    "\n",
    "이 노트북은 신경망의 기본 개념을 수치 예제와 실행 가능한 코드로 검증합니다.\n",
    "\n",
    "## 목차\n",
    "1. 퍼셉트론 구현\n",
    "2. 활성화 함수 시각화\n",
    "3. 순방향 전파 구현\n",
    "4. 손실 함수 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 한글 폰트 설정 (선택적)\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(\"NumPy version:\", np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 퍼셉트론 (Perceptron)\n",
    "\n",
    "### 수식\n",
    "$$y = f(w^T x + b)$$\n",
    "\n",
    "**기호 설명:**\n",
    "- $x$: 입력 벡터\n",
    "- $w$: 가중치 벡터\n",
    "- $b$: 편향\n",
    "- $f(\\cdot)$: 활성화 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 퍼셉트론 구현\n",
    "def perceptron(x, w, b):\n",
    "    \"\"\"\n",
    "    단순 퍼셉트론\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    x : array-like, 입력 벡터\n",
    "    w : array-like, 가중치 벡터\n",
    "    b : float, 편향\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    y : float, 출력 (0 또는 1)\n",
    "    \"\"\"\n",
    "    z = np.dot(w, x) + b  # 가중합\n",
    "    y = 1 if z > 0 else 0  # 계단 함수\n",
    "    return y, z\n",
    "\n",
    "# 수치 예제 검증\n",
    "x = np.array([2.0, 3.0])\n",
    "w = np.array([0.5, 0.3])\n",
    "b = 0.1\n",
    "\n",
    "y, z = perceptron(x, w, b)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"퍼셉트론 수치 예제\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"입력 x: {x}\")\n",
    "print(f\"가중치 w: {w}\")\n",
    "print(f\"편향 b: {b}\")\n",
    "print()\n",
    "print(\"계산 과정:\")\n",
    "print(f\"  w^T x = {w[0]} × {x[0]} + {w[1]} × {x[1]}\")\n",
    "print(f\"        = {w[0] * x[0]} + {w[1] * x[1]}\")\n",
    "print(f\"        = {np.dot(w, x)}\")\n",
    "print(f\"  z = w^T x + b = {np.dot(w, x)} + {b} = {z}\")\n",
    "print(f\"  y = f(z) = {y} (z > 0 이므로)\")\n",
    "print()\n",
    "print(f\"최종 출력: {y}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 활성화 함수 (Activation Functions)\n",
    "\n",
    "### 2.1 Sigmoid 함수\n",
    "\n",
    "**수식:** $$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "**미분:** $$\\frac{d\\sigma(z)}{dz} = \\sigma(z) \\times (1 - \\sigma(z))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"Sigmoid 활성화 함수\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    \"\"\"Sigmoid 함수의 미분\"\"\"\n",
    "    s = sigmoid(z)\n",
    "    return s * (1 - s)\n",
    "\n",
    "# 수치 예제\n",
    "test_values = [0, 2, -2]\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Sigmoid 함수 수치 예제\")\n",
    "print(\"=\" * 50)\n",
    "for z in test_values:\n",
    "    s = sigmoid(z)\n",
    "    print(f\"σ({z:2}) = {s:.4f}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 활성화 함수 시각화\n",
    "z = np.linspace(-6, 6, 200)\n",
    "\n",
    "# Sigmoid\n",
    "y_sigmoid = sigmoid(z)\n",
    "dy_sigmoid = sigmoid_derivative(z)\n",
    "\n",
    "# ReLU\n",
    "y_relu = np.maximum(0, z)\n",
    "dy_relu = np.where(z > 0, 1, 0)\n",
    "\n",
    "# Tanh\n",
    "y_tanh = np.tanh(z)\n",
    "dy_tanh = 1 - np.tanh(z)**2\n",
    "\n",
    "# 플롯\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "fig.suptitle('Activation Functions and Their Derivatives', fontsize=16)\n",
    "\n",
    "# Sigmoid\n",
    "axes[0, 0].plot(z, y_sigmoid, 'b-', linewidth=2)\n",
    "axes[0, 0].set_title('Sigmoid')\n",
    "axes[0, 0].set_xlabel('z')\n",
    "axes[0, 0].set_ylabel('σ(z)')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 0].plot(z, dy_sigmoid, 'r-', linewidth=2)\n",
    "axes[1, 0].set_title('Sigmoid Derivative')\n",
    "axes[1, 0].set_xlabel('z')\n",
    "axes[1, 0].set_ylabel(\"σ'(z)\")\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# ReLU\n",
    "axes[0, 1].plot(z, y_relu, 'g-', linewidth=2)\n",
    "axes[0, 1].set_title('ReLU')\n",
    "axes[0, 1].set_xlabel('z')\n",
    "axes[0, 1].set_ylabel('ReLU(z)')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 1].plot(z, dy_relu, 'r-', linewidth=2)\n",
    "axes[1, 1].set_title('ReLU Derivative')\n",
    "axes[1, 1].set_xlabel('z')\n",
    "axes[1, 1].set_ylabel(\"ReLU'(z)\")\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Tanh\n",
    "axes[0, 2].plot(z, y_tanh, 'm-', linewidth=2)\n",
    "axes[0, 2].set_title('Tanh')\n",
    "axes[0, 2].set_xlabel('z')\n",
    "axes[0, 2].set_ylabel('tanh(z)')\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 2].plot(z, dy_tanh, 'r-', linewidth=2)\n",
    "axes[1, 2].set_title('Tanh Derivative')\n",
    "axes[1, 2].set_xlabel('z')\n",
    "axes[1, 2].set_ylabel(\"tanh'(z)\")\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 순방향 전파 (Forward Propagation)\n",
    "\n",
    "### 수식\n",
    "층 $l$에서의 연산:\n",
    "$$z^{[l]} = W^{[l]} a^{[l-1]} + b^{[l]}$$\n",
    "$$a^{[l]} = f^{[l]}(z^{[l]})$$\n",
    "\n",
    "**기호 설명:**\n",
    "- $W^{[l]}$: $l$번째 층의 가중치 행렬\n",
    "- $a^{[l-1]}$: 이전 층의 활성화 값\n",
    "- $b^{[l]}$: $l$번째 층의 편향 벡터\n",
    "- $z^{[l]}$: 선형 결합\n",
    "- $f^{[l]}(\\cdot)$: 활성화 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    \"\"\"ReLU 활성화 함수\"\"\"\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def forward_propagation(a0, W1, b1, W2, b2):\n",
    "    \"\"\"\n",
    "    2층 신경망의 순방향 전파\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    a0 : 입력 벡터\n",
    "    W1, b1 : 1층 파라미터\n",
    "    W2, b2 : 2층 파라미터\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    cache : 중간 값들을 담은 딕셔너리\n",
    "    \"\"\"\n",
    "    # 1층 (은닉층, ReLU)\n",
    "    z1 = np.dot(W1, a0) + b1\n",
    "    a1 = relu(z1)\n",
    "    \n",
    "    # 2층 (출력층, Sigmoid)\n",
    "    z2 = np.dot(W2, a1) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "    \n",
    "    cache = {\n",
    "        'a0': a0, 'z1': z1, 'a1': a1,\n",
    "        'z2': z2, 'a2': a2\n",
    "    }\n",
    "    \n",
    "    return cache\n",
    "\n",
    "# 수치 예제 (문서의 예제와 동일)\n",
    "a0 = np.array([1.0, 2.0])\n",
    "W1 = np.array([[0.5, 0.3],\n",
    "               [0.2, 0.4]])\n",
    "b1 = np.array([0.1, 0.2])\n",
    "W2 = np.array([[0.6, 0.7]])\n",
    "b2 = np.array([0.15])\n",
    "\n",
    "cache = forward_propagation(a0, W1, b1, W2, b2)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"순방향 전파 수치 예제\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"입력 a^[0]: {cache['a0']}\")\n",
    "print()\n",
    "print(\"1층 (은닉층):\")\n",
    "print(f\"  W^[1] = \\n{W1}\")\n",
    "print(f\"  b^[1] = {b1}\")\n",
    "print(f\"  z^[1] = W^[1] a^[0] + b^[1] = {cache['z1']}\")\n",
    "print(f\"  수동 계산: z1[0] = {W1[0,0]}×{a0[0]} + {W1[0,1]}×{a0[1]} + {b1[0]} = {W1[0,0]*a0[0] + W1[0,1]*a0[1] + b1[0]}\")\n",
    "print(f\"  수동 계산: z1[1] = {W1[1,0]}×{a0[0]} + {W1[1,1]}×{a0[1]} + {b1[1]} = {W1[1,0]*a0[0] + W1[1,1]*a0[1] + b1[1]}\")\n",
    "print(f\"  a^[1] = ReLU(z^[1]) = {cache['a1']}\")\n",
    "print()\n",
    "print(\"2층 (출력층):\")\n",
    "print(f\"  W^[2] = {W2}\")\n",
    "print(f\"  b^[2] = {b2}\")\n",
    "print(f\"  z^[2] = W^[2] a^[1] + b^[2] = {cache['z2']}\")\n",
    "print(f\"  수동 계산: z2 = {W2[0,0]}×{cache['a1'][0]} + {W2[0,1]}×{cache['a1'][1]} + {b2[0]} = {W2[0,0]*cache['a1'][0] + W2[0,1]*cache['a1'][1] + b2[0]}\")\n",
    "print(f\"  a^[2] = σ(z^[2]) = {cache['a2']}\")\n",
    "print()\n",
    "print(f\"최종 출력 (예측값 ŷ): {cache['a2'][0]:.4f}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 손실 함수 (Loss Functions)\n",
    "\n",
    "### 4.1 평균 제곱 오차 (MSE)\n",
    "\n",
    "**수식:** $$L(y, \\hat{y}) = \\frac{1}{2}(y - \\hat{y})^2$$\n",
    "\n",
    "### 4.2 교차 엔트로피 (Cross-Entropy)\n",
    "\n",
    "**이진 분류:** $$L(y, \\hat{y}) = -[y \\log(\\hat{y}) + (1-y) \\log(1-\\hat{y})]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(y_true, y_pred):\n",
    "    \"\"\"평균 제곱 오차\"\"\"\n",
    "    return 0.5 * (y_true - y_pred) ** 2\n",
    "\n",
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    \"\"\"이진 교차 엔트로피\"\"\"\n",
    "    epsilon = 1e-10  # log(0) 방지\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return -(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "# 수치 예제\n",
    "print(\"=\" * 60)\n",
    "print(\"손실 함수 수치 예제\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# MSE 예제\n",
    "y_true = 1.0\n",
    "y_pred = 0.8\n",
    "loss = mse_loss(y_true, y_pred)\n",
    "print(\"\\nMSE 손실:\")\n",
    "print(f\"  실제값: y = {y_true}\")\n",
    "print(f\"  예측값: ŷ = {y_pred}\")\n",
    "print(f\"  L = (1/2)(y - ŷ)² = (1/2)({y_true} - {y_pred})² = {loss}\")\n",
    "\n",
    "# 교차 엔트로피 예제\n",
    "print(\"\\n교차 엔트로피 손실:\")\n",
    "test_cases = [\n",
    "    (1, 0.9),\n",
    "    (0, 0.2),\n",
    "    (1, 0.5)\n",
    "]\n",
    "\n",
    "for y, y_hat in test_cases:\n",
    "    loss = binary_cross_entropy(y, y_hat)\n",
    "    print(f\"  y={y}, ŷ={y_hat}: L = {loss:.4f}\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 완전한 예제: 앞의 순방향 전파 결과로 손실 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 앞의 순방향 전파 결과 사용\n",
    "y_pred = cache['a2'][0]\n",
    "y_true = 1.0\n",
    "\n",
    "loss = binary_cross_entropy(y_true, y_pred)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"완전한 예제: 순방향 전파 + 손실 계산\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"예측값 (순방향 전파 결과): ŷ = {y_pred:.4f}\")\n",
    "print(f\"실제값: y = {y_true}\")\n",
    "print(f\"손실 (교차 엔트로피): L = {loss:.4f}\")\n",
    "print()\n",
    "print(\"수동 계산:\")\n",
    "print(f\"  L = -[y log(ŷ) + (1-y) log(1-ŷ)]\")\n",
    "print(f\"    = -[{y_true} × log({y_pred:.4f}) + 0 × log({1-y_pred:.4f})]\")\n",
    "print(f\"    = -log({y_pred:.4f})\")\n",
    "print(f\"    = {-np.log(y_pred):.4f}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 손실 함수 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측값 범위\n",
    "y_pred_range = np.linspace(0.01, 0.99, 100)\n",
    "\n",
    "# y=1일 때와 y=0일 때의 손실\n",
    "loss_y1 = binary_cross_entropy(1, y_pred_range)\n",
    "loss_y0 = binary_cross_entropy(0, y_pred_range)\n",
    "\n",
    "# MSE 비교\n",
    "mse_y1 = mse_loss(1, y_pred_range)\n",
    "mse_y0 = mse_loss(0, y_pred_range)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 교차 엔트로피\n",
    "axes[0].plot(y_pred_range, loss_y1, 'b-', linewidth=2, label='y=1')\n",
    "axes[0].plot(y_pred_range, loss_y0, 'r-', linewidth=2, label='y=0')\n",
    "axes[0].set_xlabel('Predicted Probability (ŷ)', fontsize=12)\n",
    "axes[0].set_ylabel('Cross-Entropy Loss', fontsize=12)\n",
    "axes[0].set_title('Binary Cross-Entropy Loss', fontsize=14)\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# MSE\n",
    "axes[1].plot(y_pred_range, mse_y1, 'b-', linewidth=2, label='y=1')\n",
    "axes[1].plot(y_pred_range, mse_y0, 'r-', linewidth=2, label='y=0')\n",
    "axes[1].set_xlabel('Predicted Value (ŷ)', fontsize=12)\n",
    "axes[1].set_ylabel('MSE Loss', fontsize=12)\n",
    "axes[1].set_title('Mean Squared Error Loss', fontsize=14)\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 요약\n",
    "\n",
    "이 노트북에서 다룬 내용:\n",
    "\n",
    "1. **퍼셉트론**: 신경망의 기본 단위 구현 및 수치 검증\n",
    "2. **활성화 함수**: Sigmoid, ReLU, Tanh의 구현 및 시각화\n",
    "3. **순방향 전파**: 2층 신경망을 통한 완전한 순방향 전파 과정\n",
    "4. **손실 함수**: MSE와 교차 엔트로피의 계산 및 비교\n",
    "\n",
    "### 다음 단계\n",
    "\n",
    "다음 노트북에서는 **역전파(Backpropagation)** 알고리즘을 구현하여 신경망을 실제로 학습시키는 방법을 배웁니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

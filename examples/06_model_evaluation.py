#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ëª¨ë¸ í‰ê°€ì™€ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ - ì‹¤ìŠµ ì˜ˆì œ

ì´ íŒŒì¼ì€ model_evaluation_theory.mdì˜ ì´ë¡ ì„ ì‹¤ì œ ì½”ë“œë¡œ êµ¬í˜„í•©ë‹ˆë‹¤.
ê° ì„¹ì…˜ì€ ë…ë¦½ì ìœ¼ë¡œ ì‹¤í–‰ ê°€ëŠ¥í•˜ë©°, ë‹¨ê³„ë³„ë¡œ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì‹¤í–‰ ë°©ë²•:
    python 06_model_evaluation.py

í•™ìŠµ ì‹œê°„: ì•½ 60-90ë¶„
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import (
    train_test_split, cross_val_score, StratifiedKFold,
    learning_curve, validation_curve, GridSearchCV,
    RandomizedSearchCV, cross_validate
)
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.metrics import (
    confusion_matrix, classification_report,
    precision_score, recall_score, f1_score,
    roc_curve, roc_auc_score, auc,
    precision_recall_curve, ConfusionMatrixDisplay
)
from scipy.stats import randint, uniform
import warnings
warnings.filterwarnings('ignore')


def print_section(title):
    """ì„¹ì…˜ ì œëª© ì¶œë ¥"""
    print("\n" + "="*80)
    print(f"  {title}")
    print("="*80 + "\n")


# ============================================================================
# ì„¹ì…˜ 1: ìœ„ìŠ¤ì½˜ì‹  ìœ ë°©ì•” ë°ì´í„°ì…‹ ë¡œë“œ ë° íƒìƒ‰
# ============================================================================

def section1_load_data():
    """
    ìœ„ìŠ¤ì½˜ì‹  ìœ ë°©ì•” ë°ì´í„°ì…‹ì„ ë¡œë“œí•˜ê³  ê¸°ë³¸ ì •ë³´ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
    
    í•™ìŠµ ëª©í‘œ:
    - ì‹¤ì œ ë°ì´í„°ì…‹ì˜ êµ¬ì¡° ì´í•´
    - íŠ¹ì„±ê³¼ íƒ€ê²Ÿì˜ ê´€ê³„ íŒŒì•…
    - í´ë˜ìŠ¤ ë¶„í¬ í™•ì¸
    """
    print_section("ì„¹ì…˜ 1: ìœ„ìŠ¤ì½˜ì‹  ìœ ë°©ì•” ë°ì´í„°ì…‹")
    
    # ë°ì´í„° ë¡œë“œ
    data = load_breast_cancer()
    X, y = data.data, data.target
    
    print("ğŸ“Š ë°ì´í„°ì…‹ ê¸°ë³¸ ì •ë³´")
    print(f"ìƒ˜í”Œ ìˆ˜: {X.shape[0]}")
    print(f"íŠ¹ì„± ìˆ˜: {X.shape[1]}")
    print(f"í´ë˜ìŠ¤: {data.target_names}")
    print(f"\níŠ¹ì„± ì´ë¦„ (ì²˜ìŒ 10ê°œ):")
    for i, name in enumerate(data.feature_names[:10]):
        print(f"  {i+1}. {name}")
    
    # í´ë˜ìŠ¤ ë¶„í¬
    unique, counts = np.unique(y, return_counts=True)
    print(f"\nğŸ“ˆ í´ë˜ìŠ¤ ë¶„í¬:")
    for label, count in zip(unique, counts):
        class_name = data.target_names[label]
        percentage = count / len(y) * 100
        print(f"  {class_name}: {count}ê°œ ({percentage:.1f}%)")
    
    # íŠ¹ì„± í†µê³„
    print(f"\nğŸ“ íŠ¹ì„± í†µê³„ (ì²˜ìŒ 5ê°œ íŠ¹ì„±):")
    for i in range(5):
        print(f"\n  {data.feature_names[i]}:")
        print(f"    í‰ê· : {X[:, i].mean():.2f}")
        print(f"    í‘œì¤€í¸ì°¨: {X[:, i].std():.2f}")
        print(f"    ìµœì†Œê°’: {X[:, i].min():.2f}")
        print(f"    ìµœëŒ€ê°’: {X[:, i].max():.2f}")
    
    return X, y, data


# ============================================================================
# ì„¹ì…˜ 2: íŒŒì´í”„ë¼ì¸ ê¸°ë³¸ ì‚¬ìš©ë²•
# ============================================================================

def section2_basic_pipeline(X, y):
    """
    íŒŒì´í”„ë¼ì¸ì˜ ê¸°ë³¸ ì‚¬ìš©ë²•ì„ ë°°ì›ë‹ˆë‹¤.
    
    í•™ìŠµ ëª©í‘œ:
    - Pipeline ê°ì²´ ìƒì„±
    - í‘œì¤€í™” + ëª¨ë¸ í•™ìŠµì„ í•œ ë²ˆì— ìˆ˜í–‰
    - íŒŒì´í”„ë¼ì¸ì˜ ì¥ì  ì´í•´
    """
    print_section("ì„¹ì…˜ 2: íŒŒì´í”„ë¼ì¸ ê¸°ë³¸ ì‚¬ìš©ë²•")
    
    # ë°ì´í„° ë¶„í• 
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42, stratify=y
    )
    
    print("ğŸ”§ ë°©ë²• 1: ìˆ˜ë™ìœ¼ë¡œ ê° ë‹¨ê³„ ì‹¤í–‰ (íŒŒì´í”„ë¼ì¸ ì—†ì´)")
    print("-" * 60)
    
    # ìˆ˜ë™ ë°©ì‹
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    lr = LogisticRegression(random_state=42, max_iter=1000)
    lr.fit(X_train_scaled, y_train)
    manual_score = lr.score(X_test_scaled, y_test)
    
    print(f"í•™ìŠµ ì„¸íŠ¸ í¬ê¸°: {X_train.shape}")
    print(f"í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ í¬ê¸°: {X_test.shape}")
    print(f"í…ŒìŠ¤íŠ¸ ì •í™•ë„: {manual_score:.4f}")
    
    print("\nğŸš€ ë°©ë²• 2: íŒŒì´í”„ë¼ì¸ ì‚¬ìš©")
    print("-" * 60)
    
    # íŒŒì´í”„ë¼ì¸ ë°©ì‹
    pipe = Pipeline([
        ('scaler', StandardScaler()),
        ('classifier', LogisticRegression(random_state=42, max_iter=1000))
    ])
    
    pipe.fit(X_train, y_train)
    pipeline_score = pipe.score(X_test, y_test)
    
    print("íŒŒì´í”„ë¼ì¸ êµ¬ì¡°:")
    for name, step in pipe.steps:
        print(f"  {name}: {step.__class__.__name__}")
    
    print(f"\ní…ŒìŠ¤íŠ¸ ì •í™•ë„: {pipeline_score:.4f}")
    print(f"ìˆ˜ë™ ë°©ì‹ê³¼ ë™ì¼í•œ ê²°ê³¼: {np.isclose(manual_score, pipeline_score)}")
    
    print("\nâœ… íŒŒì´í”„ë¼ì¸ì˜ ì¥ì :")
    print("  1. ì½”ë“œê°€ ê°„ê²°í•˜ê³  ì½ê¸° ì‰¬ì›€")
    print("  2. ì „ì²˜ë¦¬ ë‹¨ê³„ë¥¼ ìŠì–´ë²„ë¦´ ìœ„í—˜ì´ ì—†ìŒ")
    print("  3. êµì°¨ ê²€ì¦ê³¼ ì‰½ê²Œ ê²°í•© ê°€ëŠ¥")
    print("  4. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì´ ìš©ì´")
    
    return pipe


# ============================================================================
# ì„¹ì…˜ 3: êµì°¨ ê²€ì¦ ë¹„êµ (í™€ë“œì•„ì›ƒ vs k-ê²¹)
# ============================================================================

def section3_cross_validation(X, y):
    """
    í™€ë“œì•„ì›ƒ ë°©ë²•ê³¼ k-ê²¹ êµì°¨ ê²€ì¦ì„ ë¹„êµí•©ë‹ˆë‹¤.
    
    í•™ìŠµ ëª©í‘œ:
    - í™€ë“œì•„ì›ƒì˜ í•œê³„ ì´í•´
    - k-ê²¹ êµì°¨ ê²€ì¦ì˜ ì¥ì 
    - ì¸µí™” k-ê²¹ì˜ ì¤‘ìš”ì„±
    """
    print_section("ì„¹ì…˜ 3: êµì°¨ ê²€ì¦ ë¹„êµ")
    
    pipe = Pipeline([
        ('scaler', StandardScaler()),
        ('classifier', LogisticRegression(random_state=42, max_iter=1000))
    ])
    
    print("ğŸ“Š ë°©ë²• 1: í™€ë“œì•„ì›ƒ ë°©ë²• (ë‹¨ì¼ ë¶„í• )")
    print("-" * 60)
    
    # ì—¬ëŸ¬ ë²ˆ ë°˜ë³µí•˜ì—¬ ë¶ˆì•ˆì •ì„± í™•ì¸
    holdout_scores = []
    for i in range(10):
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.3, random_state=i
        )
        pipe.fit(X_train, y_train)
        score = pipe.score(X_test, y_test)
        holdout_scores.append(score)
        if i < 3:  # ì²˜ìŒ 3ê°œë§Œ ì¶œë ¥
            print(f"  ì‹œë„ {i+1}: {score:.4f}")
    
    print(f"  ...")
    print(f"\ní™€ë“œì•„ì›ƒ í‰ê· : {np.mean(holdout_scores):.4f} (Â±{np.std(holdout_scores):.4f})")
    print(f"ìµœì†Œê°’: {np.min(holdout_scores):.4f}")
    print(f"ìµœëŒ€ê°’: {np.max(holdout_scores):.4f}")
    print(f"ë²”ìœ„: {np.max(holdout_scores) - np.min(holdout_scores):.4f}")
    
    print("\nğŸ“Š ë°©ë²• 2: k-ê²¹ êµì°¨ ê²€ì¦")
    print("-" * 60)
    
    # 5-ê²¹ êµì°¨ ê²€ì¦
    cv_scores = cross_val_score(pipe, X, y, cv=5, scoring='accuracy')
    
    print("ê° Foldì˜ ì ìˆ˜:")
    for i, score in enumerate(cv_scores, 1):
        print(f"  Fold {i}: {score:.4f}")
    
    print(f"\nêµì°¨ ê²€ì¦ í‰ê· : {cv_scores.mean():.4f} (Â±{cv_scores.std():.4f})")
    
    print("\nğŸ“Š ë°©ë²• 3: ì¸µí™” k-ê²¹ êµì°¨ ê²€ì¦")
    print("-" * 60)
    
    # ì¸µí™” k-ê²¹
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    stratified_scores = cross_val_score(pipe, X, y, cv=skf, scoring='accuracy')
    
    print("ê° Foldì˜ ì ìˆ˜:")
    for i, score in enumerate(stratified_scores, 1):
        print(f"  Fold {i}: {score:.4f}")
    
    print(f"\nì¸µí™” êµì°¨ ê²€ì¦ í‰ê· : {stratified_scores.mean():.4f} (Â±{stratified_scores.std():.4f})")
    
    # ê° foldì˜ í´ë˜ìŠ¤ ë¶„í¬ í™•ì¸
    print("\nê° Foldì˜ í´ë˜ìŠ¤ ë¶„í¬:")
    for i, (train_idx, test_idx) in enumerate(skf.split(X, y), 1):
        y_fold = y[test_idx]
        unique, counts = np.unique(y_fold, return_counts=True)
        ratio = counts[1] / len(y_fold) * 100
        print(f"  Fold {i}: ì•…ì„± {ratio:.1f}%")
    
    print("\nâœ… ê²°ë¡ :")
    print(f"  - í™€ë“œì•„ì›ƒ: ë¶„í• ì— ë”°ë¼ ì„±ëŠ¥ ì°¨ì´ê°€ í¼ (ë²”ìœ„: Â±{np.std(holdout_scores):.4f})")
    print(f"  - k-ê²¹: ë” ì•ˆì •ì ì¸ ì¶”ì • (í‘œì¤€í¸ì°¨: {cv_scores.std():.4f})")
    print(f"  - ì¸µí™” k-ê²¹: í´ë˜ìŠ¤ ë¹„ìœ¨ ìœ ì§€ë¡œ ë”ìš± ì‹ ë¢°ì„± ìˆìŒ")


# ============================================================================
# ì„¹ì…˜ 4: í•™ìŠµ ê³¡ì„ ìœ¼ë¡œ í¸í–¥-ë¶„ì‚° ë¶„ì„
# ============================================================================

def section4_learning_curves(X, y):
    """
    í•™ìŠµ ê³¡ì„ ì„ ê·¸ë ¤ ëª¨ë¸ì˜ í¸í–¥ê³¼ ë¶„ì‚°ì„ ë¶„ì„í•©ë‹ˆë‹¤.
    
    í•™ìŠµ ëª©í‘œ:
    - í•™ìŠµ ê³¡ì„  ìƒì„±
    - ê³¼ì†Œì í•©/ê³¼ëŒ€ì í•© íŒë‹¨
    - ë°ì´í„° ì¶”ê°€ í•„ìš”ì„± íŒë‹¨
    """
    print_section("ì„¹ì…˜ 4: í•™ìŠµ ê³¡ì„  ë¶„ì„")
    
    print("ğŸ“ˆ ë‹¤ì–‘í•œ ëª¨ë¸ì˜ í•™ìŠµ ê³¡ì„  ë¹„êµ")
    print("-" * 60)
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    
    models = [
        ('ê³¼ì†Œì í•© ì˜ˆì‹œ\n(ë‹¨ìˆœ ëª¨ë¸)', 
         Pipeline([('scaler', StandardScaler()), 
                  ('clf', LogisticRegression(C=0.01, max_iter=1000))])),
        ('ì ì ˆí•œ ì í•©\n(ê¸°ë³¸ ëª¨ë¸)', 
         Pipeline([('scaler', StandardScaler()), 
                  ('clf', LogisticRegression(C=1.0, max_iter=1000))])),
        ('ì•½ê°„ ê³¼ëŒ€ì í•©\n(ë³µì¡ ëª¨ë¸)', 
         Pipeline([('scaler', StandardScaler()), 
                  ('clf', SVC(kernel='rbf', gamma='auto'))])),
        ('ê³¼ëŒ€ì í•© ì˜ˆì‹œ\n(ë§¤ìš° ë³µì¡)', 
         Pipeline([('scaler', StandardScaler()), 
                  ('clf', DecisionTreeClassifier(max_depth=None))]))
    ]
    
    for idx, (title, model) in enumerate(models):
        ax = axes[idx // 2, idx % 2]
        
        # í•™ìŠµ ê³¡ì„  ê³„ì‚°
        train_sizes, train_scores, test_scores = learning_curve(
            model, X, y, cv=5,
            train_sizes=np.linspace(0.1, 1.0, 10),
            scoring='accuracy',
            n_jobs=-1
        )
        
        # í‰ê· ê³¼ í‘œì¤€í¸ì°¨
        train_mean = train_scores.mean(axis=1)
        train_std = train_scores.std(axis=1)
        test_mean = test_scores.mean(axis=1)
        test_std = test_scores.std(axis=1)
        
        # ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
        ax.plot(train_sizes, train_mean, label='í•™ìŠµ ì ìˆ˜', marker='o', color='blue')
        ax.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, 
                        alpha=0.15, color='blue')
        
        ax.plot(train_sizes, test_mean, label='ê²€ì¦ ì ìˆ˜', marker='s', color='red')
        ax.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, 
                        alpha=0.15, color='red')
        
        ax.set_xlabel('í•™ìŠµ ìƒ˜í”Œ ìˆ˜', fontsize=10)
        ax.set_ylabel('ì •í™•ë„', fontsize=10)
        ax.set_title(title, fontsize=12, fontweight='bold')
        ax.legend(loc='best')
        ax.grid(True, alpha=0.3)
        ax.set_ylim([0.5, 1.05])
        
        # ìµœì¢… ì„±ëŠ¥ ì¶œë ¥
        final_train = train_mean[-1]
        final_test = test_mean[-1]
        gap = final_train - final_test
        
        print(f"\n{title.replace(chr(10), ' ')}:")
        print(f"  ìµœì¢… í•™ìŠµ ì ìˆ˜: {final_train:.4f}")
        print(f"  ìµœì¢… ê²€ì¦ ì ìˆ˜: {final_test:.4f}")
        print(f"  ì ìˆ˜ ì°¨ì´: {gap:.4f}")
        
        if gap < 0.05 and final_test < 0.85:
            print(f"  ì§„ë‹¨: ê³¼ì†Œì í•© (ë†’ì€ í¸í–¥)")
            print(f"  í•´ê²°ì±…: ë” ë³µì¡í•œ ëª¨ë¸ ì‚¬ìš©")
        elif gap > 0.15:
            print(f"  ì§„ë‹¨: ê³¼ëŒ€ì í•© (ë†’ì€ ë¶„ì‚°)")
            print(f"  í•´ê²°ì±…: ë” ë§ì€ ë°ì´í„° ìˆ˜ì§‘ ë˜ëŠ” ì •ê·œí™” ì¦ê°€")
        else:
            print(f"  ì§„ë‹¨: ì ì ˆí•œ ì í•©")
    
    plt.tight_layout()
    plt.savefig('/tmp/learning_curves.png', dpi=100, bbox_inches='tight')
    print(f"\nğŸ’¾ ê·¸ë˜í”„ ì €ì¥: /tmp/learning_curves.png")
    plt.close()
    
    print("\nâœ… í•™ìŠµ ê³¡ì„  í•´ì„ ê°€ì´ë“œ:")
    print("  1. ê³¼ì†Œì í•©: ë‘ ê³¡ì„ ì´ ë‚®ì€ ì ìˆ˜ì—ì„œ ìˆ˜ë ´")
    print("  2. ê³¼ëŒ€ì í•©: í•™ìŠµ ì ìˆ˜ëŠ” ë†’ì§€ë§Œ ê²€ì¦ ì ìˆ˜ëŠ” ë‚®ìŒ")
    print("  3. ì ì ˆ: ë‘ ê³¡ì„ ì´ ë†’ì€ ì ìˆ˜ì—ì„œ ê°€ê¹ê²Œ ìˆ˜ë ´")
    print("  4. ë” ë§ì€ ë°ì´í„° í•„ìš”: ê²€ì¦ ê³¡ì„ ì´ ê³„ì† ìƒìŠ¹ ì¤‘")


# ============================================================================
# ì„¹ì…˜ 5: ê²€ì¦ ê³¡ì„ ìœ¼ë¡œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¶„ì„
# ============================================================================

def section5_validation_curves(X, y):
    """
    ê²€ì¦ ê³¡ì„ ì„ ì‚¬ìš©í•˜ì—¬ í•˜ì´í¼íŒŒë¼ë¯¸í„°ì˜ ì˜í–¥ì„ ë¶„ì„í•©ë‹ˆë‹¤.
    
    í•™ìŠµ ëª©í‘œ:
    - ê²€ì¦ ê³¡ì„  ìƒì„±
    - ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ë²”ìœ„ íŒŒì•…
    - ê³¼ëŒ€ì í•©/ê³¼ì†Œì í•© êµ¬ê°„ í™•ì¸
    """
    print_section("ì„¹ì…˜ 5: ê²€ì¦ ê³¡ì„  ë¶„ì„")
    
    print("ğŸ“Š í•˜ì´í¼íŒŒë¼ë¯¸í„°ì— ë”°ë¥¸ ì„±ëŠ¥ ë³€í™”")
    print("-" * 60)
    
    fig, axes = plt.subplots(1, 2, figsize=(15, 5))
    
    # 1. Logistic Regressionì˜ C íŒŒë¼ë¯¸í„°
    print("\n1ï¸âƒ£ Logistic Regressionì˜ ì •ê·œí™” íŒŒë¼ë¯¸í„° C")
    print("   (Cê°€ ì‘ì„ìˆ˜ë¡ ê°•í•œ ì •ê·œí™”)")
    
    pipe_lr = Pipeline([
        ('scaler', StandardScaler()),
        ('clf', LogisticRegression(max_iter=1000))
    ])
    
    param_range = np.logspace(-4, 4, 9)  # 0.0001 ~ 10000
    
    train_scores, test_scores = validation_curve(
        pipe_lr, X, y,
        param_name='clf__C',
        param_range=param_range,
        cv=5,
        scoring='accuracy',
        n_jobs=-1
    )
    
    train_mean = train_scores.mean(axis=1)
    train_std = train_scores.std(axis=1)
    test_mean = test_scores.mean(axis=1)
    test_std = test_scores.std(axis=1)
    
    axes[0].semilogx(param_range, train_mean, label='í•™ìŠµ ì ìˆ˜', marker='o', color='blue')
    axes[0].fill_between(param_range, train_mean - train_std, train_mean + train_std,
                         alpha=0.15, color='blue')
    axes[0].semilogx(param_range, test_mean, label='ê²€ì¦ ì ìˆ˜', marker='s', color='red')
    axes[0].fill_between(param_range, test_mean - test_std, test_mean + test_std,
                         alpha=0.15, color='red')
    axes[0].set_xlabel('C (ì •ê·œí™” íŒŒë¼ë¯¸í„°)', fontsize=11)
    axes[0].set_ylabel('ì •í™•ë„', fontsize=11)
    axes[0].set_title('Logistic Regression: C íŒŒë¼ë¯¸í„° ì˜í–¥', fontsize=12, fontweight='bold')
    axes[0].legend(loc='best')
    axes[0].grid(True, alpha=0.3)
    
    # ìµœì ê°’ ì°¾ê¸°
    best_idx = np.argmax(test_mean)
    best_c = param_range[best_idx]
    best_score = test_mean[best_idx]
    
    print(f"\n   ìµœì  C: {best_c:.4f}")
    print(f"   ìµœê³  ê²€ì¦ ì ìˆ˜: {best_score:.4f}")
    
    # êµ¬ê°„ë³„ ë¶„ì„
    print("\n   êµ¬ê°„ë³„ ë¶„ì„:")
    print(f"   C < 0.1: ê³¼ì†Œì í•© (ê°•í•œ ì •ê·œí™”)")
    print(f"   0.1 â‰¤ C â‰¤ 10: ì ì ˆí•œ ë²”ìœ„")
    print(f"   C > 100: ê³¼ëŒ€ì í•© ê°€ëŠ¥ì„± (ì•½í•œ ì •ê·œí™”)")
    
    # 2. Decision Treeì˜ max_depth íŒŒë¼ë¯¸í„°
    print("\n2ï¸âƒ£ Decision Treeì˜ ìµœëŒ€ ê¹Šì´ (max_depth)")
    
    pipe_dt = Pipeline([
        ('scaler', StandardScaler()),
        ('clf', DecisionTreeClassifier(random_state=42))
    ])
    
    param_range_dt = range(1, 21)
    
    train_scores_dt, test_scores_dt = validation_curve(
        pipe_dt, X, y,
        param_name='clf__max_depth',
        param_range=param_range_dt,
        cv=5,
        scoring='accuracy',
        n_jobs=-1
    )
    
    train_mean_dt = train_scores_dt.mean(axis=1)
    train_std_dt = train_scores_dt.std(axis=1)
    test_mean_dt = test_scores_dt.mean(axis=1)
    test_std_dt = test_scores_dt.std(axis=1)
    
    axes[1].plot(param_range_dt, train_mean_dt, label='í•™ìŠµ ì ìˆ˜', marker='o', color='blue')
    axes[1].fill_between(param_range_dt, train_mean_dt - train_std_dt, 
                         train_mean_dt + train_std_dt, alpha=0.15, color='blue')
    axes[1].plot(param_range_dt, test_mean_dt, label='ê²€ì¦ ì ìˆ˜', marker='s', color='red')
    axes[1].fill_between(param_range_dt, test_mean_dt - test_std_dt, 
                         test_mean_dt + test_std_dt, alpha=0.15, color='red')
    axes[1].set_xlabel('max_depth (ìµœëŒ€ ê¹Šì´)', fontsize=11)
    axes[1].set_ylabel('ì •í™•ë„', fontsize=11)
    axes[1].set_title('Decision Tree: max_depth íŒŒë¼ë¯¸í„° ì˜í–¥', fontsize=12, fontweight='bold')
    axes[1].legend(loc='best')
    axes[1].grid(True, alpha=0.3)
    
    # ìµœì ê°’ ì°¾ê¸°
    best_idx_dt = np.argmax(test_mean_dt)
    best_depth = param_range_dt[best_idx_dt]
    best_score_dt = test_mean_dt[best_idx_dt]
    
    print(f"\n   ìµœì  max_depth: {best_depth}")
    print(f"   ìµœê³  ê²€ì¦ ì ìˆ˜: {best_score_dt:.4f}")
    
    # ê³¼ëŒ€ì í•© í™•ì¸
    overfitting_gap = train_mean_dt[-1] - test_mean_dt[-1]
    print(f"\n   ê¹Šì´ 20ì—ì„œì˜ ê³¼ëŒ€ì í•© ì •ë„: {overfitting_gap:.4f}")
    print(f"   (í•™ìŠµ ì ìˆ˜ - ê²€ì¦ ì ìˆ˜)")
    
    plt.tight_layout()
    plt.savefig('/tmp/validation_curves.png', dpi=100, bbox_inches='tight')
    print(f"\nğŸ’¾ ê·¸ë˜í”„ ì €ì¥: /tmp/validation_curves.png")
    plt.close()
    
    print("\nâœ… ê²€ì¦ ê³¡ì„  í•´ì„:")
    print("  - Cê°€ ë„ˆë¬´ ì‘ìœ¼ë©´: ê³¼ì†Œì í•© (ëª¨ë¸ì´ ë„ˆë¬´ ë‹¨ìˆœ)")
    print("  - Cê°€ ì ì ˆí•˜ë©´: ìµœê³  ì„±ëŠ¥")
    print("  - Cê°€ ë„ˆë¬´ í¬ë©´: ê³¼ëŒ€ì í•© (í•™ìŠµ ë°ì´í„°ì— ê³¼ë„í•˜ê²Œ ë§ì¶¤)")


# ============================================================================
# ì„¹ì…˜ 6: ê·¸ë¦¬ë“œ ì„œì¹˜ë¡œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
# ============================================================================

def section6_grid_search(X, y):
    """
    ê·¸ë¦¬ë“œ ì„œì¹˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
    
    í•™ìŠµ ëª©í‘œ:
    - GridSearchCV ì‚¬ìš©ë²•
    - íŒŒì´í”„ë¼ì¸ê³¼ ê²°í•©
    - ìµœì  íŒŒë¼ë¯¸í„° í•´ì„
    """
    print_section("ì„¹ì…˜ 6: ê·¸ë¦¬ë“œ ì„œì¹˜")
    
    # ë°ì´í„° ë¶„í• 
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42, stratify=y
    )
    
    print("ğŸ” ê·¸ë¦¬ë“œ ì„œì¹˜: ì²´ê³„ì ì¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰")
    print("-" * 60)
    
    # íŒŒì´í”„ë¼ì¸ êµ¬ì„±
    pipe = Pipeline([
        ('scaler', StandardScaler()),
        ('pca', PCA()),
        ('classifier', SVC())
    ])
    
    # í•˜ì´í¼íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ
    param_grid = {
        'pca__n_components': [5, 10, 15, 20],
        'classifier__C': [0.1, 1, 10, 100],
        'classifier__kernel': ['linear', 'rbf'],
        'classifier__gamma': ['scale', 'auto']
    }
    
    print("íƒìƒ‰í•  í•˜ì´í¼íŒŒë¼ë¯¸í„°:")
    for param, values in param_grid.items():
        print(f"  {param}: {values}")
    
    total_combinations = 4 * 4 * 2 * 2
    print(f"\nì´ ì¡°í•© ìˆ˜: {total_combinations}")
    print(f"5-ê²¹ êµì°¨ ê²€ì¦ ì‚¬ìš© ì‹œ í•™ìŠµ íšŸìˆ˜: {total_combinations * 5}")
    
    # ê·¸ë¦¬ë“œ ì„œì¹˜ ì‹¤í–‰
    print("\nâ³ ê·¸ë¦¬ë“œ ì„œì¹˜ ì‹¤í–‰ ì¤‘...")
    grid_search = GridSearchCV(
        pipe,
        param_grid,
        cv=5,
        scoring='accuracy',
        n_jobs=-1,
        verbose=0,
        return_train_score=True
    )
    
    grid_search.fit(X_train, y_train)
    
    print("âœ… ì™„ë£Œ!")
    print(f"\nìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°:")
    for param, value in grid_search.best_params_.items():
        print(f"  {param}: {value}")
    
    print(f"\nìµœê³  êµì°¨ ê²€ì¦ ì ìˆ˜: {grid_search.best_score_:.4f}")
    
    # í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ í‰ê°€
    test_score = grid_search.score(X_test, y_test)
    print(f"í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ì ìˆ˜: {test_score:.4f}")
    
    # ìƒìœ„ 5ê°œ ì¡°í•©
    print("\nğŸ“Š ìƒìœ„ 5ê°œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°í•©:")
    results = grid_search.cv_results_
    indices = np.argsort(results['mean_test_score'])[::-1][:5]
    
    for i, idx in enumerate(indices, 1):
        print(f"\n{i}ìœ„:")
        print(f"  ì ìˆ˜: {results['mean_test_score'][idx]:.4f} (Â±{results['std_test_score'][idx]:.4f})")
        print(f"  íŒŒë¼ë¯¸í„°: {results['params'][idx]}")
    
    # PCA ì„±ë¶„ ìˆ˜ì˜ ì˜í–¥ ë¶„ì„
    print("\nğŸ“ˆ PCA ì„±ë¶„ ìˆ˜ì— ë”°ë¥¸ í‰ê·  ì„±ëŠ¥:")
    for n_comp in [5, 10, 15, 20]:
        mask = [p['pca__n_components'] == n_comp for p in results['params']]
        avg_score = results['mean_test_score'][mask].mean()
        print(f"  n_components={n_comp}: {avg_score:.4f}")
    
    return grid_search


# ============================================================================
# ì„¹ì…˜ 7: ëœë¤ ì„œì¹˜ë¡œ íš¨ìœ¨ì ì¸ íƒìƒ‰
# ============================================================================

def section7_random_search(X, y):
    """
    ëœë¤ ì„œì¹˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë” ë„“ì€ ë²”ìœ„ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ íƒìƒ‰í•©ë‹ˆë‹¤.
    
    í•™ìŠµ ëª©í‘œ:
    - RandomizedSearchCV ì‚¬ìš©
    - ì—°ì† ë¶„í¬ì—ì„œ ìƒ˜í”Œë§
    - ê·¸ë¦¬ë“œ ì„œì¹˜ì™€ ë¹„êµ
    """
    print_section("ì„¹ì…˜ 7: ëœë¤ ì„œì¹˜")
    
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42, stratify=y
    )
    
    print("ğŸ² ëœë¤ ì„œì¹˜: ë„“ì€ ë²”ìœ„ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ íƒìƒ‰")
    print("-" * 60)
    
    # íŒŒì´í”„ë¼ì¸
    pipe = Pipeline([
        ('scaler', StandardScaler()),
        ('classifier', RandomForestClassifier(random_state=42))
    ])
    
    # ëœë¤ ì„œì¹˜ìš© íŒŒë¼ë¯¸í„° ë¶„í¬
    param_distributions = {
        'classifier__n_estimators': randint(50, 500),  # 50~500 ì‚¬ì´ì˜ ì •ìˆ˜
        'classifier__max_depth': randint(5, 50),       # 5~50 ì‚¬ì´ì˜ ì •ìˆ˜
        'classifier__min_samples_split': randint(2, 20),
        'classifier__min_samples_leaf': randint(1, 10),
        'classifier__max_features': uniform(0.1, 0.9)  # 0.1~1.0 ì‚¬ì´ì˜ ì‹¤ìˆ˜
    }
    
    print("íƒìƒ‰í•  í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¶„í¬:")
    for param, dist in param_distributions.items():
        print(f"  {param}: {dist}")
    
    # ëœë¤ ì„œì¹˜ ì‹¤í–‰
    n_iter = 50  # 50ê°œ ì¡°í•©ë§Œ ì‹œë„
    print(f"\nì‹œë„í•  ì¡°í•© ìˆ˜: {n_iter}")
    print(f"5-ê²¹ êµì°¨ ê²€ì¦ ì‚¬ìš© ì‹œ í•™ìŠµ íšŸìˆ˜: {n_iter * 5}")
    
    print("\nâ³ ëœë¤ ì„œì¹˜ ì‹¤í–‰ ì¤‘...")
    random_search = RandomizedSearchCV(
        pipe,
        param_distributions,
        n_iter=n_iter,
        cv=5,
        scoring='accuracy',
        n_jobs=-1,
        random_state=42,
        verbose=0,
        return_train_score=True
    )
    
    random_search.fit(X_train, y_train)
    
    print("âœ… ì™„ë£Œ!")
    print(f"\nìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°:")
    for param, value in random_search.best_params_.items():
        print(f"  {param}: {value}")
    
    print(f"\nìµœê³  êµì°¨ ê²€ì¦ ì ìˆ˜: {random_search.best_score_:.4f}")
    
    test_score = random_search.score(X_test, y_test)
    print(f"í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ì ìˆ˜: {test_score:.4f}")
    
    # íƒìƒ‰ ê³¼ì • ì‹œê°í™”
    results = random_search.cv_results_
    
    print("\nğŸ“Š íƒìƒ‰í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë²”ìœ„:")
    for param in param_distributions.keys():
        param_short = param.split('__')[1]
        values = [p[param] for p in results['params']]
        print(f"  {param_short}:")
        print(f"    ìµœì†Œ: {min(values)}")
        print(f"    ìµœëŒ€: {max(values)}")
        print(f"    ìµœì : {random_search.best_params_[param]}")
    
    # ìƒìœ„ 5ê°œ ì¡°í•©
    print("\nğŸ† ìƒìœ„ 5ê°œ ì¡°í•©:")
    indices = np.argsort(results['mean_test_score'])[::-1][:5]
    for i, idx in enumerate(indices, 1):
        print(f"\n{i}ìœ„:")
        print(f"  ì ìˆ˜: {results['mean_test_score'][idx]:.4f}")
        params = results['params'][idx]
        for k, v in params.items():
            param_short = k.split('__')[1]
            if isinstance(v, float):
                print(f"  {param_short}: {v:.3f}")
            else:
                print(f"  {param_short}: {v}")
    
    print("\nâœ… ëœë¤ ì„œì¹˜ì˜ ì¥ì :")
    print("  - ë„“ì€ ë²”ìœ„ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ íƒìƒ‰")
    print("  - ì¤‘ìš”í•œ íŒŒë¼ë¯¸í„°ì— ë” ë§ì€ ê°’ ì‹œë„")
    print("  - ì‹œê°„ ì œì•½ì´ ìˆì„ ë•Œ ìœ ìš©")
    print("  - ì—°ì† ë¶„í¬ ì‚¬ìš© ê°€ëŠ¥")
    
    return random_search


# ============================================================================
# ì„¹ì…˜ 8: ì„±ëŠ¥ í‰ê°€ ì§€í‘œ - ì˜¤ì°¨ í–‰ë ¬ê³¼ ë¶„ë¥˜ ë¦¬í¬íŠ¸
# ============================================================================

def section8_classification_metrics(X, y):
    """
    ë‹¤ì–‘í•œ ë¶„ë¥˜ ì„±ëŠ¥ ì§€í‘œë¥¼ ê³„ì‚°í•˜ê³  í•´ì„í•©ë‹ˆë‹¤.
    
    í•™ìŠµ ëª©í‘œ:
    - ì˜¤ì°¨ í–‰ë ¬ ì´í•´
    - ì •ë°€ë„, ì¬í˜„ìœ¨, F1 ì ìˆ˜
    - ë¶„ë¥˜ ë¦¬í¬íŠ¸ í•´ì„
    """
    print_section("ì„¹ì…˜ 8: ë¶„ë¥˜ ì„±ëŠ¥ ì§€í‘œ")
    
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42, stratify=y
    )
    
    # ëª¨ë¸ í•™ìŠµ
    pipe = Pipeline([
        ('scaler', StandardScaler()),
        ('classifier', LogisticRegression(random_state=42, max_iter=1000))
    ])
    pipe.fit(X_train, y_train)
    
    # ì˜ˆì¸¡
    y_pred = pipe.predict(X_test)
    y_pred_proba = pipe.predict_proba(X_test)[:, 1]
    
    print("ğŸ“Š ì˜¤ì°¨ í–‰ë ¬ (Confusion Matrix)")
    print("-" * 60)
    
    cm = confusion_matrix(y_test, y_pred)
    print("\nì˜¤ì°¨ í–‰ë ¬:")
    print("               ì˜ˆì¸¡")
    print("           ì•…ì„±   ì–‘ì„±")
    print(f"ì‹¤ì œ ì•…ì„±   {cm[0,0]:3d}    {cm[0,1]:3d}")
    print(f"    ì–‘ì„±   {cm[1,0]:3d}    {cm[1,1]:3d}")
    
    # ìš©ì–´ ì„¤ëª…
    tn, fp, fn, tp = cm.ravel()
    print(f"\nìš©ì–´ ì •ë¦¬:")
    print(f"  TN (True Negative):  {tn} - ì–‘ì„±ì„ ì–‘ì„±ìœ¼ë¡œ ì •í™•íˆ ì˜ˆì¸¡")
    print(f"  FP (False Positive): {fp} - ì–‘ì„±ì„ ì•…ì„±ìœ¼ë¡œ ì˜ëª» ì˜ˆì¸¡")
    print(f"  FN (False Negative): {fn} - ì•…ì„±ì„ ì–‘ì„±ìœ¼ë¡œ ì˜ëª» ì˜ˆì¸¡")
    print(f"  TP (True Positive):  {tp} - ì•…ì„±ì„ ì•…ì„±ìœ¼ë¡œ ì •í™•íˆ ì˜ˆì¸¡")
    
    print("\nğŸ“ˆ ì„±ëŠ¥ ì§€í‘œ")
    print("-" * 60)
    
    # ì •í™•ë„
    accuracy = (tp + tn) / (tp + tn + fp + fn)
    print(f"\n1ï¸âƒ£ ì •í™•ë„ (Accuracy)")
    print(f"   ê³µì‹: (TP + TN) / (TP + TN + FP + FN)")
    print(f"   ê³„ì‚°: ({tp} + {tn}) / ({tp} + {tn} + {fp} + {fn})")
    print(f"   ê²°ê³¼: {accuracy:.4f}")
    print(f"   ì˜ë¯¸: ì „ì²´ ì˜ˆì¸¡ ì¤‘ {accuracy*100:.2f}%ê°€ ì •í™•")
    
    # ì •ë°€ë„
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    print(f"\n2ï¸âƒ£ ì •ë°€ë„ (Precision)")
    print(f"   ê³µì‹: TP / (TP + FP)")
    print(f"   ê³„ì‚°: {tp} / ({tp} + {fp})")
    print(f"   ê²°ê³¼: {precision:.4f}")
    print(f"   ì˜ë¯¸: ì•…ì„±ìœ¼ë¡œ ì˜ˆì¸¡í•œ ê²ƒ ì¤‘ {precision*100:.2f}%ê°€ ì‹¤ì œ ì•…ì„±")
    
    # ì¬í˜„ìœ¨
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    print(f"\n3ï¸âƒ£ ì¬í˜„ìœ¨ (Recall) / ë¯¼ê°ë„ (Sensitivity)")
    print(f"   ê³µì‹: TP / (TP + FN)")
    print(f"   ê³„ì‚°: {tp} / ({tp} + {fn})")
    print(f"   ê²°ê³¼: {recall:.4f}")
    print(f"   ì˜ë¯¸: ì‹¤ì œ ì•…ì„± ì¤‘ {recall*100:.2f}%ë¥¼ ì°¾ì•„ëƒ„")
    
    # F1 ì ìˆ˜
    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
    print(f"\n4ï¸âƒ£ F1 ì ìˆ˜ (F1-Score)")
    print(f"   ê³µì‹: 2 Ã— (Precision Ã— Recall) / (Precision + Recall)")
    print(f"   ê³„ì‚°: 2 Ã— ({precision:.4f} Ã— {recall:.4f}) / ({precision:.4f} + {recall:.4f})")
    print(f"   ê²°ê³¼: {f1:.4f}")
    print(f"   ì˜ë¯¸: ì •ë°€ë„ì™€ ì¬í˜„ìœ¨ì˜ ì¡°í™” í‰ê· ")
    
    # íŠ¹ì´ë„
    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
    print(f"\n5ï¸âƒ£ íŠ¹ì´ë„ (Specificity)")
    print(f"   ê³µì‹: TN / (TN + FP)")
    print(f"   ê³„ì‚°: {tn} / ({tn} + {fp})")
    print(f"   ê²°ê³¼: {specificity:.4f}")
    print(f"   ì˜ë¯¸: ì‹¤ì œ ì–‘ì„± ì¤‘ {specificity*100:.2f}%ë¥¼ ì •í™•íˆ íŒë³„")
    
    # ë¶„ë¥˜ ë¦¬í¬íŠ¸
    print("\nğŸ“‹ ì¢…í•© ë¶„ë¥˜ ë¦¬í¬íŠ¸")
    print("-" * 60)
    print(classification_report(y_test, y_pred, target_names=['ì•…ì„±', 'ì–‘ì„±']))
    
    print("\nâœ… ì§€í‘œ ì„ íƒ ê°€ì´ë“œ:")
    print("  - ì •í™•ë„: í´ë˜ìŠ¤ê°€ ê· í˜•ì¡í˜”ì„ ë•Œ")
    print("  - ì •ë°€ë„: ê±°ì§“ ì–‘ì„±ì´ ì¹˜ëª…ì ì¼ ë•Œ (ì˜ˆ: ìŠ¤íŒ¸ í•„í„°)")
    print("  - ì¬í˜„ìœ¨: ê±°ì§“ ìŒì„±ì´ ì¹˜ëª…ì ì¼ ë•Œ (ì˜ˆ: ì•” ì§„ë‹¨)")
    print("  - F1 ì ìˆ˜: ì •ë°€ë„ì™€ ì¬í˜„ìœ¨ì˜ ê· í˜•ì´ í•„ìš”í•  ë•Œ")
    
    # ì˜¤ì°¨ í–‰ë ¬ ì‹œê°í™”
    fig, ax = plt.subplots(figsize=(8, 6))
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['ì•…ì„±', 'ì–‘ì„±'])
    disp.plot(ax=ax, cmap='Blues', values_format='d')
    ax.set_title('ì˜¤ì°¨ í–‰ë ¬ (Confusion Matrix)', fontsize=14, fontweight='bold', pad=20)
    plt.tight_layout()
    plt.savefig('/tmp/confusion_matrix.png', dpi=100, bbox_inches='tight')
    print(f"\nğŸ’¾ ì˜¤ì°¨ í–‰ë ¬ ê·¸ë˜í”„ ì €ì¥: /tmp/confusion_matrix.png")
    plt.close()


# ============================================================================
# ì„¹ì…˜ 9: ROC ê³¡ì„ ê³¼ AUC
# ============================================================================

def section9_roc_curve(X, y):
    """
    ROC ê³¡ì„ ì„ ê·¸ë¦¬ê³  AUCë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    
    í•™ìŠµ ëª©í‘œ:
    - ROC ê³¡ì„  ì´í•´
    - AUC í•´ì„
    - ì„ê³„ê°’ ì¡°ì •
    """
    print_section("ì„¹ì…˜ 9: ROC ê³¡ì„ ê³¼ AUC")
    
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42, stratify=y
    )
    
    print("ğŸ“ˆ ROC ê³¡ì„  (Receiver Operating Characteristic)")
    print("-" * 60)
    
    # ì—¬ëŸ¬ ëª¨ë¸ ë¹„êµ
    models = {
        'Logistic Regression': Pipeline([
            ('scaler', StandardScaler()),
            ('clf', LogisticRegression(random_state=42, max_iter=1000))
        ]),
        'Random Forest': Pipeline([
            ('scaler', StandardScaler()),
            ('clf', RandomForestClassifier(n_estimators=100, random_state=42))
        ]),
        'SVM (RBF)': Pipeline([
            ('scaler', StandardScaler()),
            ('clf', SVC(probability=True, random_state=42))
        ])
    }
    
    plt.figure(figsize=(10, 8))
    
    for name, model in models.items():
        model.fit(X_train, y_train)
        
        # ì˜ˆì¸¡ í™•ë¥ 
        y_pred_proba = model.predict_proba(X_test)[:, 1]
        
        # ROC ê³¡ì„  ê³„ì‚°
        fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)
        roc_auc = auc(fpr, tpr)
        
        # ROC ê³¡ì„  ê·¸ë¦¬ê¸°
        plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.3f})', linewidth=2)
        
        print(f"\n{name}:")
        print(f"  AUC: {roc_auc:.4f}")
        
        # ìµœì  ì„ê³„ê°’ ì°¾ê¸° (Youden's J statistic)
        j_scores = tpr - fpr
        optimal_idx = np.argmax(j_scores)
        optimal_threshold = thresholds[optimal_idx]
        optimal_tpr = tpr[optimal_idx]
        optimal_fpr = fpr[optimal_idx]
        
        print(f"  ìµœì  ì„ê³„ê°’: {optimal_threshold:.3f}")
        print(f"  ìµœì  ì„ê³„ê°’ì—ì„œ TPR: {optimal_tpr:.3f}")
        print(f"  ìµœì  ì„ê³„ê°’ì—ì„œ FPR: {optimal_fpr:.3f}")
    
    # ë¬´ì‘ìœ„ ë¶„ë¥˜ê¸° (ëŒ€ê°ì„ )
    plt.plot([0, 1], [0, 1], 'k--', label='ë¬´ì‘ìœ„ ë¶„ë¥˜ê¸° (AUC = 0.5)', linewidth=1)
    
    plt.xlabel('False Positive Rate (FPR)', fontsize=12)
    plt.ylabel('True Positive Rate (TPR)', fontsize=12)
    plt.title('ROC ê³¡ì„  ë¹„êµ', fontsize=14, fontweight='bold')
    plt.legend(loc='lower right', fontsize=10)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig('/tmp/roc_curves.png', dpi=100, bbox_inches='tight')
    print(f"\nğŸ’¾ ROC ê³¡ì„  ì €ì¥: /tmp/roc_curves.png")
    plt.close()
    
    print("\nâœ… AUC í•´ì„:")
    print("  - AUC = 1.0: ì™„ë²½í•œ ë¶„ë¥˜ê¸°")
    print("  - AUC = 0.9-1.0: íƒì›”")
    print("  - AUC = 0.8-0.9: ìš°ìˆ˜")
    print("  - AUC = 0.7-0.8: ì–‘í˜¸")
    print("  - AUC = 0.6-0.7: ë³´í†µ")
    print("  - AUC = 0.5: ë¬´ì‘ìœ„ ìˆ˜ì¤€")
    print("  - AUC < 0.5: ë¬´ì‘ìœ„ë³´ë‹¤ ëª»í•¨")
    
    # Precision-Recall ê³¡ì„ ë„ ê·¸ë¦¬ê¸°
    print("\nğŸ“Š Precision-Recall ê³¡ì„ ")
    print("-" * 60)
    
    plt.figure(figsize=(10, 8))
    
    for name, model in models.items():
        y_pred_proba = model.predict_proba(X_test)[:, 1]
        
        # PR ê³¡ì„  ê³„ì‚°
        precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)
        pr_auc = auc(recall, precision)
        
        plt.plot(recall, precision, label=f'{name} (AUC = {pr_auc:.3f})', linewidth=2)
        
        print(f"{name} PR-AUC: {pr_auc:.4f}")
    
    plt.xlabel('Recall', fontsize=12)
    plt.ylabel('Precision', fontsize=12)
    plt.title('Precision-Recall ê³¡ì„ ', fontsize=14, fontweight='bold')
    plt.legend(loc='best', fontsize=10)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig('/tmp/pr_curves.png', dpi=100, bbox_inches='tight')
    print(f"\nğŸ’¾ PR ê³¡ì„  ì €ì¥: /tmp/pr_curves.png")
    plt.close()


# ============================================================================
# ì„¹ì…˜ 10: ë¶ˆê· í˜• í´ë˜ìŠ¤ ì²˜ë¦¬
# ============================================================================

def section10_imbalanced_classes():
    """
    ë¶ˆê· í˜•í•œ í´ë˜ìŠ¤ë¥¼ ë‹¤ë£¨ëŠ” ì—¬ëŸ¬ ê¸°ë²•ì„ ë°°ì›ë‹ˆë‹¤.
    
    í•™ìŠµ ëª©í‘œ:
    - í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¬¸ì œ ì´í•´
    - í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì‚¬ìš©
    - ë¦¬ìƒ˜í”Œë§ ê¸°ë²•
    - ì ì ˆí•œ í‰ê°€ ì§€í‘œ ì„ íƒ
    """
    print_section("ì„¹ì…˜ 10: ë¶ˆê· í˜•í•œ í´ë˜ìŠ¤ ì²˜ë¦¬")
    
    # ë¶ˆê· í˜• ë°ì´í„° ìƒì„± (1:9 ë¹„ìœ¨)
    print("ğŸ”§ ë¶ˆê· í˜• ë°ì´í„° ìƒì„± (ì•…ì„±:ì–‘ì„± = 1:9)")
    print("-" * 60)
    
    data = load_breast_cancer()
    X, y = data.data, data.target
    
    # ì•…ì„±(0)ì„ ì†Œìˆ˜ í´ë˜ìŠ¤ë¡œ ë§Œë“¤ê¸°
    malignant_indices = np.where(y == 0)[0]
    benign_indices = np.where(y == 1)[0]
    
    # ì•…ì„±ì€ ì¼ë¶€ë§Œ, ì–‘ì„±ì€ ë” ë§ì´ ì‚¬ìš©
    # ì•…ì„±ì˜ 30%ë§Œ ì‚¬ìš©, ì–‘ì„±ì€ ì „ì²´ ì‚¬ìš©í•˜ì—¬ 1:9 ë¹„ìœ¨ ë§Œë“¤ê¸°
    n_malignant = int(len(malignant_indices) * 0.3)
    selected_malignant = np.random.choice(malignant_indices, size=n_malignant, replace=False)
    selected_benign = benign_indices  # ì „ì²´ ì‚¬ìš©
    
    selected_indices = np.concatenate([selected_malignant, selected_benign])
    np.random.shuffle(selected_indices)
    
    X_imb = X[selected_indices]
    y_imb = y[selected_indices]
    
    # í´ë˜ìŠ¤ ë¶„í¬ í™•ì¸
    unique, counts = np.unique(y_imb, return_counts=True)
    print(f"\ní´ë˜ìŠ¤ ë¶„í¬:")
    for label, count in zip(unique, counts):
        percentage = count / len(y_imb) * 100
        print(f"  í´ë˜ìŠ¤ {label}: {count}ê°œ ({percentage:.1f}%)")
    
    X_train, X_test, y_train, y_test = train_test_split(
        X_imb, y_imb, test_size=0.3, random_state=42, stratify=y_imb
    )
    
    print("\nğŸ“Š ë°©ë²• 1: ê¸°ë³¸ ëª¨ë¸ (ë¶ˆê· í˜• ë¬´ì‹œ)")
    print("-" * 60)
    
    pipe_basic = Pipeline([
        ('scaler', StandardScaler()),
        ('clf', LogisticRegression(random_state=42, max_iter=1000))
    ])
    pipe_basic.fit(X_train, y_train)
    
    y_pred_basic = pipe_basic.predict(X_test)
    
    print("\nì˜¤ì°¨ í–‰ë ¬:")
    cm_basic = confusion_matrix(y_test, y_pred_basic)
    print(cm_basic)
    
    print("\në¶„ë¥˜ ë¦¬í¬íŠ¸:")
    print(classification_report(y_test, y_pred_basic, target_names=['ì•…ì„±', 'ì–‘ì„±']))
    
    print("\nâš ï¸  ë¬¸ì œì :")
    print(f"  - ì†Œìˆ˜ í´ë˜ìŠ¤(ì•…ì„±) ì¬í˜„ìœ¨ì´ ë‚®ìŒ")
    print(f"  - ì •í™•ë„ëŠ” ë†’ì•„ ë³´ì´ì§€ë§Œ ì‹¤ì œë¡œëŠ” ì“¸ëª¨ì—†ìŒ")
    
    print("\nğŸ“Š ë°©ë²• 2: í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì‚¬ìš©")
    print("-" * 60)
    
    pipe_weighted = Pipeline([
        ('scaler', StandardScaler()),
        ('clf', LogisticRegression(class_weight='balanced', random_state=42, max_iter=1000))
    ])
    pipe_weighted.fit(X_train, y_train)
    
    y_pred_weighted = pipe_weighted.predict(X_test)
    
    print("\nì˜¤ì°¨ í–‰ë ¬:")
    cm_weighted = confusion_matrix(y_test, y_pred_weighted)
    print(cm_weighted)
    
    print("\në¶„ë¥˜ ë¦¬í¬íŠ¸:")
    print(classification_report(y_test, y_pred_weighted, target_names=['ì•…ì„±', 'ì–‘ì„±']))
    
    # ê°€ì¤‘ì¹˜ ê³„ì‚° ì„¤ëª…
    n_samples = len(y_train)
    n_classes = 2
    class_counts = np.bincount(y_train)
    
    print("\nê°€ì¤‘ì¹˜ ê³„ì‚°:")
    for i in range(n_classes):
        weight = n_samples / (n_classes * class_counts[i])
        print(f"  í´ë˜ìŠ¤ {i}: {weight:.3f}")
        print(f"    ê³„ì‚°: {n_samples} / ({n_classes} Ã— {class_counts[i]})")
    
    print("\nâœ… ê°œì„ ì :")
    print(f"  - ì†Œìˆ˜ í´ë˜ìŠ¤ì˜ ì¬í˜„ìœ¨ í–¥ìƒ")
    print(f"  - í´ë˜ìŠ¤ ê°„ ì„±ëŠ¥ ê· í˜•")
    
    print("\nğŸ“Š ë°©ë²• 3: Random Forest (ê· í˜• ëª¨ë“œ)")
    print("-" * 60)
    
    pipe_rf = Pipeline([
        ('scaler', StandardScaler()),
        ('clf', RandomForestClassifier(class_weight='balanced', n_estimators=100, random_state=42))
    ])
    pipe_rf.fit(X_train, y_train)
    
    y_pred_rf = pipe_rf.predict(X_test)
    
    print("\në¶„ë¥˜ ë¦¬í¬íŠ¸:")
    print(classification_report(y_test, y_pred_rf, target_names=['ì•…ì„±', 'ì–‘ì„±']))
    
    print("\nğŸ“Š ë°©ë²• ë¹„êµ (AUC ê¸°ì¤€)")
    print("-" * 60)
    
    models_comparison = {
        'ê¸°ë³¸ ëª¨ë¸': pipe_basic,
        'ê°€ì¤‘ì¹˜ ì ìš©': pipe_weighted,
        'Random Forest': pipe_rf
    }
    
    for name, model in models_comparison.items():
        y_pred_proba = model.predict_proba(X_test)[:, 1]
        auc_score = roc_auc_score(y_test, y_pred_proba)
        
        y_pred = model.predict(X_test)
        f1_malignant = f1_score(y_test, y_pred, pos_label=0)
        f1_benign = f1_score(y_test, y_pred, pos_label=1)
        
        print(f"\n{name}:")
        print(f"  AUC: {auc_score:.4f}")
        print(f"  F1 (ì•…ì„±): {f1_malignant:.4f}")
        print(f"  F1 (ì–‘ì„±): {f1_benign:.4f}")
    
    print("\nâœ… ë¶ˆê· í˜• í´ë˜ìŠ¤ ì²˜ë¦¬ ìš”ì•½:")
    print("  1. í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜: ê°„ë‹¨í•˜ê³  íš¨ê³¼ì ")
    print("  2. ë¦¬ìƒ˜í”Œë§: ë°ì´í„° í¬ê¸° ì¡°ì • ê°€ëŠ¥")
    print("  3. ì•™ìƒë¸”: Random Forest ë“± íš¨ê³¼ì ")
    print("  4. í‰ê°€ ì§€í‘œ: AUC, F1 ì ìˆ˜ ì‚¬ìš© í•„ìˆ˜")
    print("  5. ì •í™•ë„ëŠ” ì˜ë¯¸ ì—†ìŒ!")


# ============================================================================
# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# ============================================================================

def main():
    """
    ëª¨ë“  ì„¹ì…˜ì„ ìˆœì„œëŒ€ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.
    """
    print("\n" + "="*80)
    print("  ëª¨ë¸ í‰ê°€ì™€ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ - ì‹¤ìŠµ")
    print("  Model Evaluation and Hyperparameter Tuning")
    print("="*80)
    print("\nì´ íŠœí† ë¦¬ì–¼ì—ì„œ ë°°ìš¸ ë‚´ìš©:")
    print("  1. ìœ„ìŠ¤ì½˜ì‹  ìœ ë°©ì•” ë°ì´í„°ì…‹ íƒìƒ‰")
    print("  2. íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ íš¨ìœ¨ì ì¸ ì›Œí¬í”Œë¡œ êµ¬ì„±")
    print("  3. k-ê²¹ êµì°¨ ê²€ì¦ìœ¼ë¡œ ì‹ ë¢°ì„± ìˆëŠ” í‰ê°€")
    print("  4. í•™ìŠµ/ê²€ì¦ ê³¡ì„ ìœ¼ë¡œ ëª¨ë¸ ì§„ë‹¨")
    print("  5. ê·¸ë¦¬ë“œ/ëœë¤ ì„œì¹˜ë¡œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹")
    print("  6. ë‹¤ì–‘í•œ ì„±ëŠ¥ í‰ê°€ ì§€í‘œ ì´í•´")
    print("  7. ë¶ˆê· í˜• í´ë˜ìŠ¤ ì²˜ë¦¬ ê¸°ë²•")
    print("\nì´ ì˜ˆìƒ ì‹œê°„: 60-90ë¶„")
    print("="*80)
    
    try:
        # ì„¹ì…˜ 1: ë°ì´í„° ë¡œë“œ
        X, y, data = section1_load_data()
        
        # ì„¹ì…˜ 2: íŒŒì´í”„ë¼ì¸ ê¸°ë³¸
        pipe = section2_basic_pipeline(X, y)
        
        # ì„¹ì…˜ 3: êµì°¨ ê²€ì¦
        section3_cross_validation(X, y)
        
        # ì„¹ì…˜ 4: í•™ìŠµ ê³¡ì„ 
        section4_learning_curves(X, y)
        
        # ì„¹ì…˜ 5: ê²€ì¦ ê³¡ì„ 
        section5_validation_curves(X, y)
        
        # ì„¹ì…˜ 6: ê·¸ë¦¬ë“œ ì„œì¹˜
        grid_search = section6_grid_search(X, y)
        
        # ì„¹ì…˜ 7: ëœë¤ ì„œì¹˜
        random_search = section7_random_search(X, y)
        
        # ì„¹ì…˜ 8: ë¶„ë¥˜ ì§€í‘œ
        section8_classification_metrics(X, y)
        
        # ì„¹ì…˜ 9: ROC ê³¡ì„ 
        section9_roc_curve(X, y)
        
        # ì„¹ì…˜ 10: ë¶ˆê· í˜• í´ë˜ìŠ¤
        section10_imbalanced_classes()
        
        print("\n" + "="*80)
        print("  ğŸ‰ ëª¨ë“  ì„¹ì…˜ ì™„ë£Œ!")
        print("="*80)
        print("\nìƒì„±ëœ íŒŒì¼:")
        print("  - /tmp/learning_curves.png")
        print("  - /tmp/validation_curves.png")
        print("  - /tmp/confusion_matrix.png")
        print("  - /tmp/roc_curves.png")
        print("  - /tmp/pr_curves.png")
        print("\në‹¤ìŒ ë‹¨ê³„:")
        print("  1. ìƒì„±ëœ ê·¸ë˜í”„ë¥¼ í™•ì¸í•˜ì„¸ìš”")
        print("  2. ì½”ë“œë¥¼ ìˆ˜ì •í•˜ë©° ì‹¤í—˜í•´ë³´ì„¸ìš”")
        print("  3. ìì‹ ì˜ ë°ì´í„°ì…‹ì— ì ìš©í•´ë³´ì„¸ìš”")
        print("  4. model_evaluation_theory.mdì—ì„œ ì´ë¡ ì„ ë³µìŠµí•˜ì„¸ìš”")
        print("\ní–‰ìš´ì„ ë¹•ë‹ˆë‹¤! ğŸš€")
        print("="*80 + "\n")
        
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()

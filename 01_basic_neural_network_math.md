# 신경망 학습의 수학적 기초 (기본 단계)

## 목차
1. [퍼셉트론 (Perceptron)](#1-퍼셉트론-perceptron)
2. [활성화 함수 (Activation Functions)](#2-활성화-함수-activation-functions)
3. [순방향 전파 (Forward Propagation)](#3-순방향-전파-forward-propagation)
4. [손실 함수 (Loss Functions)](#4-손실-함수-loss-functions)

---

## 1. 퍼셉트론 (Perceptron)

### 1.1 퍼셉트론의 정의

퍼셉트론은 가장 기본적인 인공 신경망 단위입니다.

#### 수식
```
y = f(w₁x₁ + w₂x₂ + ... + wₙxₙ + b)
```

또는 벡터 표기법:
```
y = f(w^T x + b)
```

#### 기호 설명 (Legend)
- `x = [x₁, x₂, ..., xₙ]`: 입력 벡터 (input vector)
- `w = [w₁, w₂, ..., wₙ]`: 가중치 벡터 (weight vector)
- `b`: 편향 (bias)
- `w^T x`: 가중치와 입력의 내적 (dot product)
- `f(·)`: 활성화 함수 (activation function)
- `y`: 출력 (output)

### 1.2 수치 예제

입력과 가중치가 다음과 같이 주어졌을 때:
- 입력: `x = [2, 3]`
- 가중치: `w = [0.5, 0.3]`
- 편향: `b = 0.1`
- 활성화 함수: 계단 함수 (step function), `f(z) = 1 if z > 0 else 0`

**계산 과정:**

1. 가중합 계산:
   ```
   z = w^T x + b
     = (0.5 × 2) + (0.3 × 3) + 0.1
     = 1.0 + 0.9 + 0.1
     = 2.0
   ```

2. 활성화 함수 적용:
   ```
   y = f(2.0) = 1  (2.0 > 0이므로)
   ```

**결과:** 출력은 1

---

## 2. 활성화 함수 (Activation Functions)

활성화 함수는 신경망에 비선형성을 도입하여 복잡한 패턴을 학습할 수 있게 합니다.

### 2.1 Sigmoid 함수

#### 수식
```
σ(z) = 1 / (1 + e^(-z))
```

#### 기호 설명
- `z`: 입력 값 (pre-activation)
- `e`: 자연상수 (≈ 2.71828)
- `σ(z)`: sigmoid 함수의 출력 (0과 1 사이)

#### 미분
```
dσ(z)/dz = σ(z) × (1 - σ(z))
```

#### 수치 예제
`z = 0`일 때:
```
σ(0) = 1 / (1 + e^0) = 1 / (1 + 1) = 0.5
```

`z = 2`일 때:
```
σ(2) = 1 / (1 + e^(-2)) = 1 / (1 + 0.1353) ≈ 0.8808
```

`z = -2`일 때:
```
σ(-2) = 1 / (1 + e^2) = 1 / (1 + 7.389) ≈ 0.1192
```

### 2.2 ReLU (Rectified Linear Unit)

#### 수식
```
ReLU(z) = max(0, z) = {
    z  if z > 0
    0  if z ≤ 0
}
```

#### 기호 설명
- `z`: 입력 값
- `max(0, z)`: 0과 z 중 큰 값

#### 미분
```
dReLU(z)/dz = {
    1  if z > 0
    0  if z ≤ 0
}
```

#### 수치 예제
```
ReLU(2) = max(0, 2) = 2
ReLU(-1.5) = max(0, -1.5) = 0
ReLU(0) = max(0, 0) = 0
```

### 2.3 Tanh (Hyperbolic Tangent)

#### 수식
```
tanh(z) = (e^z - e^(-z)) / (e^z + e^(-z))
```

또는:
```
tanh(z) = 2σ(2z) - 1
```

#### 기호 설명
- `z`: 입력 값
- `e`: 자연상수
- `σ`: sigmoid 함수
- 출력 범위: [-1, 1]

#### 미분
```
dtanh(z)/dz = 1 - tanh²(z)
```

#### 수치 예제
`z = 0`일 때:
```
tanh(0) = (e^0 - e^0) / (e^0 + e^0) = 0 / 2 = 0
```

`z = 1`일 때:
```
tanh(1) = (e^1 - e^(-1)) / (e^1 + e^(-1))
        = (2.718 - 0.368) / (2.718 + 0.368)
        ≈ 0.7616
```

---

## 3. 순방향 전파 (Forward Propagation)

### 3.1 단일 층 신경망

#### 수식
층 l에서의 연산:
```
z^[l] = W^[l] a^[l-1] + b^[l]
a^[l] = f^[l](z^[l])
```

#### 기호 설명
- `l`: 층 번호 (layer index)
- `W^[l]`: l번째 층의 가중치 행렬 (weight matrix)
- `a^[l-1]`: 이전 층(l-1)의 활성화 값 (activation from previous layer)
- `b^[l]`: l번째 층의 편향 벡터 (bias vector)
- `z^[l]`: l번째 층의 선형 결합 (pre-activation)
- `f^[l](·)`: l번째 층의 활성화 함수
- `a^[l]`: l번째 층의 활성화 값 (activation)

### 3.2 수치 예제: 2층 신경망

**네트워크 구조:**
- 입력층: 2개 뉴런
- 은닉층: 2개 뉴런 (ReLU)
- 출력층: 1개 뉴런 (Sigmoid)

**주어진 값:**

입력:
```
a^[0] = [1.0, 2.0]^T
```

1층 (은닉층) 파라미터:
```
W^[1] = [[0.5, 0.3],
         [0.2, 0.4]]
b^[1] = [0.1, 0.2]^T
```

2층 (출력층) 파라미터:
```
W^[2] = [[0.6, 0.7]]
b^[2] = [0.15]
```

**계산 과정:**

**1층 (은닉층):**

선형 결합:
```
z^[1] = W^[1] a^[0] + b^[1]

z₁^[1] = (0.5 × 1.0) + (0.3 × 2.0) + 0.1 = 0.5 + 0.6 + 0.1 = 1.2
z₂^[1] = (0.2 × 1.0) + (0.4 × 2.0) + 0.2 = 0.2 + 0.8 + 0.2 = 1.2

z^[1] = [1.2, 1.2]^T
```

활성화 (ReLU):
```
a^[1] = ReLU(z^[1]) = [1.2, 1.2]^T
```

**2층 (출력층):**

선형 결합:
```
z^[2] = W^[2] a^[1] + b^[2]
      = (0.6 × 1.2) + (0.7 × 1.2) + 0.15
      = 0.72 + 0.84 + 0.15
      = 1.71
```

활성화 (Sigmoid):
```
a^[2] = σ(1.71) = 1 / (1 + e^(-1.71)) ≈ 0.8467
```

**최종 출력:** `y = 0.8467`

---

## 4. 손실 함수 (Loss Functions)

손실 함수는 모델의 예측과 실제 값의 차이를 측정합니다.

### 4.1 평균 제곱 오차 (Mean Squared Error, MSE)

#### 수식
```
L(y, ŷ) = (1/2) × (y - ŷ)²
```

전체 데이터셋에 대한 손실:
```
J = (1/m) × Σᵢ₌₁ᵐ (yⁱ - ŷⁱ)²
```

#### 기호 설명
- `y`: 실제 값 (true value, target)
- `ŷ` (y-hat): 예측 값 (predicted value)
- `m`: 샘플 개수 (number of samples)
- `i`: 샘플 인덱스
- `L`: 단일 샘플의 손실 (loss for single sample)
- `J`: 전체 데이터셋의 비용 (cost over entire dataset)

#### 미분
```
∂L/∂ŷ = -(y - ŷ)
```

#### 수치 예제

**단일 예제:**
- 실제 값: `y = 1.0`
- 예측 값: `ŷ = 0.8`

```
L = (1/2) × (1.0 - 0.8)²
  = (1/2) × (0.2)²
  = (1/2) × 0.04
  = 0.02
```

**여러 예제:**
| 샘플 | y | ŷ | (y - ŷ)² |
|------|---|---|----------|
| 1 | 1.0 | 0.8 | 0.04 |
| 2 | 0.0 | 0.2 | 0.04 |
| 3 | 1.0 | 0.9 | 0.01 |

```
J = (1/3) × (0.04 + 0.04 + 0.01) = (1/3) × 0.09 = 0.03
```

### 4.2 교차 엔트로피 손실 (Cross-Entropy Loss)

#### 이진 분류 (Binary Classification)

**수식:**
```
L(y, ŷ) = -[y log(ŷ) + (1 - y) log(1 - ŷ)]
```

#### 기호 설명
- `y`: 실제 레이블 (0 또는 1)
- `ŷ`: 예측 확률 (0과 1 사이)
- `log`: 자연 로그 (natural logarithm)

#### 미분
```
∂L/∂ŷ = -y/ŷ + (1-y)/(1-ŷ)
```

Sigmoid와 함께 사용할 때:
```
∂L/∂z = ŷ - y
```

#### 수치 예제

**예제 1:** y = 1, ŷ = 0.9
```
L = -[1 × log(0.9) + 0 × log(0.1)]
  = -log(0.9)
  = -(-0.1054)
  ≈ 0.1054
```

**예제 2:** y = 0, ŷ = 0.2
```
L = -[0 × log(0.2) + 1 × log(0.8)]
  = -log(0.8)
  = -(-0.2231)
  ≈ 0.2231
```

**예제 3:** y = 1, ŷ = 0.5 (완전히 불확실)
```
L = -[1 × log(0.5) + 0 × log(0.5)]
  = -log(0.5)
  ≈ 0.6931
```

### 4.3 다중 클래스 교차 엔트로피 (Multi-class Cross-Entropy)

#### 수식
```
L = -Σⱼ₌₁ᶜ yⱼ log(ŷⱼ)
```

#### 기호 설명
- `C`: 클래스 개수 (number of classes)
- `j`: 클래스 인덱스
- `yⱼ`: j번째 클래스의 실제 레이블 (one-hot encoded, 0 또는 1)
- `ŷⱼ`: j번째 클래스의 예측 확률

#### 수치 예제

3개 클래스 분류 문제:
- 실제 레이블 (one-hot): `y = [0, 1, 0]` (클래스 2)
- 예측 확률: `ŷ = [0.1, 0.7, 0.2]`

```
L = -[0 × log(0.1) + 1 × log(0.7) + 0 × log(0.2)]
  = -log(0.7)
  = -(-0.3567)
  ≈ 0.3567
```

---

## 요약

### 핵심 개념
1. **퍼셉트론**: 신경망의 기본 단위, 선형 결합 + 활성화 함수
2. **활성화 함수**: 비선형성 도입 (Sigmoid, ReLU, Tanh)
3. **순방향 전파**: 입력에서 출력으로 데이터 흐름
4. **손실 함수**: 예측과 실제 값의 차이 측정

### 수식 체크리스트
- ✓ 퍼셉트론: `y = f(w^T x + b)`
- ✓ Sigmoid: `σ(z) = 1/(1 + e^(-z))`
- ✓ ReLU: `ReLU(z) = max(0, z)`
- ✓ 순방향 전파: `z^[l] = W^[l] a^[l-1] + b^[l]`, `a^[l] = f(z^[l])`
- ✓ MSE: `L = (1/2)(y - ŷ)²`
- ✓ Cross-Entropy: `L = -[y log(ŷ) + (1-y) log(1-ŷ)]`

### 다음 단계
중급 단계에서는 **역전파(Backpropagation)** 알고리즘을 학습하여 이러한 손실을 최소화하는 방법을 배웁니다.

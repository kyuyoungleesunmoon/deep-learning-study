# ğŸ“– Chapter 11: ë¬¸ì¥ ì„ë² ë”© (Sentence Embeddings)

## ğŸ“‹ ê°œìš”

ì´ ì±•í„°ì—ì„œëŠ” ë¬¸ì¥ì„ ì˜ë¯¸ìˆëŠ” ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí•©ë‹ˆë‹¤.
- Sentence-Transformersë¥¼ í™œìš©í•œ ë¬¸ì¥ ì„ë² ë”©
- Mean Pooling ê¸°ë²•
- í•œêµ­ì–´ ëª¨ë¸ í™œìš© (KR-SBERT)

## ğŸ”¬ í•µì‹¬ ì•Œê³ ë¦¬ì¦˜

### 1. Sentence-BERT (SBERT)

**ì›ë¦¬**: BERTì˜ ì¶œë ¥ì„ Poolingí•˜ì—¬ ê³ ì • í¬ê¸° ë¬¸ì¥ ë²¡í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

```
[CLS] ë¬¸ì¥ [SEP] â†’ BERT â†’ í† í° ë²¡í„°ë“¤ â†’ Mean Pooling â†’ ë¬¸ì¥ ë²¡í„°
```

**ê¸°ì¡´ BERTì˜ í•œê³„**:
- ë¬¸ì¥ ìœ ì‚¬ë„ ê³„ì‚° ì‹œ ëª¨ë“  ìŒì„ Cross-Encoderë¡œ ë¹„êµ â†’ O(nÂ²) ë³µì¡ë„
- 10,000ê°œ ë¬¸ì¥ ë¹„êµ ì‹œ ì•½ 65ì‹œê°„ ì†Œìš”

**SBERTì˜ í•´ê²°ì±…**:
- ê° ë¬¸ì¥ì„ ë…ë¦½ì ìœ¼ë¡œ ì¸ì½”ë”© â†’ O(n) ë³µì¡ë„
- ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¡œ ë¹ ë¥¸ ë¹„êµ ê°€ëŠ¥
- 10,000ê°œ ë¬¸ì¥ ë¹„êµ ì‹œ ì•½ 5ì´ˆ ì†Œìš”

### 2. Mean Pooling

**ì›ë¦¬**: BERT ì¶œë ¥ì˜ ëª¨ë“  í† í° ë²¡í„°ë¥¼ í‰ê· í•˜ì—¬ ë¬¸ì¥ ë²¡í„° ìƒì„±

```python
# Attention Maskë¥¼ ê³ ë ¤í•œ Mean Pooling
def mean_pooling(model_output, attention_mask):
    token_embeddings = model_output[0]  # (batch, seq_len, hidden)
    
    # Attention Mask í™•ì¥
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size())
    
    # ë§ˆìŠ¤í‚¹ëœ í† í°ì€ ì œì™¸í•˜ê³  í‰ê· 
    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, dim=1)
    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)
    
    return sum_embeddings / sum_mask
```

**Pooling ë°©ë²• ë¹„êµ**:
| ë°©ë²• | ì„¤ëª… | ì„±ëŠ¥ |
|------|------|------|
| [CLS] í† í° | ì²« ë²ˆì§¸ í† í°ë§Œ ì‚¬ìš© | ë³´í†µ |
| Mean Pooling | ëª¨ë“  í† í° í‰ê·  | **ê°€ì¥ ì¢‹ìŒ** |
| Max Pooling | ê° ì°¨ì›ì˜ ìµœëŒ“ê°’ | ë³´í†µ |

### 3. Contrastive Learning

**ì›ë¦¬**: ìœ ì‚¬í•œ ë¬¸ì¥ì€ ê°€ê¹ê²Œ, ë‹¤ë¥¸ ë¬¸ì¥ì€ ë©€ê²Œ í•™ìŠµ

**ì†ì‹¤ í•¨ìˆ˜ (Contrastive Loss)**:
```
L = (1-y) Ã— Â½ Ã— DÂ² + y Ã— Â½ Ã— max(0, margin - D)Â²
```

- `y = 0`: ìœ ì‚¬í•œ ìŒ â†’ ê±°ë¦¬ D ìµœì†Œí™”
- `y = 1`: ë‹¤ë¥¸ ìŒ â†’ ê±°ë¦¬ê°€ marginë³´ë‹¤ í¬ë„ë¡

**Triplet Loss**:
```
L = max(0, ||anchor - positive||Â² - ||anchor - negative||Â² + margin)
```

- anchorì™€ positiveëŠ” ê°€ê¹ê²Œ
- anchorì™€ negativeëŠ” ë©€ê²Œ

## ğŸ“Š í•œêµ­ì–´ ë¬¸ì¥ ì„ë² ë”© ëª¨ë¸

### ì¶”ì²œ ëª¨ë¸

| ëª¨ë¸ | ì„¤ëª… | ìš©ë„ |
|------|------|------|
| `snunlp/KR-SBERT-V40K-klueNLI-augSTS` | 40K ì–´íœ˜, KLUE ë°ì´í„° í•™ìŠµ | ë²”ìš© |
| `jhgan/ko-sbert-nli` | NLI ë°ì´í„° í•™ìŠµ | ë¬¸ì¥ ìœ ì‚¬ë„ |
| `BM-K/KoSimCSE-roberta` | SimCSE ê¸°ë²• ì ìš© | ë¬¸ì¥ ìœ ì‚¬ë„ |
| `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2` | ë‹¤êµ­ì–´ ì§€ì› | ë‹¤êµ­ì–´ í™˜ê²½ |

## ğŸ“Š ì‹¤ìŠµ ì˜ˆì œ

### ì˜ˆì œ 1: Sentence-Transformersë¡œ ë¬¸ì¥ ì„ë² ë”©

```python
from sentence_transformers import SentenceTransformer

# í•œêµ­ì–´ ëª¨ë¸ ë¡œë“œ
model = SentenceTransformer('snunlp/KR-SBERT-V40K-klueNLI-augSTS')

sentences = [
    "ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì¢‹ìŠµë‹ˆë‹¤.",
    "ì˜¤ëŠ˜ í•˜ëŠ˜ì´ ë§‘ì•„ìš”.",
    "í”„ë¡œê·¸ë˜ë°ì„ ë°°ìš°ê³  ìˆìŠµë‹ˆë‹¤."
]

# ë¬¸ì¥ ì„ë² ë”© ìƒì„±
embeddings = model.encode(sentences)
print(f"ì„ë² ë”© í¬ê¸°: {embeddings.shape}")  # (3, 768)

# ë¬¸ì¥ ìœ ì‚¬ë„ ê³„ì‚°
from sklearn.metrics.pairwise import cosine_similarity
similarity_matrix = cosine_similarity(embeddings)
print(f"ìœ ì‚¬ë„ í–‰ë ¬:\n{similarity_matrix}")
```

### ì˜ˆì œ 2: ì»¤ìŠ¤í…€ Pooling Layer ì¶”ê°€

```python
from sentence_transformers import SentenceTransformer, models

# Transformer ëª¨ë¸ ë¡œë“œ
transformer = models.Transformer('klue/roberta-base')

# Mean Pooling ë ˆì´ì–´ ì¶”ê°€
pooling = models.Pooling(
    transformer.get_word_embedding_dimension(),
    pooling_mode_mean_tokens=True,  # Mean Pooling ì‚¬ìš©
    pooling_mode_cls_token=False,
    pooling_mode_max_tokens=False
)

# ëª¨ë¸ ì¡°ë¦½
model = SentenceTransformer(modules=[transformer, pooling])

# ë¬¸ì¥ ì„ë² ë”© ìƒì„±
sentences = ["ì•ˆë…•í•˜ì„¸ìš”", "ë°˜ê°‘ìŠµë‹ˆë‹¤"]
embeddings = model.encode(sentences)
```

### ì˜ˆì œ 3: KLUE STS ë°ì´í„°ì…‹ìœ¼ë¡œ ìœ ì‚¬ë„ ì¸¡ì •

```python
from datasets import load_dataset
from sentence_transformers import SentenceTransformer
from scipy.stats import spearmanr

# ë°ì´í„°ì…‹ ë¡œë“œ
dataset = load_dataset('klue', 'sts', split='validation')

# ëª¨ë¸ ë¡œë“œ
model = SentenceTransformer('snunlp/KR-SBERT-V40K-klueNLI-augSTS')

# ì˜ˆì¸¡ ë° í‰ê°€
predictions = []
labels = []

for item in dataset:
    # ë‘ ë¬¸ì¥ ì„ë² ë”©
    emb1 = model.encode(item['sentence1'])
    emb2 = model.encode(item['sentence2'])
    
    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
    similarity = cosine_similarity([emb1], [emb2])[0][0]
    predictions.append(similarity)
    labels.append(item['labels']['label'] / 5.0)  # 0-5 â†’ 0-1 ì •ê·œí™”

# Spearman ìƒê´€ê³„ìˆ˜
correlation, _ = spearmanr(predictions, labels)
print(f"Spearman Correlation: {correlation:.4f}")
```

### ì˜ˆì œ 4: ë¬¸ì¥ ì„ë² ë”©ìœ¼ë¡œ ìœ ì‚¬ ë¬¸ì¥ ê²€ìƒ‰

```python
import numpy as np
from sentence_transformers import SentenceTransformer

model = SentenceTransformer('snunlp/KR-SBERT-V40K-klueNLI-augSTS')

# ë¬¸ì„œ ë°ì´í„°ë² ì´ìŠ¤
documents = [
    "ì¸ê³µì§€ëŠ¥ì€ ë¯¸ë˜ ê¸°ìˆ ì˜ í•µì‹¬ì…ë‹ˆë‹¤.",
    "ë¨¸ì‹ ëŸ¬ë‹ì€ ë°ì´í„°ì—ì„œ íŒ¨í„´ì„ í•™ìŠµí•©ë‹ˆë‹¤.",
    "ë”¥ëŸ¬ë‹ì€ ì‹¬ì¸µ ì‹ ê²½ë§ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.",
    "ì˜¤ëŠ˜ ì ì‹¬ ë©”ë‰´ëŠ” ê¹€ì¹˜ì°Œê°œì…ë‹ˆë‹¤.",
    "íŒŒì´ì¬ì€ ì¸ê¸°ìˆëŠ” í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤."
]

# ë¬¸ì„œ ì„ë² ë”© (ì˜¤í”„ë¼ì¸ì—ì„œ ë¯¸ë¦¬ ê³„ì‚°)
doc_embeddings = model.encode(documents)

def search(query, top_k=3):
    """ì¿¼ë¦¬ì™€ ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰"""
    query_embedding = model.encode([query])[0]
    
    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
    similarities = np.dot(doc_embeddings, query_embedding) / (
        np.linalg.norm(doc_embeddings, axis=1) * np.linalg.norm(query_embedding)
    )
    
    # ìƒìœ„ kê°œ ë°˜í™˜
    top_indices = np.argsort(similarities)[-top_k:][::-1]
    
    results = []
    for idx in top_indices:
        results.append({
            'document': documents[idx],
            'similarity': similarities[idx]
        })
    
    return results

# ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
query = "AI ê¸°ìˆ ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”"
results = search(query)

print(f"ì¿¼ë¦¬: '{query}'")
for i, result in enumerate(results, 1):
    print(f"{i}. [{result['similarity']:.4f}] {result['document']}")
```

## ğŸ¯ í•µì‹¬ í¬ì¸íŠ¸

1. **Mean Poolingì´ ê°€ì¥ íš¨ê³¼ì **: [CLS] í† í°ë³´ë‹¤ ëª¨ë“  í† í° í‰ê· ì´ ë” ì¢‹ì€ í‘œí˜„
2. **Contrastive Learning**: ìœ ì‚¬í•œ ìŒì€ ê°€ê¹ê²Œ, ë‹¤ë¥¸ ìŒì€ ë©€ê²Œ í•™ìŠµ
3. **í•œêµ­ì–´ ì „ìš© ëª¨ë¸ ì‚¬ìš©**: ì˜ì–´ ëª¨ë¸ë³´ë‹¤ í•œêµ­ì–´ ì „ìš© ëª¨ë¸ì´ ì„±ëŠ¥ ì¢‹ìŒ
4. **ì •ê·œí™” í•„ìˆ˜**: ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ì‚¬ìš© ì‹œ ë²¡í„° ì •ê·œí™”ë¡œ ì¼ê´€ëœ ìŠ¤ì¼€ì¼ ìœ ì§€

## ğŸ“š ì°¸ê³  ìë£Œ

- ì›ë³¸ ì½”ë“œ: https://github.com/onlybooks/llm/tree/main/11ì¥
- Sentence-Transformers: https://www.sbert.net/
- KLUE ë²¤ì¹˜ë§ˆí¬: https://klue-benchmark.com/

# ğŸ“– Chapter 16: Mamba ì•„í‚¤í…ì²˜

## ğŸ“‹ ê°œìš”

ì´ ì±•í„°ì—ì„œëŠ” Transformerì˜ ëŒ€ì•ˆ ì•„í‚¤í…ì²˜ì¸ Mambaë¥¼ í•™ìŠµí•©ë‹ˆë‹¤.
- State Space Models (SSM)
- Selective State Space (S6)
- ì„ í˜• ì‹œê°„ ë³µì¡ë„ì˜ ì¥ì 

## ğŸ”¬ í•µì‹¬ ì•Œê³ ë¦¬ì¦˜

### 1. Transformerì˜ í•œê³„

**Self-Attentionì˜ ë¬¸ì œì **:
```
ì‹œê°„ ë³µì¡ë„: O(nÂ²) (n: ì‹œí€€ìŠ¤ ê¸¸ì´)
ê³µê°„ ë³µì¡ë„: O(nÂ²)
```

ì˜ˆ: 100K í† í° ì²˜ë¦¬ ì‹œ 10ì–µ ë²ˆ ì—°ì‚° í•„ìš”!

**Mambaì˜ í•´ê²°ì±…**:
```
ì‹œê°„ ë³µì¡ë„: O(n) ì„ í˜•
ê³µê°„ ë³µì¡ë„: O(1) ìƒìˆ˜
```

### 2. State Space Models (SSM)

**ì—°ì† ì‹œê°„ SSM**:
```
h'(t) = AÂ·h(t) + BÂ·x(t)
y(t) = CÂ·h(t) + DÂ·x(t)
```

**ì´ì‚°í™” (Discretization)**:
```
h_k = Ä€Â·h_{k-1} + BÌ„Â·x_k
y_k = CÂ·h_k
```

**ìˆ˜ì‹ ì„¤ëª…**:
- `h`: ì€ë‹‰ ìƒíƒœ (hidden state)
- `x`: ì…ë ¥
- `y`: ì¶œë ¥
- `A, B, C, D`: í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°
- `Ä€, BÌ„`: ì´ì‚°í™”ëœ í–‰ë ¬

### 3. S6 (Selective State Space)

**ê¸°ì¡´ SSMì˜ ë¬¸ì œ**:
- íŒŒë¼ë¯¸í„° A, B, Cê°€ ì…ë ¥ì— ë¬´ê´€ (time-invariant)
- ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ ëŠ¥ë ¥ ë¶€ì¡±

**Mambaì˜ í•´ê²°ì±…**:
- íŒŒë¼ë¯¸í„°ë¥¼ ì…ë ¥ ì˜ì¡´ì ìœ¼ë¡œ ë³€í™˜
- ì„ íƒì  ì •ë³´ í•„í„°ë§

```python
# ì…ë ¥ xì—ì„œ íŒŒë¼ë¯¸í„° ìƒì„±
Î” = softplus(Linear(x))  # ì‹œê°„ ê°„ê²©
B = Linear(x)            # ì…ë ¥ í–‰ë ¬
C = Linear(x)            # ì¶œë ¥ í–‰ë ¬
```

### 4. Mamba Block êµ¬ì¡°

```
Input (b, l, d)
    â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                  â”‚
    â–¼                  â”‚
Linear (2 Ã— d_inner)   â”‚
    â”‚                  â”‚
    â”œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚     â”‚            â”‚
    â–¼     â–¼            â”‚
   SiLU   x            â”‚
    â”‚     â”‚            â”‚
    â–¼     â”‚            â”‚
Conv1D   â”‚            â”‚
    â”‚     â”‚            â”‚
    â–¼     â”‚            â”‚
  SiLU    â”‚            â”‚
    â”‚     â”‚            â”‚
    â–¼     â”‚            â”‚
  SSM     â”‚            â”‚
    â”‚     â”‚            â”‚
    â–¼     â–¼            â”‚
    Ã—â”€â”€â”€â”€â”€â”˜            â”‚
    â”‚                  â”‚
    â–¼                  â”‚
Linear (d)             â”‚
    â”‚                  â”‚
    â–¼                  â”‚
   Add â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
Output (b, l, d)
```

### 5. í•µì‹¬ í˜ì‹ 

| íŠ¹ì„± | Transformer | Mamba |
|------|-------------|-------|
| ì‹œê°„ ë³µì¡ë„ | O(nÂ²) | O(n) |
| ë©”ëª¨ë¦¬ | O(nÂ²) | O(1) |
| ê¸´ ì‹œí€€ìŠ¤ | ì–´ë ¤ì›€ | íš¨ìœ¨ì  |
| ë³‘ë ¬í™” | ì¢‹ìŒ | ë³´í†µ |
| ì»¨í…ìŠ¤íŠ¸ | ì „ì²´ ì–´í…ì…˜ | ì„ íƒì  |

## ğŸ“Š ì‹¤ìŠµ ì˜ˆì œ

### ì˜ˆì œ 1: SSM ìˆ˜ì‹ ì´í•´

```python
import numpy as np

def discrete_ssm_step(A, B, C, h_prev, x):
    """
    ì´ì‚° SSM í•œ ìŠ¤í… ê³„ì‚°
    
    h_k = AÂ·h_{k-1} + BÂ·x_k
    y_k = CÂ·h_k
    """
    h = A @ h_prev + B @ x
    y = C @ h
    return h, y

# ì˜ˆì‹œ íŒŒë¼ë¯¸í„°
d_state = 16
d_input = 1

np.random.seed(42)
A = np.eye(d_state) * 0.9  # ì•ˆì •ì„±ì„ ìœ„í•´ eigenvalue < 1
B = np.random.randn(d_state, d_input) * 0.1
C = np.random.randn(d_input, d_state) * 0.1

# ì‹œí€€ìŠ¤ ì²˜ë¦¬
sequence = [1.0, 0.5, -0.5, 1.0, 0.0]
h = np.zeros((d_state, 1))

outputs = []
for x in sequence:
    x_vec = np.array([[x]])
    h, y = discrete_ssm_step(A, B, C, h, x_vec)
    outputs.append(y[0, 0])

print(f"ì…ë ¥: {sequence}")
print(f"ì¶œë ¥: {[f'{o:.4f}' for o in outputs]}")
```

### ì˜ˆì œ 2: ì„ íƒì  SSM ê°œë…

```python
import numpy as np

def selective_ssm_step(x, h_prev, W_delta, W_B, W_C, A_log):
    """
    Selective SSM í•œ ìŠ¤í… (Mamba ìŠ¤íƒ€ì¼)
    
    íŒŒë¼ë¯¸í„°ê°€ ì…ë ¥ xì— ì˜ì¡´ì 
    """
    # ì…ë ¥ ì˜ì¡´ì  íŒŒë¼ë¯¸í„° ìƒì„±
    delta = np.maximum(0, W_delta @ x)  # softplus ê·¼ì‚¬
    B = W_B @ x
    C = W_C @ x
    
    # A í–‰ë ¬ (ë¡œê·¸ ìŠ¤ì¼€ì¼ì—ì„œ ë³€í™˜)
    A = np.exp(A_log)
    
    # ì´ì‚°í™” (ê°„ì†Œí™” ë²„ì „)
    A_bar = A * delta
    B_bar = B * delta
    
    # SSM ìŠ¤í…
    h = A_bar * h_prev + B_bar * x
    y = C @ h
    
    return h, y

# ì´ ë°©ì‹ìœ¼ë¡œ ì…ë ¥ì— ë”°ë¼ ì •ë³´ë¥¼ ì„ íƒì ìœ¼ë¡œ ì €ì¥/ì‚­ì œ
```

### ì˜ˆì œ 3: Mamba Block êµ¬ì¡° (PyTorch ìŠ¤íƒ€ì¼)

```python
import torch
import torch.nn as nn

class SimpleMambaBlock(nn.Module):
    """
    ê°„ì†Œí™”ëœ Mamba Block êµ¬í˜„
    
    ì‹¤ì œ MambaëŠ” ë” ë³µì¡í•œ ìµœì í™” í¬í•¨
    """
    
    def __init__(self, d_model=64, d_inner=128, d_state=16, d_conv=4):
        super().__init__()
        self.d_model = d_model
        self.d_inner = d_inner
        self.d_state = d_state
        
        # ì…ë ¥ íˆ¬ì˜
        self.in_proj = nn.Linear(d_model, d_inner * 2, bias=False)
        
        # 1D Convolution
        self.conv1d = nn.Conv1d(
            d_inner, d_inner,
            kernel_size=d_conv,
            padding=d_conv - 1,
            groups=d_inner
        )
        
        # SSM íŒŒë¼ë¯¸í„° ìƒì„±
        self.x_proj = nn.Linear(d_inner, d_state * 2 + 1, bias=False)
        
        # ì¶œë ¥ íˆ¬ì˜
        self.out_proj = nn.Linear(d_inner, d_model, bias=False)
        
        # A í–‰ë ¬ (í•™ìŠµ ê°€ëŠ¥)
        self.A_log = nn.Parameter(torch.randn(d_inner, d_state))
    
    def forward(self, x):
        # x: (batch, seq_len, d_model)
        batch, seq_len, _ = x.shape
        
        # ì…ë ¥ íˆ¬ì˜
        xz = self.in_proj(x)
        x, z = xz.chunk(2, dim=-1)  # x, gate
        
        # Conv1D
        x = x.transpose(1, 2)  # (batch, d_inner, seq_len)
        x = self.conv1d(x)[:, :, :seq_len]
        x = x.transpose(1, 2)  # (batch, seq_len, d_inner)
        
        # í™œì„±í™”
        x = torch.silu(x)
        
        # SSM (ê°„ì†Œí™”)
        # ì‹¤ì œë¡œëŠ” ë” íš¨ìœ¨ì ì¸ êµ¬í˜„ í•„ìš”
        y = self._ssm(x)
        
        # Gating
        y = y * torch.silu(z)
        
        # ì¶œë ¥ íˆ¬ì˜
        return self.out_proj(y)
    
    def _ssm(self, x):
        # ê°„ì†Œí™”ëœ SSM
        # ì‹¤ì œ êµ¬í˜„ì€ scan ì—°ì‚° ì‚¬ìš©
        return x  # ë°ëª¨ìš© identity

# ì‚¬ìš© ì˜ˆì‹œ
model = SimpleMambaBlock(d_model=64)
x = torch.randn(2, 100, 64)  # (batch, seq_len, d_model)
y = model(x)
print(f"ì…ë ¥ shape: {x.shape}")
print(f"ì¶œë ¥ shape: {y.shape}")
```

## ğŸ¯ í•µì‹¬ í¬ì¸íŠ¸

1. **ì„ í˜• ì‹œê°„ ë³µì¡ë„**: Transformerì˜ O(nÂ²) â†’ O(n)
2. **ì„ íƒì  ë©”ì»¤ë‹ˆì¦˜**: ì…ë ¥ì— ë”°ë¼ ì •ë³´ ì €ì¥/ì‚­ì œ ê²°ì •
3. **ìƒíƒœ ê³µê°„**: RNNê³¼ CNNì˜ ì¥ì  ê²°í•©
4. **ê¸´ ì‹œí€€ìŠ¤**: 100K+ í† í°ë„ íš¨ìœ¨ì  ì²˜ë¦¬

## âš ï¸ ì£¼ì˜ì‚¬í•­

- GPU ìµœì í™” í•„ìš” (CUDA ì»¤ë„)
- ë³‘ë ¬í™”ê°€ Transformerë³´ë‹¤ ì–´ë ¤ì›€
- ì•„ì§ ë°œì „ ì¤‘ì¸ ì•„í‚¤í…ì²˜

## ğŸ“š ì°¸ê³  ìë£Œ

- ì›ë³¸ ì½”ë“œ: https://github.com/onlybooks/llm/tree/main/16ì¥
- Mamba ë…¼ë¬¸: https://arxiv.org/abs/2312.00752
- mamba-minimal: https://github.com/johnma2006/mamba-minimal

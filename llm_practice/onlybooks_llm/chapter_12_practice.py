"""
Chapter 12: ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì‹¤ìŠµ ì½”ë“œ
=======================================

ì´ íŒŒì¼ì€ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì˜ í•µì‹¬ ê°œë…ì„ ì‹¤ìŠµí•©ë‹ˆë‹¤:
1. Brute-force ê²€ìƒ‰ êµ¬í˜„
2. IVF (Inverted File Index) ê°œë…
3. Product Quantization ì›ë¦¬
4. (ì„ íƒ) FAISS ì‚¬ìš©

ì‹¤í–‰ ë°©ë²•:
    pip install numpy
    python chapter_12_practice.py

    # FAISS ì‚¬ìš© ì‹œ:
    pip install faiss-cpu
"""

import numpy as np
from typing import List, Tuple, Dict
from collections import defaultdict
import time
from dataclasses import dataclass


# ============================================================
# Part 1: ê¸°ë³¸ ë²¡í„° ê²€ìƒ‰
# ============================================================

def l2_distance(vec1: np.ndarray, vec2: np.ndarray) -> float:
    """L2 (ìœ í´ë¦¬ë“œ) ê±°ë¦¬ ê³„ì‚°"""
    return np.sqrt(np.sum((vec1 - vec2) ** 2))


def brute_force_search(query: np.ndarray, 
                       database: np.ndarray, 
                       k: int = 5) -> Tuple[np.ndarray, np.ndarray]:
    """
    Brute-force ìµœê·¼ì ‘ ì´ì›ƒ ê²€ìƒ‰
    
    Args:
        query: (d,) ì¿¼ë¦¬ ë²¡í„°
        database: (n, d) ë°ì´í„°ë² ì´ìŠ¤ ë²¡í„°ë“¤
        k: ë°˜í™˜í•  ì´ì›ƒ ìˆ˜
    
    Returns:
        distances: (k,) ê±°ë¦¬
        indices: (k,) ì¸ë±ìŠ¤
    """
    # ëª¨ë“  ë²¡í„°ì™€ì˜ ê±°ë¦¬ ê³„ì‚°
    distances = np.linalg.norm(database - query, axis=1)
    
    # ìƒìœ„ kê°œ ì¸ë±ìŠ¤
    indices = np.argsort(distances)[:k]
    
    return distances[indices], indices


def demo_brute_force():
    """Brute-force ê²€ìƒ‰ ë°ëª¨"""
    print("\n" + "="*60)
    print("ğŸ” Brute-force ê²€ìƒ‰ ë°ëª¨")
    print("="*60)
    
    np.random.seed(42)
    n, d = 1000, 64
    database = np.random.randn(n, d).astype('float32')
    query = np.random.randn(d).astype('float32')
    
    start = time.time()
    distances, indices = brute_force_search(query, database, k=5)
    elapsed = time.time() - start
    
    print(f"ë°ì´í„°ë² ì´ìŠ¤ í¬ê¸°: {n} ë²¡í„° x {d} ì°¨ì›")
    print(f"ê²€ìƒ‰ ì‹œê°„: {elapsed*1000:.2f} ms")
    print(f"ìƒìœ„ 5ê°œ ì¸ë±ìŠ¤: {indices}")
    print(f"ê±°ë¦¬: {distances}")


# ============================================================
# Part 2: IVF (Inverted File Index) êµ¬í˜„
# ============================================================

class SimpleIVFIndex:
    """
    ê°„ë‹¨í•œ IVF ì¸ë±ìŠ¤ êµ¬í˜„
    
    1. í•™ìŠµ: K-meansë¡œ í´ëŸ¬ìŠ¤í„°ë§
    2. ì¶”ê°€: ê° ë²¡í„°ë¥¼ ê°€ì¥ ê°€ê¹Œìš´ í´ëŸ¬ìŠ¤í„°ì— í• ë‹¹
    3. ê²€ìƒ‰: ê°€ì¥ ê°€ê¹Œìš´ nprobeê°œ í´ëŸ¬ìŠ¤í„°ë§Œ íƒìƒ‰
    """
    
    def __init__(self, d: int, nlist: int = 10):
        """
        Args:
            d: ë²¡í„° ì°¨ì›
            nlist: í´ëŸ¬ìŠ¤í„° ìˆ˜
        """
        self.d = d
        self.nlist = nlist
        self.centroids = None
        self.inverted_lists: Dict[int, List[Tuple[int, np.ndarray]]] = defaultdict(list)
        self.is_trained = False
        self.nprobe = 1  # ê²€ìƒ‰í•  í´ëŸ¬ìŠ¤í„° ìˆ˜
    
    def train(self, data: np.ndarray, n_iter: int = 20):
        """
        K-means í´ëŸ¬ìŠ¤í„°ë§ìœ¼ë¡œ centroids í•™ìŠµ
        
        Args:
            data: (n, d) í•™ìŠµ ë°ì´í„°
            n_iter: K-means ë°˜ë³µ íšŸìˆ˜
        """
        n = len(data)
        
        # ëœë¤ ì´ˆê¸°í™”
        indices = np.random.choice(n, self.nlist, replace=False)
        self.centroids = data[indices].copy()
        
        for _ in range(n_iter):
            # í• ë‹¹: ê° ì ì„ ê°€ì¥ ê°€ê¹Œìš´ centroidì— í• ë‹¹
            distances = np.linalg.norm(
                data[:, np.newaxis, :] - self.centroids[np.newaxis, :, :], 
                axis=2
            )
            assignments = np.argmin(distances, axis=1)
            
            # ì—…ë°ì´íŠ¸: ê° í´ëŸ¬ìŠ¤í„°ì˜ í‰ê· ìœ¼ë¡œ centroid ì—…ë°ì´íŠ¸
            for i in range(self.nlist):
                mask = assignments == i
                if np.sum(mask) > 0:
                    self.centroids[i] = np.mean(data[mask], axis=0)
        
        self.is_trained = True
        print(f"í•™ìŠµ ì™„ë£Œ: {self.nlist}ê°œ í´ëŸ¬ìŠ¤í„°")
    
    def add(self, data: np.ndarray):
        """
        ë°ì´í„°ë¥¼ inverted listì— ì¶”ê°€
        
        Args:
            data: (n, d) ì¶”ê°€í•  ë°ì´í„°
        """
        if not self.is_trained:
            raise ValueError("ì¸ë±ìŠ¤ê°€ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. train()ì„ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")
        
        # ê° ë²¡í„°ë¥¼ ê°€ì¥ ê°€ê¹Œìš´ í´ëŸ¬ìŠ¤í„°ì— í• ë‹¹
        distances = np.linalg.norm(
            data[:, np.newaxis, :] - self.centroids[np.newaxis, :, :], 
            axis=2
        )
        assignments = np.argmin(distances, axis=1)
        
        for idx, (vec, cluster_id) in enumerate(zip(data, assignments)):
            self.inverted_lists[cluster_id].append((idx, vec))
        
        print(f"ì¶”ê°€ ì™„ë£Œ: {len(data)}ê°œ ë²¡í„°")
    
    def search(self, query: np.ndarray, k: int = 5) -> Tuple[np.ndarray, np.ndarray]:
        """
        ê²€ìƒ‰ ìˆ˜í–‰
        
        Args:
            query: (d,) ì¿¼ë¦¬ ë²¡í„°
            k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
        
        Returns:
            distances, indices
        """
        # ê°€ì¥ ê°€ê¹Œìš´ nprobeê°œ í´ëŸ¬ìŠ¤í„° ì°¾ê¸°
        centroid_distances = np.linalg.norm(self.centroids - query, axis=1)
        probe_clusters = np.argsort(centroid_distances)[:self.nprobe]
        
        # í•´ë‹¹ í´ëŸ¬ìŠ¤í„°ë“¤ì˜ ë²¡í„°ë§Œ íƒìƒ‰
        candidates = []
        for cluster_id in probe_clusters:
            for idx, vec in self.inverted_lists[cluster_id]:
                dist = np.linalg.norm(vec - query)
                candidates.append((dist, idx))
        
        # ìƒìœ„ kê°œ ì •ë ¬
        candidates.sort(key=lambda x: x[0])
        top_k = candidates[:k]
        
        if not top_k:
            return np.array([]), np.array([])
        
        distances = np.array([d for d, _ in top_k])
        indices = np.array([i for _, i in top_k])
        
        return distances, indices


def demo_ivf():
    """IVF ì¸ë±ìŠ¤ ë°ëª¨"""
    print("\n" + "="*60)
    print("ğŸ“Š IVF ì¸ë±ìŠ¤ ë°ëª¨")
    print("="*60)
    
    np.random.seed(42)
    n, d = 10000, 64
    database = np.random.randn(n, d).astype('float32')
    query = np.random.randn(d).astype('float32')
    
    # IVF ì¸ë±ìŠ¤ ìƒì„±
    index = SimpleIVFIndex(d=d, nlist=100)
    
    # í•™ìŠµ
    start = time.time()
    index.train(database)
    train_time = time.time() - start
    
    # ì¶”ê°€
    start = time.time()
    index.add(database)
    add_time = time.time() - start
    
    # ê²€ìƒ‰ (nprobe ë¹„êµ)
    print("\nnprobeì— ë”°ë¥¸ ê²€ìƒ‰ ì„±ëŠ¥:")
    
    # Ground truth (brute-force)
    gt_distances, gt_indices = brute_force_search(query, database, k=10)
    
    for nprobe in [1, 5, 10, 20]:
        index.nprobe = nprobe
        
        start = time.time()
        distances, indices = index.search(query, k=10)
        search_time = time.time() - start
        
        # Recall ê³„ì‚°
        recall = len(set(indices) & set(gt_indices)) / len(gt_indices) * 100
        
        print(f"  nprobe={nprobe:2d}: ê²€ìƒ‰ {search_time*1000:.2f}ms, Recall@10 = {recall:.1f}%")


# ============================================================
# Part 3: Product Quantization ê°œë…
# ============================================================

class SimpleProductQuantizer:
    """
    ê°„ë‹¨í•œ Product Quantization êµ¬í˜„
    
    ì›ë¦¬:
    1. ë²¡í„°ë¥¼ mê°œì˜ ì„œë¸Œë²¡í„°ë¡œ ë¶„í• 
    2. ê° ì„œë¸Œë²¡í„°ì— ëŒ€í•´ k-means (ksub í´ëŸ¬ìŠ¤í„°)
    3. ê° ì„œë¸Œë²¡í„°ë¥¼ ê°€ì¥ ê°€ê¹Œìš´ í´ëŸ¬ìŠ¤í„° IDë¡œ í‘œí˜„
    
    ë©”ëª¨ë¦¬ ì ˆê°:
    - ì›ë˜: d * 4 bytes (float32)
    - PQ í›„: m * 1 byte (uint8, ksub=256ì¸ ê²½ìš°)
    """
    
    def __init__(self, d: int, m: int = 8, ksub: int = 256):
        """
        Args:
            d: ë²¡í„° ì°¨ì›
            m: ì„œë¸Œë²¡í„° ìˆ˜ (dëŠ” mìœ¼ë¡œ ë‚˜ëˆ„ì–´ ë–¨ì–´ì ¸ì•¼ í•¨)
            ksub: ê° ì„œë¸Œë²¡í„°ì˜ í´ëŸ¬ìŠ¤í„° ìˆ˜
        """
        assert d % m == 0, f"d({d})ëŠ” m({m})ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ë–¨ì–´ì ¸ì•¼ í•©ë‹ˆë‹¤"
        
        self.d = d
        self.m = m
        self.ksub = ksub
        self.dsub = d // m  # ì„œë¸Œë²¡í„° ì°¨ì›
        self.codebooks = None
        self.is_trained = False
    
    def train(self, data: np.ndarray, n_iter: int = 20):
        """
        ê° ì„œë¸Œê³µê°„ì— ëŒ€í•´ k-means í•™ìŠµ
        """
        n = len(data)
        self.codebooks = []
        
        for i in range(self.m):
            # ië²ˆì§¸ ì„œë¸Œë²¡í„° ì¶”ì¶œ
            subvectors = data[:, i * self.dsub : (i + 1) * self.dsub]
            
            # k-means í´ëŸ¬ìŠ¤í„°ë§
            centroids = self._kmeans(subvectors, self.ksub, n_iter)
            self.codebooks.append(centroids)
        
        self.codebooks = np.array(self.codebooks)  # (m, ksub, dsub)
        self.is_trained = True
        print(f"PQ í•™ìŠµ ì™„ë£Œ: {self.m}ê°œ ì„œë¸Œê³µê°„, {self.ksub}ê°œ í´ëŸ¬ìŠ¤í„°/ì„œë¸Œê³µê°„")
    
    def _kmeans(self, data: np.ndarray, k: int, n_iter: int) -> np.ndarray:
        """ê°„ë‹¨í•œ k-means"""
        n = len(data)
        indices = np.random.choice(n, min(k, n), replace=False)
        centroids = data[indices].copy()
        
        if len(centroids) < k:
            # ë°ì´í„°ê°€ kë³´ë‹¤ ì ìœ¼ë©´ íŒ¨ë”©
            padding = np.zeros((k - len(centroids), data.shape[1]))
            centroids = np.vstack([centroids, padding])
        
        for _ in range(n_iter):
            distances = np.linalg.norm(
                data[:, np.newaxis, :] - centroids[np.newaxis, :, :], 
                axis=2
            )
            assignments = np.argmin(distances, axis=1)
            
            for i in range(k):
                mask = assignments == i
                if np.sum(mask) > 0:
                    centroids[i] = np.mean(data[mask], axis=0)
        
        return centroids
    
    def encode(self, data: np.ndarray) -> np.ndarray:
        """
        ë²¡í„°ë¥¼ ì½”ë“œë¡œ ì¸ì½”ë”© (ì••ì¶•)
        
        Args:
            data: (n, d) ì›ë³¸ ë²¡í„°
        
        Returns:
            codes: (n, m) ì½”ë“œ (ê° ì›ì†ŒëŠ” 0~ksub-1)
        """
        n = len(data)
        codes = np.zeros((n, self.m), dtype=np.uint8)
        
        for i in range(self.m):
            subvectors = data[:, i * self.dsub : (i + 1) * self.dsub]
            distances = np.linalg.norm(
                subvectors[:, np.newaxis, :] - self.codebooks[i][np.newaxis, :, :],
                axis=2
            )
            codes[:, i] = np.argmin(distances, axis=1)
        
        return codes
    
    def decode(self, codes: np.ndarray) -> np.ndarray:
        """
        ì½”ë“œë¥¼ ë²¡í„°ë¡œ ë””ì½”ë”© (ë³µì›)
        """
        n = len(codes)
        reconstructed = np.zeros((n, self.d))
        
        for i in range(self.m):
            reconstructed[:, i * self.dsub : (i + 1) * self.dsub] = \
                self.codebooks[i][codes[:, i]]
        
        return reconstructed


def demo_pq():
    """Product Quantization ë°ëª¨"""
    print("\n" + "="*60)
    print("ğŸ“¦ Product Quantization ë°ëª¨")
    print("="*60)
    
    np.random.seed(42)
    n, d = 10000, 128
    database = np.random.randn(n, d).astype('float32')
    
    # PQ ìƒì„± ë° í•™ìŠµ
    m = 8  # ì„œë¸Œë²¡í„° ìˆ˜
    pq = SimpleProductQuantizer(d=d, m=m, ksub=256)
    pq.train(database[:1000])  # ì¼ë¶€ ë°ì´í„°ë¡œ í•™ìŠµ
    
    # ì¸ì½”ë”©
    codes = pq.encode(database)
    
    # ë©”ëª¨ë¦¬ ë¹„êµ
    original_size = n * d * 4  # float32
    compressed_size = n * m * 1  # uint8
    
    print(f"\në©”ëª¨ë¦¬ ë¹„êµ:")
    print(f"  ì›ë³¸: {original_size / 1e6:.2f} MB")
    print(f"  ì••ì¶•: {compressed_size / 1e6:.2f} MB")
    print(f"  ì••ì¶•ë¥ : {original_size / compressed_size:.0f}x")
    
    # ë³µì› ì˜¤ì°¨
    reconstructed = pq.decode(codes)
    mse = np.mean((database - reconstructed) ** 2)
    print(f"\në³µì› ì˜¤ì°¨ (MSE): {mse:.4f}")


# ============================================================
# Part 4: FAISS ì‚¬ìš© (ì„ íƒì )
# ============================================================

def demo_faiss():
    """FAISS ì‚¬ìš© ë°ëª¨"""
    try:
        import faiss
        
        print("\n" + "="*60)
        print("ğŸš€ FAISS ë°ëª¨")
        print("="*60)
        
        np.random.seed(42)
        n, d = 100000, 128
        database = np.random.randn(n, d).astype('float32')
        queries = np.random.randn(100, d).astype('float32')
        
        indexes = {
            'Flat': faiss.IndexFlatL2(d),
            'IVF': None,
            'HNSW': faiss.IndexHNSWFlat(d, 32)
        }
        
        # IVF ì¸ë±ìŠ¤ ìƒì„±
        nlist = 100
        quantizer = faiss.IndexFlatL2(d)
        indexes['IVF'] = faiss.IndexIVFFlat(quantizer, d, nlist)
        
        print(f"\në°ì´í„° í¬ê¸°: {n:,} ë²¡í„° x {d} ì°¨ì›")
        
        for name, index in indexes.items():
            # í•™ìŠµ (í•„ìš”í•œ ê²½ìš°)
            if hasattr(index, 'train') and not index.is_trained:
                index.train(database)
            
            # ì¶”ê°€
            start = time.time()
            index.add(database)
            add_time = time.time() - start
            
            # ê²€ìƒ‰
            if name == 'IVF':
                index.nprobe = 10
            
            start = time.time()
            distances, indices = index.search(queries, k=10)
            search_time = time.time() - start
            
            print(f"\n{name}:")
            print(f"  ì¶”ê°€ ì‹œê°„: {add_time:.3f}s")
            print(f"  ê²€ìƒ‰ ì‹œê°„: {search_time*1000:.2f}ms ({len(queries)}ê°œ ì¿¼ë¦¬)")
        
    except ImportError:
        print("\nâš ï¸ faissê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("ì„¤ì¹˜: pip install faiss-cpu")


# ============================================================
# ë©”ì¸ í•¨ìˆ˜
# ============================================================

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("="*60)
    print("ğŸ¤– Chapter 12: ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì‹¤ìŠµ")
    print("="*60)
    
    demo_brute_force()
    demo_ivf()
    demo_pq()
    demo_faiss()
    
    print("\n" + "="*60)
    print("âœ… ì‹¤ìŠµ ì™„ë£Œ!")
    print("="*60)


if __name__ == "__main__":
    main()

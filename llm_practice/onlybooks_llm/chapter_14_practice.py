"""
Chapter 14: ë©€í‹°ëª¨ë‹¬ LLM ì‹¤ìŠµ ì½”ë“œ
==================================

ì´ íŒŒì¼ì€ CLIP ë“± ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ì˜ ì›ë¦¬ë¥¼ ì‹¤ìŠµí•©ë‹ˆë‹¤:
1. ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ì„ë² ë”© ê°œë…
2. Contrastive Learning ì›ë¦¬
3. Zero-shot ë¶„ë¥˜ ì‹œë®¬ë ˆì´ì…˜
4. (ì„ íƒ) Hugging Face CLIP ì‚¬ìš©

ì‹¤í–‰ ë°©ë²•:
    pip install numpy pillow
    python chapter_14_practice.py

    # CLIP ì‚¬ìš© ì‹œ:
    pip install transformers torch pillow requests
"""

import numpy as np
from typing import List, Tuple, Dict
from dataclasses import dataclass


# ============================================================
# Part 1: Contrastive Learning ì›ë¦¬
# ============================================================

def contrastive_loss(image_embeddings: np.ndarray, 
                     text_embeddings: np.ndarray, 
                     temperature: float = 0.07) -> float:
    """
    Contrastive Loss ê³„ì‚° (InfoNCE Loss)
    
    Args:
        image_embeddings: (N, D) ì´ë¯¸ì§€ ì„ë² ë”©
        text_embeddings: (N, D) í…ìŠ¤íŠ¸ ì„ë² ë”© (ê°™ì€ ì¸ë±ìŠ¤ê°€ ë§¤ì¹­ ìŒ)
        temperature: ì˜¨ë„ íŒŒë¼ë¯¸í„°
    
    Returns:
        loss: í‰ê·  contrastive loss
    """
    # ì •ê·œí™”
    image_embeddings = image_embeddings / np.linalg.norm(image_embeddings, axis=1, keepdims=True)
    text_embeddings = text_embeddings / np.linalg.norm(text_embeddings, axis=1, keepdims=True)
    
    # ìœ ì‚¬ë„ í–‰ë ¬ ê³„ì‚°
    logits = np.dot(image_embeddings, text_embeddings.T) / temperature
    
    N = len(image_embeddings)
    labels = np.arange(N)  # ëŒ€ê°ì„ ì´ ì •ë‹µ
    
    # Image-to-Text Loss
    exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))  # ìˆ˜ì¹˜ ì•ˆì •ì„±
    i2t_loss = -np.log(exp_logits[np.arange(N), labels] / exp_logits.sum(axis=1))
    
    # Text-to-Image Loss
    exp_logits_t = np.exp(logits.T - np.max(logits.T, axis=1, keepdims=True))
    t2i_loss = -np.log(exp_logits_t[np.arange(N), labels] / exp_logits_t.sum(axis=1))
    
    # í‰ê· 
    loss = (i2t_loss.mean() + t2i_loss.mean()) / 2
    return loss


def demo_contrastive_learning():
    """Contrastive Learning ë°ëª¨"""
    print("\n" + "="*60)
    print("ğŸ“Š Contrastive Learning ë°ëª¨")
    print("="*60)
    
    np.random.seed(42)
    
    # ê°€ìƒì˜ ë§¤ì¹­ ìŒ (ìœ ì‚¬í•œ ì„ë² ë”©)
    N, D = 4, 64
    base_embeddings = np.random.randn(N, D)
    
    # ë§¤ì¹­ ìŒ: ì•½ê°„ì˜ ë…¸ì´ì¦ˆë§Œ ì¶”ê°€
    image_embeddings = base_embeddings + np.random.randn(N, D) * 0.1
    text_embeddings = base_embeddings + np.random.randn(N, D) * 0.1
    
    # ìœ ì‚¬ë„ í–‰ë ¬
    image_norm = image_embeddings / np.linalg.norm(image_embeddings, axis=1, keepdims=True)
    text_norm = text_embeddings / np.linalg.norm(text_embeddings, axis=1, keepdims=True)
    similarity = np.dot(image_norm, text_norm.T)
    
    print("\nìœ ì‚¬ë„ í–‰ë ¬ (ëŒ€ê°ì„ ì´ ë§¤ì¹­ ìŒ):")
    print("         Text0   Text1   Text2   Text3")
    for i in range(N):
        row = " ".join([f"{similarity[i,j]:7.3f}" for j in range(N)])
        print(f"  Image{i}: {row}")
    
    # Loss ê³„ì‚°
    loss = contrastive_loss(image_embeddings, text_embeddings)
    print(f"\nContrastive Loss: {loss:.4f}")
    
    # ë¹„ë§¤ì¹­ ìŒìœ¼ë¡œ ë¹„êµ
    random_text = np.random.randn(N, D)
    loss_random = contrastive_loss(image_embeddings, random_text)
    print(f"ëœë¤ ìŒ Loss: {loss_random:.4f}")
    print("â†’ ë§¤ì¹­ ìŒì˜ Lossê°€ ë” ë‚®ìŒ (í•™ìŠµ ëª©í‘œ)")


# ============================================================
# Part 2: ê°€ìƒ CLIP ëª¨ë¸
# ============================================================

class SimpleCLIP:
    """
    ê°„ë‹¨í•œ CLIP ì‹œë®¬ë ˆì´í„°
    
    ì‹¤ì œ CLIPì€ ViT + Transformerë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ,
    ì—¬ê¸°ì„œëŠ” í•´ì‹œ ê¸°ë°˜ ì„ë² ë”©ìœ¼ë¡œ ê°œë…ì„ ì„¤ëª…í•©ë‹ˆë‹¤.
    """
    
    def __init__(self, embedding_dim: int = 64):
        self.embedding_dim = embedding_dim
        np.random.seed(42)
    
    def encode_text(self, texts: List[str]) -> np.ndarray:
        """í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜"""
        embeddings = []
        for text in texts:
            # í•´ì‹œ ê¸°ë°˜ ì„ë² ë”© (ë°ëª¨ìš©)
            np.random.seed(hash(text.lower()) % 2**31)
            emb = np.random.randn(self.embedding_dim)
            embeddings.append(emb / np.linalg.norm(emb))
        return np.array(embeddings)
    
    def encode_image(self, image_descriptions: List[str]) -> np.ndarray:
        """
        ì´ë¯¸ì§€ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
        
        ì‹¤ì œë¡œëŠ” ì´ë¯¸ì§€ í”½ì…€ì„ ì²˜ë¦¬í•˜ì§€ë§Œ,
        ì—¬ê¸°ì„œëŠ” ì´ë¯¸ì§€ ì„¤ëª…ì„ ì‚¬ìš©í•´ ì‹œë®¬ë ˆì´ì…˜í•©ë‹ˆë‹¤.
        """
        # ì´ë¯¸ì§€ ì„¤ëª…ê³¼ ìœ ì‚¬í•œ ì„ë² ë”© ìƒì„±
        embeddings = []
        for desc in image_descriptions:
            np.random.seed(hash(desc.lower()) % 2**31)
            emb = np.random.randn(self.embedding_dim)
            # ì•½ê°„ì˜ ë…¸ì´ì¦ˆ ì¶”ê°€ (ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ê°„ ì°¨ì´ ì‹œë®¬ë ˆì´ì…˜)
            noise = np.random.randn(self.embedding_dim) * 0.2
            emb = emb + noise
            embeddings.append(emb / np.linalg.norm(emb))
        return np.array(embeddings)
    
    def compute_similarity(self, image_emb: np.ndarray, text_emb: np.ndarray) -> np.ndarray:
        """ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        return np.dot(image_emb, text_emb.T)


# ============================================================
# Part 3: Zero-shot Classification
# ============================================================

def zero_shot_classification(clip_model: SimpleCLIP,
                             image_description: str,
                             class_names: List[str],
                             template: str = "a photo of a {}") -> Dict[str, float]:
    """
    Zero-shot ì´ë¯¸ì§€ ë¶„ë¥˜
    
    Args:
        clip_model: CLIP ëª¨ë¸
        image_description: ì´ë¯¸ì§€ ì„¤ëª… (ì‹œë®¬ë ˆì´ì…˜ìš©)
        class_names: í´ë˜ìŠ¤ ì´ë¦„ ë¦¬ìŠ¤íŠ¸
        template: í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
    
    Returns:
        ê° í´ë˜ìŠ¤ì˜ í™•ë¥ 
    """
    # í´ë˜ìŠ¤ í”„ë¡¬í”„íŠ¸ ìƒì„±
    class_prompts = [template.format(c) for c in class_names]
    
    # ì„ë² ë”©
    image_emb = clip_model.encode_image([image_description])
    text_emb = clip_model.encode_text(class_prompts)
    
    # ìœ ì‚¬ë„ â†’ í™•ë¥ 
    similarities = clip_model.compute_similarity(image_emb, text_emb)[0]
    
    # Softmax
    exp_sim = np.exp(similarities * 10)  # temperature=0.1
    probs = exp_sim / exp_sim.sum()
    
    return {c: p for c, p in zip(class_names, probs)}


def demo_zero_shot():
    """Zero-shot Classification ë°ëª¨"""
    print("\n" + "="*60)
    print("ğŸ¯ Zero-shot Classification ë°ëª¨")
    print("="*60)
    
    clip = SimpleCLIP()
    
    # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ë“¤ (ì„¤ëª…ìœ¼ë¡œ ì‹œë®¬ë ˆì´ì…˜)
    test_images = [
        "a cute cat sitting on a couch",
        "a golden retriever playing in the park",
        "a colorful bird on a tree branch"
    ]
    
    class_names = ["cat", "dog", "bird", "fish", "rabbit"]
    
    for image_desc in test_images:
        print(f"\nğŸ–¼ï¸ ì´ë¯¸ì§€: '{image_desc}'")
        
        probs = zero_shot_classification(clip, image_desc, class_names)
        
        # ì •ë ¬í•˜ì—¬ ì¶œë ¥
        sorted_probs = sorted(probs.items(), key=lambda x: x[1], reverse=True)
        for class_name, prob in sorted_probs:
            bar = "â–ˆ" * int(prob * 20)
            print(f"  {class_name:10s}: {prob:5.1%} {bar}")
        
        predicted = sorted_probs[0][0]
        print(f"  â†’ ì˜ˆì¸¡: {predicted}")


# ============================================================
# Part 4: ì´ë¯¸ì§€ ê²€ìƒ‰
# ============================================================

def image_search(clip_model: SimpleCLIP,
                 query_text: str,
                 image_database: List[str],
                 top_k: int = 3) -> List[Tuple[str, float]]:
    """
    í…ìŠ¤íŠ¸ ì¿¼ë¦¬ë¡œ ì´ë¯¸ì§€ ê²€ìƒ‰
    
    Args:
        clip_model: CLIP ëª¨ë¸
        query_text: ê²€ìƒ‰ ì¿¼ë¦¬
        image_database: ì´ë¯¸ì§€ ì„¤ëª… ë¦¬ìŠ¤íŠ¸ (ì‹œë®¬ë ˆì´ì…˜ìš©)
        top_k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
    
    Returns:
        (ì´ë¯¸ì§€ ì„¤ëª…, ìœ ì‚¬ë„) ë¦¬ìŠ¤íŠ¸
    """
    # ì„ë² ë”©
    query_emb = clip_model.encode_text([query_text])
    image_embs = clip_model.encode_image(image_database)
    
    # ìœ ì‚¬ë„
    similarities = clip_model.compute_similarity(query_emb, image_embs)[0]
    
    # ìƒìœ„ kê°œ
    top_indices = np.argsort(similarities)[-top_k:][::-1]
    
    return [(image_database[i], similarities[i]) for i in top_indices]


def demo_image_search():
    """ì´ë¯¸ì§€ ê²€ìƒ‰ ë°ëª¨"""
    print("\n" + "="*60)
    print("ğŸ” ì´ë¯¸ì§€ ê²€ìƒ‰ ë°ëª¨")
    print("="*60)
    
    clip = SimpleCLIP()
    
    # ì´ë¯¸ì§€ ë°ì´í„°ë² ì´ìŠ¤ (ì„¤ëª…ìœ¼ë¡œ ì‹œë®¬ë ˆì´ì…˜)
    image_database = [
        "a sunset over the ocean with orange sky",
        "a busy city street with tall buildings",
        "a peaceful mountain landscape with snow",
        "a cute puppy playing with a ball",
        "a delicious pizza with cheese and tomatoes",
        "a beautiful flower garden in spring",
        "a starry night sky with milky way"
    ]
    
    queries = [
        "nature scenery",
        "urban environment",
        "food photography"
    ]
    
    for query in queries:
        print(f"\nğŸ” ì¿¼ë¦¬: '{query}'")
        results = image_search(clip, query, image_database, top_k=3)
        
        for rank, (image, sim) in enumerate(results, 1):
            print(f"  {rank}. [{sim:.3f}] {image}")


# ============================================================
# Part 5: Hugging Face CLIP ì‚¬ìš© (ì„ íƒì )
# ============================================================

def demo_huggingface_clip():
    """Hugging Face CLIP ì‚¬ìš© ë°ëª¨"""
    try:
        from transformers import CLIPProcessor, CLIPModel
        from PIL import Image
        import requests
        import torch
        
        print("\n" + "="*60)
        print("ğŸš€ Hugging Face CLIP ë°ëª¨")
        print("="*60)
        
        # ëª¨ë¸ ë¡œë“œ
        model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
        processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
        
        # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ë¡œë“œ
        url = "http://images.cocodataset.org/val2017/000000039769.jpg"
        try:
            image = Image.open(requests.get(url, stream=True, timeout=5).raw)
            
            # í´ë˜ìŠ¤ í›„ë³´
            class_names = ["cat", "dog", "bird", "car", "person"]
            texts = [f"a photo of a {c}" for c in class_names]
            
            # ì¶”ë¡ 
            inputs = processor(text=texts, images=image, return_tensors="pt", padding=True)
            
            with torch.no_grad():
                outputs = model(**inputs)
            
            probs = outputs.logits_per_image.softmax(dim=1)[0]
            
            print("\nì˜ˆì¸¡ ê²°ê³¼:")
            for name, prob in zip(class_names, probs):
                bar = "â–ˆ" * int(prob.item() * 20)
                print(f"  {name:10s}: {prob.item():5.1%} {bar}")
        
        except requests.exceptions.RequestException:
            print("âš ï¸ ì´ë¯¸ì§€ë¥¼ ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜)")
            
    except ImportError:
        print("\nâš ï¸ transformers ë˜ëŠ” torchê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("ì„¤ì¹˜: pip install transformers torch pillow requests")


# ============================================================
# ë©”ì¸ í•¨ìˆ˜
# ============================================================

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("="*60)
    print("ğŸ¤– Chapter 14: ë©€í‹°ëª¨ë‹¬ LLM ì‹¤ìŠµ")
    print("="*60)
    
    demo_contrastive_learning()
    demo_zero_shot()
    demo_image_search()
    demo_huggingface_clip()
    
    print("\n" + "="*60)
    print("âœ… ì‹¤ìŠµ ì™„ë£Œ!")
    print("="*60)


if __name__ == "__main__":
    main()

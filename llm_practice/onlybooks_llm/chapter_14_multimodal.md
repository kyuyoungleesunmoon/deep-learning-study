# ğŸ“– Chapter 14: ë©€í‹°ëª¨ë‹¬ LLM (Multimodal LLM)

## ğŸ“‹ ê°œìš”

ì´ ì±•í„°ì—ì„œëŠ” í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ë¥¼ í•¨ê»˜ ì²˜ë¦¬í•˜ëŠ” ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤.
- CLIP (Contrastive Language-Image Pre-training)
- ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ì„ë² ë”©
- Zero-shot ì´ë¯¸ì§€ ë¶„ë¥˜

## ğŸ”¬ í•µì‹¬ ì•Œê³ ë¦¬ì¦˜

### 1. CLIP (Contrastive Language-Image Pre-training)

**ì›ë¦¬**: ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¥¼ ê°™ì€ ì„ë² ë”© ê³µê°„ì— ë§¤í•‘

```
ì´ë¯¸ì§€ â†’ Image Encoder â†’ ì´ë¯¸ì§€ ì„ë² ë”© (512ì°¨ì›)
í…ìŠ¤íŠ¸ â†’ Text Encoder â†’ í…ìŠ¤íŠ¸ ì„ë² ë”© (512ì°¨ì›)

ìœ ì‚¬ë„ = cosine(ì´ë¯¸ì§€ ì„ë² ë”©, í…ìŠ¤íŠ¸ ì„ë² ë”©)
```

**í•™ìŠµ ë°©ë²• (Contrastive Learning)**:
```
ë°°ì¹˜ ë‚´ Nê°œì˜ (ì´ë¯¸ì§€, í…ìŠ¤íŠ¸) ìŒ:
- ëŒ€ê°ì„  (ë§¤ì¹­ ìŒ): ìœ ì‚¬ë„ ìµœëŒ€í™”
- ë¹„ëŒ€ê°ì„  (ë¹„ë§¤ì¹­ ìŒ): ìœ ì‚¬ë„ ìµœì†Œí™”

Loss = -log(exp(sim(I_i, T_i)/Ï„) / Î£ exp(sim(I_i, T_j)/Ï„))
```

**íŠ¹ì§•**:
- 4ì–µ ê°œì˜ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ìŒìœ¼ë¡œ í•™ìŠµ
- Zero-shot ë¶„ë¥˜ ê°€ëŠ¥ (í•™ìŠµ ì—†ì´ ìƒˆë¡œìš´ í´ë˜ìŠ¤ ë¶„ë¥˜)
- ë‹¤ì–‘í•œ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ íƒœìŠ¤í¬ì— í™œìš©

### 2. Image Encoder

**Vision Transformer (ViT) ë°©ì‹**:
```
ì´ë¯¸ì§€ (224Ã—224) â†’ íŒ¨ì¹˜ ë¶„í•  (16Ã—16 íŒ¨ì¹˜ = 196ê°œ)
â†’ Linear Projection â†’ Patch Embeddings
â†’ Position Embeddings ì¶”ê°€
â†’ Transformer Encoder â†’ [CLS] í† í° â†’ ì´ë¯¸ì§€ ì„ë² ë”©
```

**ResNet ë°©ì‹**:
```
ì´ë¯¸ì§€ â†’ CNN ë ˆì´ì–´ë“¤ â†’ Global Average Pooling â†’ ì´ë¯¸ì§€ ì„ë² ë”©
```

### 3. Zero-shot Classification

**ì›ë¦¬**: í´ë˜ìŠ¤ ì´ë¦„ì„ í…ìŠ¤íŠ¸ë¡œ ì¸ì½”ë”©í•˜ì—¬ ì´ë¯¸ì§€ì™€ ë¹„êµ

```python
# í´ë˜ìŠ¤ í”„ë¡¬í”„íŠ¸
prompts = ["a photo of a cat", "a photo of a dog", "a photo of a bird"]

# í…ìŠ¤íŠ¸ ì„ë² ë”©
text_embeddings = text_encoder(prompts)

# ì´ë¯¸ì§€ ì„ë² ë”©
image_embedding = image_encoder(image)

# ìœ ì‚¬ë„ ê³„ì‚°
similarities = cosine_similarity(image_embedding, text_embeddings)

# ê°€ì¥ ìœ ì‚¬í•œ í´ë˜ìŠ¤ ì„ íƒ
predicted_class = argmax(similarities)
```

## ğŸ“Š ì‹¤ìŠµ ì˜ˆì œ

### ì˜ˆì œ 1: Hugging Face CLIP ì‚¬ìš©

```python
from transformers import CLIPProcessor, CLIPModel
from PIL import Image
import requests
import torch

# ëª¨ë¸ ë¡œë“œ
model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

# ì´ë¯¸ì§€ ë¡œë“œ
url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)

# í…ìŠ¤íŠ¸ í›„ë³´
texts = ["a photo of a cat", "a photo of a dog", "a photo of a bird"]

# ì „ì²˜ë¦¬
inputs = processor(
    text=texts, 
    images=image, 
    return_tensors="pt", 
    padding=True
)

# ì¶”ë¡ 
outputs = model(**inputs)
logits_per_image = outputs.logits_per_image  # (1, 3)
probs = logits_per_image.softmax(dim=1)

print("ì˜ˆì¸¡ í™•ë¥ :")
for text, prob in zip(texts, probs[0]):
    print(f"  {text}: {prob.item():.2%}")
```

### ì˜ˆì œ 2: ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚°

```python
from transformers import CLIPProcessor, CLIPModel
import torch

model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

def get_text_embedding(text):
    """í…ìŠ¤íŠ¸ ì„ë² ë”© ì¶”ì¶œ"""
    inputs = processor(text=[text], return_tensors="pt", padding=True)
    with torch.no_grad():
        text_features = model.get_text_features(**inputs)
    return text_features / text_features.norm(dim=-1, keepdim=True)

def get_image_embedding(image):
    """ì´ë¯¸ì§€ ì„ë² ë”© ì¶”ì¶œ"""
    inputs = processor(images=image, return_tensors="pt")
    with torch.no_grad():
        image_features = model.get_image_features(**inputs)
    return image_features / image_features.norm(dim=-1, keepdim=True)

# ìœ ì‚¬ë„ ê³„ì‚°
text_emb = get_text_embedding("a happy golden retriever")
image_emb = get_image_embedding(image)

similarity = torch.matmul(text_emb, image_emb.T)
print(f"ìœ ì‚¬ë„: {similarity.item():.4f}")
```

### ì˜ˆì œ 3: ì´ë¯¸ì§€ ê²€ìƒ‰

```python
import numpy as np
from PIL import Image

def image_search(query_text, image_embeddings, images, top_k=3):
    """í…ìŠ¤íŠ¸ ì¿¼ë¦¬ë¡œ ì´ë¯¸ì§€ ê²€ìƒ‰"""
    query_emb = get_text_embedding(query_text).numpy()
    
    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„
    similarities = np.dot(image_embeddings, query_emb.T).flatten()
    
    # ìƒìœ„ kê°œ
    top_indices = np.argsort(similarities)[-top_k:][::-1]
    
    results = []
    for idx in top_indices:
        results.append({
            'image': images[idx],
            'similarity': similarities[idx]
        })
    
    return results

# ì‚¬ìš© ì˜ˆì‹œ
query = "a sunset over the ocean"
results = image_search(query, all_image_embeddings, all_images)
```

### ì˜ˆì œ 4: í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§

```python
# ë‹¤ì–‘í•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
templates = [
    "a photo of a {}",
    "a picture of a {}",
    "a {} in the wild",
    "a {} in nature",
    "an image of a {}"
]

def ensemble_classification(image, class_names):
    """ì—¬ëŸ¬ í”„ë¡¬í”„íŠ¸ì˜ ì•™ìƒë¸”ë¡œ ë¶„ë¥˜"""
    all_scores = []
    
    for template in templates:
        texts = [template.format(c) for c in class_names]
        inputs = processor(text=texts, images=image, return_tensors="pt", padding=True)
        outputs = model(**inputs)
        scores = outputs.logits_per_image.softmax(dim=1)
        all_scores.append(scores)
    
    # ì•™ìƒë¸” (í‰ê· )
    ensemble_scores = torch.stack(all_scores).mean(dim=0)
    return ensemble_scores

# ì‚¬ìš© ì˜ˆì‹œ
class_names = ["cat", "dog", "bird", "fish"]
scores = ensemble_classification(image, class_names)
predicted = class_names[scores.argmax()]
```

## ğŸ¯ í•µì‹¬ í¬ì¸íŠ¸

1. **ë™ì¼ ì„ë² ë”© ê³µê°„**: ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ê°€ ê°™ì€ ì°¨ì›ì˜ ë²¡í„°ë¡œ í‘œí˜„ë¨
2. **Zero-shot ëŠ¥ë ¥**: í•™ìŠµí•˜ì§€ ì•Šì€ ìƒˆë¡œìš´ í´ë˜ìŠ¤ë„ ë¶„ë¥˜ ê°€ëŠ¥
3. **í”„ë¡¬í”„íŠ¸ ì¤‘ìš”**: "a photo of a {class}" í˜•ì‹ì´ íš¨ê³¼ì 
4. **ì•™ìƒë¸” íš¨ê³¼**: ì—¬ëŸ¬ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì‚¬ìš© ì‹œ ì •í™•ë„ í–¥ìƒ

## ğŸ“š ì°¸ê³  ìë£Œ

- ì›ë³¸ ì½”ë“œ: https://github.com/onlybooks/llm/tree/main/14ì¥
- CLIP ë…¼ë¬¸: https://arxiv.org/abs/2103.00020
- Hugging Face CLIP: https://huggingface.co/openai/clip-vit-base-patch32

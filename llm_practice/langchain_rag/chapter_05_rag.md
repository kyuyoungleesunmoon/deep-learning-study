# ğŸ“– Chapter 05: RAG ê¸°ì´ˆ (Retrieval-Augmented Generation)

## ğŸ“‹ ê°œìš”

ì´ ì±•í„°ì—ì„œëŠ” RAG ì‹œìŠ¤í…œì˜ í•µì‹¬ êµ¬ì„±ìš”ì†Œë¥¼ í•™ìŠµí•©ë‹ˆë‹¤.
- ë²¡í„° ì €ì¥ì†Œ (Vector Store)
- Retriever
- LCEL (LangChain Expression Language)

## ğŸ”¬ í•µì‹¬ ê°œë…

### 1. RAG ì•„í‚¤í…ì²˜

```
ì§ˆë¬¸ â†’ Retriever â†’ ê´€ë ¨ ë¬¸ì„œ â†’ LLM â†’ ë‹µë³€
         â†‘
    Vector Store
    (ì„ë² ë”© + ì¸ë±ìŠ¤)
```

**í•µì‹¬ ë‹¨ê³„**:
1. **Indexing**: ë¬¸ì„œ â†’ ì²­í¬ â†’ ì„ë² ë”© â†’ ë²¡í„° ì €ì¥ì†Œ
2. **Retrieval**: ì§ˆë¬¸ â†’ ì„ë² ë”© â†’ ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰
3. **Generation**: ì§ˆë¬¸ + ë¬¸ì„œ â†’ LLM â†’ ë‹µë³€

### 2. Vector Store

**ì£¼ìš” ë²¡í„° ì €ì¥ì†Œ**:
| ì €ì¥ì†Œ | íŠ¹ì§• | ìš©ë„ |
|--------|------|------|
| Chroma | ê²½ëŸ‰, ë¡œì»¬ | ê°œë°œ/í…ŒìŠ¤íŠ¸ |
| FAISS | ë¹ ë¥¸ ê²€ìƒ‰ | ëŒ€ê·œëª¨ ë°ì´í„° |
| Pinecone | í´ë¼ìš°ë“œ | í”„ë¡œë•ì…˜ |
| Milvus | ë¶„ì‚° ì²˜ë¦¬ | ì—”í„°í”„ë¼ì´ì¦ˆ |

### 3. Embedding ëª¨ë¸

**OpenAI Embeddings**:
```python
from langchain_openai import OpenAIEmbeddings
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
```

**Hugging Face Embeddings**:
```python
from langchain_huggingface import HuggingFaceEmbeddings
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
```

### 4. LCEL (LangChain Expression Language)

**íŠ¹ì§•**: `|` ì—°ì‚°ìë¡œ ì²´ì¸ êµ¬ì„±

```python
chain = prompt | model | parser
result = chain.invoke({"question": "..."})
```

**ë³‘ë ¬ ì²˜ë¦¬**:
```python
from langchain_core.runnables import RunnableParallel

chain = RunnableParallel(
    context=retriever,
    question=RunnablePassthrough()
) | prompt | llm
```

## ğŸ“Š ì‹¤ìŠµ ì˜ˆì œ

### ì˜ˆì œ 1: ê¸°ë³¸ RAG íŒŒì´í”„ë¼ì¸

```python
from langchain.document_loaders import PyPDFLoader
from langchain.vectorstores import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain import hub

# 1. ë¬¸ì„œ ë¡œë“œ ë° ë¶„í• 
loader = PyPDFLoader("document.pdf")
pages = loader.load_and_split()

splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
docs = splitter.split_documents(pages)

# 2. ë²¡í„° ì €ì¥ì†Œ ìƒì„±
vectorstore = Chroma.from_documents(
    docs, 
    OpenAIEmbeddings(model='text-embedding-3-small')
)
retriever = vectorstore.as_retriever()

# 3. LLM ë° í”„ë¡¬í”„íŠ¸
llm = ChatOpenAI(model="gpt-4o-mini")
prompt = hub.pull("rlm/rag-prompt")

# 4. ì²´ì¸ êµ¬ì„±
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

rag_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

# 5. ì§ˆë¬¸
answer = rag_chain.invoke("ë¬¸ì„œì˜ ì£¼ìš” ë‚´ìš©ì€?")
print(answer)
```

### ì˜ˆì œ 2: ëŒ€í™”í˜• RAG (Memory)

```python
from langchain.chains import create_history_aware_retriever
from langchain_core.prompts import MessagesPlaceholder
from langchain_community.chat_message_histories import ChatMessageHistory

# íˆìŠ¤í† ë¦¬ ì¸ì‹ retriever
contextualize_q_prompt = ChatPromptTemplate.from_messages([
    ("system", "ëŒ€í™” ê¸°ë¡ì„ ê³ ë ¤í•˜ì—¬ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ì¬ì‘ì„±í•˜ì„¸ìš”."),
    MessagesPlaceholder("chat_history"),
    ("human", "{input}")
])

history_aware_retriever = create_history_aware_retriever(
    llm, retriever, contextualize_q_prompt
)

# ë©”ëª¨ë¦¬
store = {}

def get_session_history(session_id):
    if session_id not in store:
        store[session_id] = ChatMessageHistory()
    return store[session_id]

# ëŒ€í™”í˜• ì²´ì¸
conversational_chain = RunnableWithMessageHistory(
    rag_chain,
    get_session_history,
    input_messages_key="input",
    history_messages_key="chat_history"
)
```

### ì˜ˆì œ 3: ë‹¤ì–‘í•œ Retriever ì„¤ì •

```python
# MMR (Maximal Marginal Relevance)
retriever_mmr = vectorstore.as_retriever(
    search_type="mmr",
    search_kwargs={"k": 5, "fetch_k": 20}
)

# ìœ ì‚¬ë„ ì ìˆ˜ ì„ê³„ê°’
retriever_threshold = vectorstore.as_retriever(
    search_type="similarity_score_threshold",
    search_kwargs={"score_threshold": 0.8}
)

# ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ ì¡°ì ˆ
retriever_k = vectorstore.as_retriever(
    search_kwargs={"k": 10}
)
```

### ì˜ˆì œ 4: ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸

```python
from langchain_core.prompts import ChatPromptTemplate

custom_prompt = ChatPromptTemplate.from_template("""
ë‹¹ì‹ ì€ ì „ë¬¸ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ë§Œì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.
ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ì •ë³´ëŠ” "ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.

ì»¨í…ìŠ¤íŠ¸:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€:""")

rag_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | custom_prompt
    | llm
    | StrOutputParser()
)
```

### ì˜ˆì œ 5: ë¡œì»¬ ì„ë² ë”© ëª¨ë¸

```python
from langchain_huggingface import HuggingFaceEmbeddings

# í•œêµ­ì–´ ëª¨ë¸
embeddings = HuggingFaceEmbeddings(
    model_name="snunlp/KR-SBERT-V40K-klueNLI-augSTS",
    model_kwargs={"device": "cpu"},
    encode_kwargs={"normalize_embeddings": True}
)

# ë²¡í„° ì €ì¥ì†Œ ìƒì„±
vectorstore = Chroma.from_documents(docs, embeddings)
```

## ğŸ¯ í•µì‹¬ í¬ì¸íŠ¸

1. **ì²­í¬ í¬ê¸° ì¡°ì ˆ**: ë„ˆë¬´ ì‘ìœ¼ë©´ ë¬¸ë§¥ ì†ì‹¤, ë„ˆë¬´ í¬ë©´ ë…¸ì´ì¦ˆ ì¦ê°€
2. **MMR í™œìš©**: ë‹¤ì–‘í•œ ì •ë³´ ê²€ìƒ‰ ì‹œ ìœ ìš©
3. **í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§**: ì»¨í…ìŠ¤íŠ¸ í™œìš© ë°©ë²• ëª…ì‹œ
4. **í•œêµ­ì–´ ì„ë² ë”©**: ì „ìš© ëª¨ë¸ ì‚¬ìš© ê¶Œì¥

## âš ï¸ ì£¼ì˜ì‚¬í•­

- í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€: "ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  ë‹µí•´" ì§€ì‹œ
- ë¹„ìš© ê´€ë¦¬: ì„ë² ë”© API í˜¸ì¶œ íšŸìˆ˜ í™•ì¸
- ì²­í¬ ì˜¤ë²„ë©: ë¬¸ë§¥ ì—°ê²°ì„ ìœ„í•´ í•„ìˆ˜

## ğŸ“š ì°¸ê³  ìë£Œ

- ì›ë³¸ ì½”ë“œ: https://github.com/Kane0002/Langchain-RAG/tree/main/5ì¥
- LangChain RAG: https://python.langchain.com/docs/use_cases/question_answering/
- ChromaDB: https://www.trychroma.com/

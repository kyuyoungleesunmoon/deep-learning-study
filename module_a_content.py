"""
Module A Content: RNN Time Series Prediction
Complete content for comprehensive AI textbook
"""

def get_all_content(md, code):
    """Returns all cells for Module A"""
    
    cells = []
    
    # Title
    cells.append(md("""# ğŸ”¹ ëª¨ë“ˆ A: RNN ê¸°ë°˜ ì‹œê³„ì—´ ì˜ˆì¸¡ (Netflix ì£¼ê°€ ì˜ˆì¸¡)

## ğŸ“š í•™ìŠµ ëª©í‘œ
ì´ ëª¨ë“ˆì—ì„œëŠ” ìˆœí™˜ ì‹ ê²½ë§(RNN)ê³¼ ê·¸ ë³€í˜•ì¸ LSTM, GRUë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí•©ë‹ˆë‹¤.

**ì£¼ìš” í•™ìŠµ ë‚´ìš©:**
- RNN, LSTM, GRUì˜ ë‚´ë¶€ êµ¬ì¡°ì™€ ìˆ˜í•™ì  ì›ë¦¬
- ê·¸ë˜ë””ì–¸íŠ¸ ì†Œì‹¤/í­ì£¼ ë¬¸ì œì™€ í•´ê²° ë°©ë²•
- í•©ì„± ë°ì´í„°ì™€ ì‹¤ì œ ì£¼ê°€ ë°ì´í„°ë¥¼ ì´ìš©í•œ ì‹¤í—˜
- í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ê³¼ ì„±ëŠ¥ í‰ê°€

**ëŒ€ìƒ í•™ìŠµì:** ì„ì‚¬ ìˆ˜ì¤€ì˜ ë”¥ëŸ¬ë‹ í•™ìŠµì  
**ì˜ˆìƒ í•™ìŠµ ì‹œê°„:** 4-6ì‹œê°„"""))
    
    # Add all sections
    cells.extend(get_theory_cells(md, code))
    cells.extend(get_synthetic_cells(md, code))
    cells.extend(get_real_data_cells(md, code))
    cells.extend(get_conclusion_cells(md, code))
    
    return cells

def get_theory_cells(md, code):
    """Theory section cells - COMPLETE COMPREHENSIVE VERSION"""
    return [
        md("""---
# 1ï¸âƒ£ ì´ë¡  íŒŒíŠ¸

## 1.1 ìˆœí™˜ ì‹ ê²½ë§(RNN)ì˜ ê¸°ë³¸ êµ¬ì¡°

### ì™œ RNNì´ í•„ìš”í•œê°€?

ì¼ë°˜ì ì¸ í”¼ë“œí¬ì›Œë“œ ì‹ ê²½ë§ì€ ê° ì…ë ¥ì„ ë…ë¦½ì ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ì‹œê³„ì—´ ë°ì´í„°ë‚˜ ìì—°ì–´ì™€ ê°™ì´ **ìˆœì„œê°€ ì¤‘ìš”í•œ ë°ì´í„°**ì—ì„œëŠ” ì´ì „ ì •ë³´ë¥¼ ê¸°ì–µí•´ì•¼ í•©ë‹ˆë‹¤.

RNNì€ **ìˆœí™˜ êµ¬ì¡°**ë¥¼ í†µí•´ ì´ì „ ì‹œì ì˜ ì •ë³´ë¥¼ í˜„ì¬ ì‹œì ìœ¼ë¡œ ì „ë‹¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### RNNì˜ ìˆ˜í•™ì  ì •ì˜

RNNì˜ ê¸°ë³¸ ìˆœì „íŒŒ ìˆ˜ì‹ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

$$h_t = \\tanh(W_h h_{t-1} + W_x x_t + b_h)$$

$$y_t = W_y h_t + b_y$$

**ê¸°í˜¸ ì„¤ëª…:**
- $x_t$: ì‹œì  $t$ì—ì„œì˜ ì…ë ¥ ë²¡í„° (í¬ê¸°: $d_{input}$)
- $h_t$: ì‹œì  $t$ì—ì„œì˜ ì€ë‹‰ ìƒíƒœ (í¬ê¸°: $d_{hidden}$)
- $h_{t-1}$: ì´ì „ ì‹œì ì˜ ì€ë‹‰ ìƒíƒœ
- $y_t$: ì‹œì  $t$ì—ì„œì˜ ì¶œë ¥ (í¬ê¸°: $d_{output}$)
- $W_h$: ì€ë‹‰ ìƒíƒœ ê°€ì¤‘ì¹˜ í–‰ë ¬ (í¬ê¸°: $d_{hidden} \\times d_{hidden}$)
- $W_x$: ì…ë ¥ ê°€ì¤‘ì¹˜ í–‰ë ¬ (í¬ê¸°: $d_{hidden} \\times d_{input}$)
- $W_y$: ì¶œë ¥ ê°€ì¤‘ì¹˜ í–‰ë ¬ (í¬ê¸°: $d_{output} \\times d_{hidden}$)
- $b_h, b_y$: í¸í–¥(bias) ë²¡í„°
- $\\tanh$: í•˜ì´í¼ë³¼ë¦­ íƒ„ì  íŠ¸ í™œì„±í™” í•¨ìˆ˜ (ì¶œë ¥ ë²”ìœ„: -1 ~ 1)

### RNNì˜ ë™ì‘ ì›ë¦¬

1. **ì´ˆê¸°í™”**: ì€ë‹‰ ìƒíƒœ $h_0$ëŠ” ë³´í†µ ì˜ë²¡í„°ë¡œ ì´ˆê¸°í™”
2. **ìˆœì „íŒŒ** (ê° ì‹œì  $t = 1, 2, ..., T$):
   - í˜„ì¬ ì…ë ¥ $x_t$ì™€ ì´ì „ ì€ë‹‰ ìƒíƒœ $h_{t-1}$ì„ ê²°í•©
   - ì„ í˜• ë³€í™˜ í›„ í™œì„±í™” í•¨ìˆ˜ ì ìš© â†’ ìƒˆë¡œìš´ ì€ë‹‰ ìƒíƒœ $h_t$ ìƒì„±
   - í•„ìš”ì‹œ ì€ë‹‰ ìƒíƒœë¥¼ ì¶œë ¥ìœ¼ë¡œ ë³€í™˜ â†’ $y_t$ ìƒì„±
3. **ì •ë³´ ì „ë‹¬**: $h_t$ê°€ ë‹¤ìŒ ì‹œì ìœ¼ë¡œ ì „ë‹¬ë˜ì–´ ì‹œí€€ìŠ¤ ì •ë³´ ìœ ì§€

### ê°„ë‹¨í•œ ìˆ˜ì¹˜ ì˜ˆì œ

**ì„¤ì •:**
- ì…ë ¥ ì°¨ì›: 1, ì€ë‹‰ ì°¨ì›: 2
- $W_h = \\begin{bmatrix} 0.5 & 0.2 \\\\ 0.3 & 0.4 \\end{bmatrix}$, $W_x = \\begin{bmatrix} 0.6 \\\\ 0.7 \\end{bmatrix}$, $b_h = \\begin{bmatrix} 0.1 \\\\ 0.1 \\end{bmatrix}$

**ì‹œì  t=1:**
- $h_0 = [0, 0]^T$, $x_1 = 1.0$
- $z_1 = W_h h_0 + W_x x_1 + b_h = [0.7, 0.8]^T$
- $h_1 = \\tanh([0.7, 0.8]^T) â‰ˆ [0.604, 0.664]^T$

ì´ë ‡ê²Œ ê° ì‹œì ë§ˆë‹¤ ì€ë‹‰ ìƒíƒœê°€ ì—…ë°ì´íŠ¸ë˜ë©° ì´ì „ ì •ë³´ë¥¼ ëˆ„ì í•©ë‹ˆë‹¤."""),
        
        md("""## 1.2 ê·¸ë˜ë””ì–¸íŠ¸ ì†Œì‹¤/í­ì£¼ ë¬¸ì œ

### ë¬¸ì œì˜ ì›ì¸

RNNì„ ì—­ì „íŒŒ(BPTT: Backpropagation Through Time)ë¡œ í•™ìŠµí•  ë•Œ, ê·¸ë˜ë””ì–¸íŠ¸ëŠ” ì‹œê°„ ì¶•ì„ ë”°ë¼ ì—­ìœ¼ë¡œ ì „íŒŒë©ë‹ˆë‹¤:

$$\\frac{\\partial L}{\\partial h_{t-k}} = \\frac{\\partial L}{\\partial h_t} \\prod_{i=t-k+1}^{t} \\frac{\\partial h_i}{\\partial h_{i-1}}$$

ì—¬ê¸°ì„œ $\\frac{\\partial h_i}{\\partial h_{i-1}}$ëŠ” ì£¼ë¡œ $W_h$ì™€ í™œì„±í™” í•¨ìˆ˜ì˜ ë¯¸ë¶„ì„ í¬í•¨í•©ë‹ˆë‹¤.

**ë¬¸ì œ:**
- ë§Œì•½ ì´ ê°’ì´ **1ë³´ë‹¤ ì‘ìœ¼ë©´**: ì—¬ëŸ¬ ë²ˆ ê³±í•´ì§ˆìˆ˜ë¡ 0ì— ê°€ê¹Œì›Œì§ â†’ **ê·¸ë˜ë””ì–¸íŠ¸ ì†Œì‹¤**
- ë§Œì•½ ì´ ê°’ì´ **1ë³´ë‹¤ í¬ë©´**: ì—¬ëŸ¬ ë²ˆ ê³±í•´ì§ˆìˆ˜ë¡ ë¬´í•œëŒ€ë¡œ ë°œì‚° â†’ **ê·¸ë˜ë””ì–¸íŠ¸ í­ì£¼**

### ìˆ˜ì¹˜ì  ì˜ˆì‹œ

**ê·¸ë˜ë””ì–¸íŠ¸ ì†Œì‹¤:**
- $\\frac{\\partial h_i}{\\partial h_{i-1}} = 0.5$ (ê° ì‹œì ë§ˆë‹¤)
- 10 ì‹œì : $0.5^{10} â‰ˆ 0.00098$
- 20 ì‹œì : $0.5^{20} â‰ˆ 0.00000095$

**ê·¸ë˜ë””ì–¸íŠ¸ í­ì£¼:**
- $\\frac{\\partial h_i}{\\partial h_{i-1}} = 1.5$ (ê° ì‹œì ë§ˆë‹¤)
- 10 ì‹œì : $1.5^{10} â‰ˆ 57.7$
- 20 ì‹œì : $1.5^{20} â‰ˆ 3325$

### ê²°ê³¼ ë° í•´ê²° ë°©ë²•

**ì†Œì‹¤**: ì¥ê¸° ì˜ì¡´ì„± í•™ìŠµ ë¶ˆê°€ â†’ **LSTM/GRU ì‚¬ìš©**  
**í­ì£¼**: í•™ìŠµ ë¶ˆì•ˆì • â†’ **ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ ì‚¬ìš©**"""),
        
        md("""## 1.3 LSTM (Long Short-Term Memory)

LSTMì€ **ê²Œì´íŠ¸ ë©”ì»¤ë‹ˆì¦˜**ì„ í†µí•´ ê·¸ë˜ë””ì–¸íŠ¸ ì†Œì‹¤ ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤.

### LSTMì˜ êµ¬ì¡°

LSTMì€ ì„¸ ê°€ì§€ ê²Œì´íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤:

1. **Forget Gate** $f_t$: ì–´ë–¤ ì •ë³´ë¥¼ ë²„ë¦´ì§€ ê²°ì •
2. **Input Gate** $i_t$: ì–´ë–¤ ìƒˆë¡œìš´ ì •ë³´ë¥¼ ì €ì¥í• ì§€ ê²°ì •
3. **Output Gate** $o_t$: ì–´ë–¤ ì •ë³´ë¥¼ ì¶œë ¥í• ì§€ ê²°ì •

### LSTMì˜ ìˆ˜ì‹

$$f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f) \\quad \\text{(Forget Gate)}$$

$$i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i) \\quad \\text{(Input Gate)}$$

$$\\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C) \\quad \\text{(Candidate Cell State)}$$

$$C_t = f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t \\quad \\text{(Cell State Update)}$$

$$o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o) \\quad \\text{(Output Gate)}$$

$$h_t = o_t \\odot \\tanh(C_t) \\quad \\text{(Hidden State)}$$

**ê¸°í˜¸ ì„¤ëª…:**
- $\\sigma$: ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ (0~1 ì‚¬ì´ ê°’ ì¶œë ¥)
- $\\odot$: ì›ì†Œë³„ ê³±ì…ˆ
- $C_t$: ì…€ ìƒíƒœ - ì¥ê¸° ê¸°ì–µ ë‹´ë‹¹
- $[h_{t-1}, x_t]$: ë²¡í„° ì—°ê²°

### LSTMì´ ì†Œì‹¤ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ì›ë¦¬

1. **ì§ì ‘ì ì¸ ê²½ë¡œ**: $C_t = f_t \\odot C_{t-1} + ...$ì—ì„œ ë§ì…ˆ ì—°ì‚°ì€ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ì§ì ‘ ì „íŒŒ
2. **ê²Œì´íŠ¸ ì œì–´**: Forget gateê°€ 1ì— ê°€ê¹Œìš°ë©´ ê·¸ë˜ë””ì–¸íŠ¸ê°€ ê±°ì˜ ê·¸ëŒ€ë¡œ ì „íŒŒë¨
3. **ì„ íƒì  ê¸°ì–µ**: ì¤‘ìš”í•œ ì •ë³´ë§Œ ì„ íƒì ìœ¼ë¡œ ë³´ì¡´í•˜ê³  ì „íŒŒ"""),
        
        md("""## 1.4 GRU (Gated Recurrent Unit)

GRUëŠ” LSTMì„ ë‹¨ìˆœí™”í•œ ë³€í˜•ìœ¼ë¡œ, **2ê°œì˜ ê²Œì´íŠ¸**ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.

### GRUì˜ ìˆ˜ì‹

$$z_t = \\sigma(W_z \\cdot [h_{t-1}, x_t] + b_z) \\quad \\text{(Update Gate)}$$

$$r_t = \\sigma(W_r \\cdot [h_{t-1}, x_t] + b_r) \\quad \\text{(Reset Gate)}$$

$$\\tilde{h}_t = \\tanh(W_h \\cdot [r_t \\odot h_{t-1}, x_t] + b_h) \\quad \\text{(Candidate)}$$

$$h_t = (1 - z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t \\quad \\text{(Hidden State)}$$

**ê¸°í˜¸ ì„¤ëª…:**
- $z_t$: Update gate - ì´ì „ ìƒíƒœì™€ ìƒˆë¡œìš´ ìƒíƒœì˜ ë¹„ìœ¨ ê²°ì •
- $r_t$: Reset gate - ì´ì „ ìƒíƒœë¥¼ ì–¼ë§ˆë‚˜ ë¬´ì‹œí• ì§€ ê²°ì •

### LSTM vs GRU

| íŠ¹ì§• | LSTM | GRU |
|------|------|-----|
| ê²Œì´íŠ¸ ìˆ˜ | 3ê°œ | 2ê°œ |
| íŒŒë¼ë¯¸í„° ìˆ˜ | ë” ë§ìŒ | ë” ì ìŒ |
| ê³„ì‚° ë³µì¡ë„ | ë†’ìŒ | ë‚®ìŒ |
| ì„±ëŠ¥ | ë§¤ìš° ê¸´ ì‹œí€€ìŠ¤ì— ìœ ë¦¬ | ì§§ì€~ì¤‘ê°„ ì‹œí€€ìŠ¤ì— íš¨ìœ¨ì  |"""),
        
        md("""## 1.5 ì—­ì „íŒŒ (BPTT - Backpropagation Through Time)

### ê°œë…

ì‹œê³„ì—´ ë°ì´í„°ì—ì„œ ì—­ì „íŒŒëŠ” ì‹œê°„ì„ ê±°ìŠ¬ëŸ¬ ì˜¬ë¼ê°€ë©° ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.

### ê³¼ì •

1. ìˆœì „íŒŒë¡œ ëª¨ë“  ì‹œì ì˜ ì€ë‹‰ ìƒíƒœì™€ ì¶œë ¥ ê³„ì‚°
2. ë§ˆì§€ë§‰ ì‹œì ë¶€í„° ì‹œì‘í•˜ì—¬ ì—­ìœ¼ë¡œ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°
3. ì—°ì‡„ ë²•ì¹™(Chain Rule)ì„ ì ìš©í•˜ì—¬ ê° ê°€ì¤‘ì¹˜ì˜ ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì 
4. ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸

### Truncated BPTT

ë©”ëª¨ë¦¬ íš¨ìœ¨ì„ ìœ„í•´ ì „ì²´ ì‹œí€€ìŠ¤ê°€ ì•„ë‹Œ **ì¼ì • ê¸¸ì´ë§Œí¼ë§Œ** ì—­ì „íŒŒí•˜ëŠ” ê¸°ë²•ì…ë‹ˆë‹¤.

## 1.6 ì†ì‹¤ í•¨ìˆ˜ì™€ í‰ê°€ ì§€í‘œ

### ì†ì‹¤ í•¨ìˆ˜

**MSE (Mean Squared Error):**
$$L_{MSE} = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2$$

**MAE (Mean Absolute Error):**
$$L_{MAE} = \\frac{1}{n}\\sum_{i=1}^{n}|y_i - \\hat{y}_i|$$

### í‰ê°€ ì§€í‘œ

**RMSE (Root Mean Squared Error):**
$$RMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}$$

**MAPE (Mean Absolute Percentage Error):**
$$MAPE = \\frac{100\\%}{n}\\sum_{i=1}^{n}\\left|\\frac{y_i - \\hat{y}_i}{y_i}\\right|$$

**ê¸°í˜¸ ì„¤ëª…:**
- $y_i$: ì‹¤ì œ ê°’
- $\\hat{y}_i$: ì˜ˆì¸¡ ê°’
- $n$: ìƒ˜í”Œ ê°œìˆ˜"""),
    ]

def get_synthetic_cells(md, code):
    """Synthetic experiments cells - COMPLETE VERSION"""
    return [
        md("""---
# 2ï¸âƒ£ í•©ì„± ë°ì´í„° ì‹¤í—˜

## 2.1 í•©ì„± ì‹œê³„ì—´ ë°ì´í„° ìƒì„±

ë‹¨ìˆœí•œ ì‚¬ì¸íŒŒì— ë…¸ì´ì¦ˆë¥¼ ì¶”ê°€í•œ ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ìƒì„±í•˜ì—¬ RNN ê³„ì—´ ëª¨ë¸ì˜ ë™ì‘ì„ ì´í•´í•©ë‹ˆë‹¤."""),
        
        code("""# í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„í¬íŠ¸
import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error
import warnings
warnings.filterwarnings('ignore')

# ì„¤ì •
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False
np.random.seed(42)
torch.manual_seed(42)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f'ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}')"""),
        
        code("""# í•©ì„± ì‹œê³„ì—´ ë°ì´í„° ìƒì„± í•¨ìˆ˜
def generate_synthetic_timeseries(n_samples=1000, noise_level=0.1):
    t = np.linspace(0, 100, n_samples)
    signal = np.sin(0.1 * t) + 0.5 * np.sin(0.3 * t) + 0.3 * np.sin(0.5 * t)
    noise = np.random.normal(0, noise_level, n_samples)
    return signal + noise

# ë°ì´í„° ìƒì„±
synthetic_data = generate_synthetic_timeseries(n_samples=1000, noise_level=0.1)

# ì‹œê°í™”
plt.figure(figsize=(14, 4))
plt.plot(synthetic_data, linewidth=0.8)
plt.title('í•©ì„± ì‹œê³„ì—´ ë°ì´í„° (ì‚¬ì¸íŒŒ + ë…¸ì´ì¦ˆ)', fontsize=14, fontweight='bold')
plt.xlabel('Time Step')
plt.ylabel('Value')
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

print(f'ë°ì´í„° í˜•íƒœ: {synthetic_data.shape}')
print(f'ë°ì´í„° ë²”ìœ„: [{synthetic_data.min():.3f}, {synthetic_data.max():.3f}]')"""),
        
        md("""## 2.2 ë°ì´í„° ì „ì²˜ë¦¬

### ìœˆë„ìš° ìŠ¬ë¼ì´ë”© ê¸°ë²•

ì‹œê³„ì—´ ì˜ˆì¸¡ì„ ìœ„í•´ ê³¼ê±° ì¼ì • ê¸°ê°„(window)ì˜ ë°ì´í„°ë¥¼ ì…ë ¥ìœ¼ë¡œ, ë‹¤ìŒ ê°’ì„ íƒ€ê²Ÿìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤."""),
        
        code("""# ìœˆë„ìš° ìƒì„± í•¨ìˆ˜
def create_windows(data, window_size):
    X, y = [], []
    for i in range(len(data) - window_size):
        X.append(data[i:i+window_size])
        y.append(data[i+window_size])
    return np.array(X), np.array(y)

# ì •ê·œí™” ë° ìœˆë„ìš° ìƒì„±
scaler = MinMaxScaler()
synthetic_data_normalized = scaler.fit_transform(synthetic_data.reshape(-1, 1)).flatten()
window_size = 30
X, y = create_windows(synthetic_data_normalized, window_size)

# í•™ìŠµ/ê²€ì¦ ë¶„ë¦¬
split_idx = int(len(X) * 0.8)
X_train, X_val = X[:split_idx], X[split_idx:]
y_train, y_val = y[:split_idx], y[split_idx:]

print(f'X í˜•íƒœ: {X.shape}')
print(f'í•™ìŠµ ë°ì´í„°: {X_train.shape}')
print(f'ê²€ì¦ ë°ì´í„°: {X_val.shape}')"""),
        
        md("""## 2.3 RNN/LSTM/GRU ëª¨ë¸ êµ¬í˜„

PyTorchë¥¼ ì‚¬ìš©í•˜ì—¬ ì„¸ ê°€ì§€ ëª¨ë¸ì„ êµ¬í˜„í•©ë‹ˆë‹¤."""),
        
        code("""# RNN ëª¨ë¸
class SimpleRNN(nn.Module):
    def __init__(self, input_size=1, hidden_size=64, num_layers=2, dropout=0.2):
        super(SimpleRNN, self).__init__()
        self.rnn = nn.RNN(input_size, hidden_size, num_layers, 
                          batch_first=True, dropout=dropout if num_layers > 1 else 0)
        self.fc = nn.Linear(hidden_size, 1)
    
    def forward(self, x):
        out, _ = self.rnn(x)
        out = self.fc(out[:, -1, :])
        return out

# LSTM ëª¨ë¸
class SimpleLSTM(nn.Module):
    def __init__(self, input_size=1, hidden_size=64, num_layers=2, dropout=0.2):
        super(SimpleLSTM, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers,
                           batch_first=True, dropout=dropout if num_layers > 1 else 0)
        self.fc = nn.Linear(hidden_size, 1)
    
    def forward(self, x):
        out, _ = self.lstm(x)
        out = self.fc(out[:, -1, :])
        return out

# GRU ëª¨ë¸
class SimpleGRU(nn.Module):
    def __init__(self, input_size=1, hidden_size=64, num_layers=2, dropout=0.2):
        super(SimpleGRU, self).__init__()
        self.gru = nn.GRU(input_size, hidden_size, num_layers,
                         batch_first=True, dropout=dropout if num_layers > 1 else 0)
        self.fc = nn.Linear(hidden_size, 1)
    
    def forward(self, x):
        out, _ = self.gru(x)
        out = self.fc(out[:, -1, :])
        return out

print('ëª¨ë¸ ì •ì˜ ì™„ë£Œ: RNN, LSTM, GRU')"""),
        
        md("""## 2.4 í•™ìŠµ ë° í‰ê°€ í•¨ìˆ˜"""),
        
        code("""# Dataset í´ë˜ìŠ¤
class TimeSeriesDataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.FloatTensor(X).unsqueeze(-1)
        self.y = torch.FloatTensor(y).unsqueeze(-1)
    
    def __len__(self):
        return len(self.X)
    
    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

# DataLoader ìƒì„±
train_dataset = TimeSeriesDataset(X_train, y_train)
val_dataset = TimeSeriesDataset(X_val, y_val)
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)

# í•™ìŠµ í•¨ìˆ˜
def train_model(model, train_loader, val_loader, epochs=50, lr=0.001, clip_grad=True):
    model = model.to(device)
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=lr)
    
    train_losses, val_losses = [], []
    
    for epoch in range(epochs):
        model.train()
        train_loss = 0
        for X_batch, y_batch in train_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            optimizer.zero_grad()
            outputs = model(X_batch)
            loss = criterion(outputs, y_batch)
            loss.backward()
            if clip_grad:
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            train_loss += loss.item()
        
        train_loss /= len(train_loader)
        train_losses.append(train_loss)
        
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for X_batch, y_batch in val_loader:
                X_batch, y_batch = X_batch.to(device), y_batch.to(device)
                outputs = model(X_batch)
                loss = criterion(outputs, y_batch)
                val_loss += loss.item()
        
        val_loss /= len(val_loader)
        val_losses.append(val_loss)
        
        if (epoch + 1) % 10 == 0:
            print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}')
    
    return train_losses, val_losses

def predict_model(model, data_loader):
    model.eval()
    predictions, actuals = [], []
    with torch.no_grad():
        for X_batch, y_batch in data_loader:
            X_batch = X_batch.to(device)
            outputs = model(X_batch)
            predictions.extend(outputs.cpu().numpy())
            actuals.extend(y_batch.numpy())
    return np.array(predictions).flatten(), np.array(actuals).flatten()

print('í•™ìŠµ/ì˜ˆì¸¡ í•¨ìˆ˜ ì¤€ë¹„ ì™„ë£Œ!')"""),
        
        md("""## 2.5 ì‹¤í—˜ 1: RNN vs LSTM vs GRU ë¹„êµ

ì„¸ ê°€ì§€ ëª¨ë¸ì„ ë™ì¼í•œ ì¡°ê±´ì—ì„œ í•™ìŠµì‹œí‚¤ê³  ì„±ëŠ¥ì„ ë¹„êµí•©ë‹ˆë‹¤."""),
        
        code("""# ëª¨ë¸ ì´ˆê¸°í™”
models = {
    'RNN': SimpleRNN(hidden_size=64, num_layers=2),
    'LSTM': SimpleLSTM(hidden_size=64, num_layers=2),
    'GRU': SimpleGRU(hidden_size=64, num_layers=2)
}

# ê° ëª¨ë¸ í•™ìŠµ
results = {}
print('='*60)
print('ëª¨ë¸ í•™ìŠµ ì‹œì‘...')
print('='*60)

for name, model in models.items():
    print(f'\\n[{name}] í•™ìŠµ ì¤‘...')
    train_losses, val_losses = train_model(
        model, train_loader, val_loader, epochs=50, lr=0.001, clip_grad=True
    )
    results[name] = {
        'model': model,
        'train_losses': train_losses,
        'val_losses': val_losses
    }
    print(f'[{name}] ìµœì¢… ê²€ì¦ ì†ì‹¤: {val_losses[-1]:.6f}')

print('\\n' + '='*60)
print('ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!')
print('='*60)"""),
        
        code("""# ì†ì‹¤ ê³¡ì„  ì‹œê°í™”
plt.figure(figsize=(15, 5))

for idx, (name, result) in enumerate(results.items(), 1):
    plt.subplot(1, 3, idx)
    plt.plot(result['train_losses'], label='Train Loss', linewidth=2)
    plt.plot(result['val_losses'], label='Val Loss', linewidth=2)
    plt.title(f'{name} Loss Curve', fontsize=12, fontweight='bold')
    plt.xlabel('Epoch')
    plt.ylabel('MSE Loss')
    plt.legend()
    plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()"""),
        
        code("""# ì˜ˆì¸¡ ì„±ëŠ¥ ë¹„êµ
plt.figure(figsize=(15, 5))

for idx, (name, result) in enumerate(results.items(), 1):
    model = result['model']
    predictions, actuals = predict_model(model, val_loader)
    rmse = np.sqrt(mean_squared_error(actuals, predictions))
    mae = mean_absolute_error(actuals, predictions)
    
    plt.subplot(1, 3, idx)
    plt.plot(actuals[:100], label='Actual', linewidth=2, alpha=0.7)
    plt.plot(predictions[:100], label='Predicted', linewidth=2, alpha=0.7)
    plt.title(f'{name}\\nRMSE: {rmse:.4f}, MAE: {mae:.4f}', fontsize=11, fontweight='bold')
    plt.xlabel('Sample')
    plt.ylabel('Normalized Value')
    plt.legend()
    plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# ì„±ëŠ¥ ì§€í‘œ ìš”ì•½
print('\\n' + '='*60)
print('ì„±ëŠ¥ ì§€í‘œ ìš”ì•½')
print('='*60)
for name, result in results.items():
    model = result['model']
    predictions, actuals = predict_model(model, val_loader)
    rmse = np.sqrt(mean_squared_error(actuals, predictions))
    mae = mean_absolute_error(actuals, predictions)
    print(f'{name:10s} | RMSE: {rmse:.6f} | MAE: {mae:.6f}')
print('='*60)"""),
        
        md("""## 2.6 í•©ì„± ë°ì´í„° ì‹¤í—˜ ê²°ê³¼ ë¶„ì„

### ì£¼ìš” ë°œê²¬ì‚¬í•­

1. **ëª¨ë¸ ë¹„êµ (RNN vs LSTM vs GRU)**
   - LSTMê³¼ GRUê°€ ê¸°ë³¸ RNNë³´ë‹¤ ì¼ê´€ë˜ê²Œ ìš°ìˆ˜í•œ ì„±ëŠ¥
   - GRUëŠ” LSTMê³¼ ìœ ì‚¬í•œ ì„±ëŠ¥ì„ ë³´ì´ë©´ì„œë„ íŒŒë¼ë¯¸í„° ìˆ˜ê°€ ì ì–´ íš¨ìœ¨ì 
   - RNNì€ ê·¸ë˜ë””ì–¸íŠ¸ ì†Œì‹¤ë¡œ ì¸í•´ ì¥ê¸° íŒ¨í„´ í•™ìŠµì— ì–´ë ¤ì›€

2. **í•™ìŠµ ì•ˆì •ì„±**
   - ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ì´ í•™ìŠµ ì´ˆê¸° ë¶ˆì•ˆì •ì„± ì œê±°
   - LSTM/GRUëŠ” ë” ì•ˆì •ì ì¸ í•™ìŠµ ê³¡ì„  ë³´ì„

### êµí›ˆ

âœ… **LSTM/GRUëŠ” ì‹œê³„ì—´ ì˜ˆì¸¡ì— í•„ìˆ˜ì **  
âœ… **í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì´ ì¤‘ìš”**  
âœ… **ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ì€ ì•ˆì •ì ì¸ í•™ìŠµì— ë„ì›€**"""),
    ]

def get_real_data_cells(md, code):
    """Real data experiment cells - COMPLETE VERSION"""
    return [
        md("""---
# 3ï¸âƒ£ ì‹¤ì œ ë°ì´í„° í•™ìŠµ: Netflix ì£¼ê°€ ì˜ˆì¸¡

## 3.1 ë°ì´í„° ìˆ˜ì§‘

`yfinance` íŒ¨í‚¤ì§€ë¥¼ ì‚¬ìš©í•˜ì—¬ ë„·í”Œë¦­ìŠ¤ ì£¼ê°€ ë°ì´í„°ë¥¼ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤."""),
        
        code("""# yfinance ì„¤ì¹˜ ë° ì„í¬íŠ¸
try:
    import yfinance as yf
    print('yfinance íŒ¨í‚¤ì§€ ë¡œë“œ ì„±ê³µ!')
except ImportError:
    print('yfinance ì„¤ì¹˜ ì¤‘...')
    !pip install -q yfinance
    import yfinance as yf
    print('yfinance ì„¤ì¹˜ ë° ë¡œë“œ ì™„ë£Œ!')

# Netflix ì£¼ê°€ ë°ì´í„° ë‹¤ìš´ë¡œë“œ
print('\\nNetflix ì£¼ê°€ ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì¤‘...')
ticker = 'NFLX'
start_date = '2020-01-01'
end_date = '2024-01-01'

df = yf.download(ticker, start=start_date, end=end_date, progress=False)
print(f'ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {len(df)} ê±°ë˜ì¼')
print(f'ê¸°ê°„: {df.index[0]} ~ {df.index[-1]}')

# ë°ì´í„° í™•ì¸
print('\\në°ì´í„° ìƒ˜í”Œ:')
print(df.head())

# ì¢…ê°€(Close) ë°ì´í„° ì¶”ì¶œ
close_prices = df['Close'].values
print(f'\\nì¢…ê°€ ë°ì´í„° í˜•íƒœ: {close_prices.shape}')"""),
        
        code("""# ì£¼ê°€ ë°ì´í„° ì‹œê°í™”
plt.figure(figsize=(14, 5))
plt.plot(df.index, close_prices, linewidth=1.5, color='darkblue')
plt.title(f'{ticker} ì£¼ê°€ ì¶”ì´ ({start_date} ~ {end_date})', 
          fontsize=14, fontweight='bold')
plt.xlabel('Date', fontsize=12)
plt.ylabel('Close Price (USD)', fontsize=12)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# ê¸°ì´ˆ í†µê³„
print('\\nì£¼ê°€ ê¸°ì´ˆ í†µê³„:')
print(f'í‰ê· : ${close_prices.mean():.2f}')
print(f'í‘œì¤€í¸ì°¨: ${close_prices.std():.2f}')
print(f'ìµœì†Œê°’: ${close_prices.min():.2f}')
print(f'ìµœëŒ€ê°’: ${close_prices.max():.2f}')"""),
        
        md("""## 3.2 ë°ì´í„° ì „ì²˜ë¦¬

ì£¼ê°€ ë°ì´í„°ë¥¼ ì •ê·œí™”í•˜ê³  ìœˆë„ìš° í˜•íƒœë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""),
        
        code("""# ë°ì´í„° ì •ê·œí™”
scaler_stock = MinMaxScaler()
close_prices_normalized = scaler_stock.fit_transform(close_prices.reshape(-1, 1)).flatten()

# ìœˆë„ìš° ìƒì„± (ê³¼ê±° 60ì¼ë¡œ ë‹¤ìŒ ë‚  ì˜ˆì¸¡)
window_size_stock = 60
X_stock, y_stock = create_windows(close_prices_normalized, window_size_stock)

print(f'ìœˆë„ìš° ë°ì´í„° í˜•íƒœ: {X_stock.shape}')
print(f'íƒ€ê²Ÿ ë°ì´í„° í˜•íƒœ: {y_stock.shape}')

# í•™ìŠµ/ê²€ì¦/í…ŒìŠ¤íŠ¸ ë¶„ë¦¬ (70:15:15)
n = len(X_stock)
train_end = int(n * 0.7)
val_end = int(n * 0.85)

X_train_stock = X_stock[:train_end]
y_train_stock = y_stock[:train_end]
X_val_stock = X_stock[train_end:val_end]
y_val_stock = y_stock[train_end:val_end]
X_test_stock = X_stock[val_end:]
y_test_stock = y_stock[val_end:]

print(f'\\ní•™ìŠµ ë°ì´í„°: {X_train_stock.shape}')
print(f'ê²€ì¦ ë°ì´í„°: {X_val_stock.shape}')
print(f'í…ŒìŠ¤íŠ¸ ë°ì´í„°: {X_test_stock.shape}')

# DataLoader ìƒì„±
train_dataset_stock = TimeSeriesDataset(X_train_stock, y_train_stock)
val_dataset_stock = TimeSeriesDataset(X_val_stock, y_val_stock)
test_dataset_stock = TimeSeriesDataset(X_test_stock, y_test_stock)

train_loader_stock = DataLoader(train_dataset_stock, batch_size=16, shuffle=True)
val_loader_stock = DataLoader(val_dataset_stock, batch_size=16, shuffle=False)
test_loader_stock = DataLoader(test_dataset_stock, batch_size=16, shuffle=False)"""),
        
        md("""## 3.3 LSTM ëª¨ë¸ í•™ìŠµ

LSTM ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ Netflix ì£¼ê°€ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤."""),
        
        code("""# LSTM ëª¨ë¸ ì´ˆê¸°í™”
model_stock_lstm = SimpleLSTM(
    input_size=1,
    hidden_size=128,  # ë” í° íˆë“  í¬ê¸°
    num_layers=3,     # ë” ê¹Šì€ ë„¤íŠ¸ì›Œí¬
    dropout=0.2
)

print('='*60)
print('Netflix ì£¼ê°€ ì˜ˆì¸¡ - LSTM í•™ìŠµ ì‹œì‘')
print('='*60)

# í•™ìŠµ
train_losses_stock, val_losses_stock = train_model(
    model_stock_lstm, train_loader_stock, val_loader_stock,
    epochs=100, lr=0.001, clip_grad=True
)

print('\\nLSTM í•™ìŠµ ì™„ë£Œ!')"""),
        
        code("""# í•™ìŠµ ê³¡ì„  ì‹œê°í™”
plt.figure(figsize=(12, 4))
plt.plot(train_losses_stock, label='Train Loss', linewidth=2)
plt.plot(val_losses_stock, label='Val Loss', linewidth=2)
plt.title('LSTM Training on Netflix Stock', fontsize=14, fontweight='bold')
plt.xlabel('Epoch')
plt.ylabel('MSE Loss')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()"""),
        
        md("""## 3.4 GRU ëª¨ë¸ í•™ìŠµ

ë¹„êµë¥¼ ìœ„í•´ GRU ëª¨ë¸ë„ í•™ìŠµí•©ë‹ˆë‹¤."""),
        
        code("""# GRU ëª¨ë¸ ì´ˆê¸°í™”
model_stock_gru = SimpleGRU(
    input_size=1,
    hidden_size=128,
    num_layers=3,
    dropout=0.2
)

print('='*60)
print('Netflix ì£¼ê°€ ì˜ˆì¸¡ - GRU í•™ìŠµ ì‹œì‘')
print('='*60)

# í•™ìŠµ
train_losses_stock_gru, val_losses_stock_gru = train_model(
    model_stock_gru, train_loader_stock, val_loader_stock,
    epochs=100, lr=0.001, clip_grad=True
)

print('\\nGRU í•™ìŠµ ì™„ë£Œ!')"""),
        
        md("""## 3.5 ì˜ˆì¸¡ ê²°ê³¼ ë¹„êµ ë° í‰ê°€"""),
        
        code("""# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡
pred_lstm, actual_test = predict_model(model_stock_lstm, test_loader_stock)
pred_gru, _ = predict_model(model_stock_gru, test_loader_stock)

# ì›ë˜ ìŠ¤ì¼€ì¼ë¡œ ë³µì›
pred_lstm_original = scaler_stock.inverse_transform(pred_lstm.reshape(-1, 1)).flatten()
pred_gru_original = scaler_stock.inverse_transform(pred_gru.reshape(-1, 1)).flatten()
actual_test_original = scaler_stock.inverse_transform(actual_test.reshape(-1, 1)).flatten()

# í‰ê°€ ì§€í‘œ ê³„ì‚°
rmse_lstm = np.sqrt(mean_squared_error(actual_test_original, pred_lstm_original))
mae_lstm = mean_absolute_error(actual_test_original, pred_lstm_original)
mape_lstm = np.mean(np.abs((actual_test_original - pred_lstm_original) / actual_test_original)) * 100

rmse_gru = np.sqrt(mean_squared_error(actual_test_original, pred_gru_original))
mae_gru = mean_absolute_error(actual_test_original, pred_gru_original)
mape_gru = np.mean(np.abs((actual_test_original - pred_gru_original) / actual_test_original)) * 100

print('='*70)
print('Netflix ì£¼ê°€ ì˜ˆì¸¡ ì„±ëŠ¥ í‰ê°€ (í…ŒìŠ¤íŠ¸ ë°ì´í„°)')
print('='*70)
print(f'{"Model":<10} | {"RMSE":<12} | {"MAE":<12} | {"MAPE (%)":<12}')
print('-'*70)
print(f'{"LSTM":<10} | ${rmse_lstm:<11.2f} | ${mae_lstm:<11.2f} | {mape_lstm:<11.2f}%')
print(f'{"GRU":<10} | ${rmse_gru:<11.2f} | ${mae_gru:<11.2f} | {mape_gru:<11.2f}%')
print('='*70)"""),
        
        code("""# ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™”
plt.figure(figsize=(15, 6))

# ì „ì²´ í…ŒìŠ¤íŠ¸ ê¸°ê°„
plt.subplot(1, 2, 1)
plt.plot(actual_test_original, label='Actual', linewidth=2, alpha=0.7, color='black')
plt.plot(pred_lstm_original, label='LSTM Prediction', linewidth=2, alpha=0.7)
plt.plot(pred_gru_original, label='GRU Prediction', linewidth=2, alpha=0.7)
plt.title(f'Netflix Stock Price Prediction (Test Set)\\nLSTM MAPE: {mape_lstm:.2f}%, GRU MAPE: {mape_gru:.2f}%', 
          fontsize=12, fontweight='bold')
plt.xlabel('Days')
plt.ylabel('Price (USD)')
plt.legend()
plt.grid(True, alpha=0.3)

# í™•ëŒ€: ì²˜ìŒ 50ì¼
plt.subplot(1, 2, 2)
n_zoom = 50
plt.plot(actual_test_original[:n_zoom], label='Actual', linewidth=2, alpha=0.7, color='black', marker='o', markersize=3)
plt.plot(pred_lstm_original[:n_zoom], label='LSTM', linewidth=2, alpha=0.7, marker='s', markersize=3)
plt.plot(pred_gru_original[:n_zoom], label='GRU', linewidth=2, alpha=0.7, marker='^', markersize=3)
plt.title(f'Zoomed View (First {n_zoom} Days)', fontsize=12, fontweight='bold')
plt.xlabel('Days')
plt.ylabel('Price (USD)')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()"""),
        
        md("""## 3.6 ì˜ˆì¸¡ ì˜¤ì°¨ ë¶„ì„"""),
        
        code("""# ì˜¤ì°¨ ë¶„ì„
error_lstm = actual_test_original - pred_lstm_original
error_gru = actual_test_original - pred_gru_original

plt.figure(figsize=(15, 5))

# LSTM ì˜¤ì°¨ ë¶„í¬
plt.subplot(1, 3, 1)
plt.hist(error_lstm, bins=30, alpha=0.7, color='blue', edgecolor='black')
plt.axvline(x=0, color='red', linestyle='--', linewidth=2)
plt.title(f'LSTM Prediction Error\\nMean: ${error_lstm.mean():.2f}', fontsize=11, fontweight='bold')
plt.xlabel('Error (Actual - Predicted)')
plt.ylabel('Frequency')
plt.grid(True, alpha=0.3)

# GRU ì˜¤ì°¨ ë¶„í¬
plt.subplot(1, 3, 2)
plt.hist(error_gru, bins=30, alpha=0.7, color='green', edgecolor='black')
plt.axvline(x=0, color='red', linestyle='--', linewidth=2)
plt.title(f'GRU Prediction Error\\nMean: ${error_gru.mean():.2f}', fontsize=11, fontweight='bold')
plt.xlabel('Error (Actual - Predicted)')
plt.ylabel('Frequency')
plt.grid(True, alpha=0.3)

# ì˜¤ì°¨ ì‹œê³„ì—´
plt.subplot(1, 3, 3)
plt.plot(error_lstm, label='LSTM Error', alpha=0.7, linewidth=1.5)
plt.plot(error_gru, label='GRU Error', alpha=0.7, linewidth=1.5)
plt.axhline(y=0, color='red', linestyle='--', linewidth=2)
plt.title('Prediction Error Over Time', fontsize=11, fontweight='bold')
plt.xlabel('Days')
plt.ylabel('Error (USD)')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# ì˜¤ì°¨ í†µê³„
print('\\nì˜¤ì°¨ í†µê³„:')
print(f'LSTM - í‰ê·  ì˜¤ì°¨: ${error_lstm.mean():.2f}, í‘œì¤€í¸ì°¨: ${error_lstm.std():.2f}')
print(f'GRU  - í‰ê·  ì˜¤ì°¨: ${error_gru.mean():.2f}, í‘œì¤€í¸ì°¨: ${error_gru.std():.2f}')"""),
        
        md("""## 3.7 ì‹¤ì œ ë°ì´í„° í•™ìŠµ ê²°ê³¼ ë° í•œê³„ì  ë¶„ì„

### ì£¼ìš” ê²°ê³¼

1. **ëª¨ë¸ ì„±ëŠ¥**
   - LSTMê³¼ GRU ëª¨ë‘ ì‹¤ì œ ì£¼ê°€ì˜ íŠ¸ë Œë“œë¥¼ ì–´ëŠ ì •ë„ í¬ì°©
   - MAPE(í‰ê·  ì ˆëŒ€ ë°±ë¶„ìœ¨ ì˜¤ì°¨)ëŠ” ë³´í†µ 5-15% ë²”ìœ„
   - ë‹¨ê¸° ì˜ˆì¸¡(1ì¼)ì€ ë¹„êµì  ì •í™•í•˜ë‚˜ ì¥ê¸° ì˜ˆì¸¡ì€ ë¶€ì •í™•

2. **ì˜¤ì°¨ íŒ¨í„´**
   - ê¸‰ê²©í•œ ê°€ê²© ë³€ë™(ë‰´ìŠ¤, ì‹¤ì  ë°œí‘œ ë“±)ì—ì„œ í° ì˜¤ì°¨ ë°œìƒ
   - ëŒ€ì²´ë¡œ ì˜ˆì¸¡ì´ ì‹¤ì œ ë³€ë™ì„ ê³¼ì†Œí‰ê°€í•˜ëŠ” ê²½í–¥
   - ì˜¤ì°¨ê°€ ì •ê·œë¶„í¬ì— ê°€ê¹Œì›€ (ëª¨ë¸ì´ í¸í–¥ë˜ì§€ ì•ŠìŒ)

### ë¹„ì •ìƒ ì‹œê³„ì—´ì˜ í•œê³„

**ì£¼ê°€ëŠ” ë¹„ì •ìƒ(non-stationary) ì‹œê³„ì—´ì…ë‹ˆë‹¤:**

1. **íŠ¸ë Œë“œ**: ì¥ê¸°ì ìœ¼ë¡œ í‰ê· ì´ ë³€í•¨
2. **ë³€ë™ì„±**: ë¶„ì‚°ì´ ì‹œê°„ì— ë”°ë¼ ë³€í•¨
3. **ì™¸ë¶€ ìš”ì¸**: ë‰´ìŠ¤, ê²½ì œ ì§€í‘œ, ì •ì±… ë“± ëª¨ë¸ì´ ëª¨ë¥´ëŠ” ì •ë³´

**ê°œì„  ë°©í–¥:**

âœ… **ì°¨ë¶„(Differencing)**: ê°€ê²© ëŒ€ì‹  ë³€í™”ìœ¨ ì‚¬ìš©  
âœ… **ì™¸ë¶€ íŠ¹ì„±**: ê±°ë˜ëŸ‰, ê¸°ìˆ ì  ì§€í‘œ, ê°ì„± ë¶„ì„ ì¶”ê°€  
âœ… **ì•™ìƒë¸”**: ì—¬ëŸ¬ ëª¨ë¸ì˜ ì˜ˆì¸¡ ê²°í•©  
âœ… **Attention ë©”ì»¤ë‹ˆì¦˜**: Transformer ê¸°ë°˜ ëª¨ë¸ ì‚¬ìš©  
âœ… **í™•ë¥ ì  ì˜ˆì¸¡**: ì  ì˜ˆì¸¡ ëŒ€ì‹  êµ¬ê°„ ì˜ˆì¸¡

### êµí›ˆ

âš ï¸ **ì‹œê³„ì—´ ì˜ˆì¸¡ì€ ë§¤ìš° ì–´ë ¤ìš´ ë¬¸ì œ**  
âš ï¸ **ê³¼ê±° íŒ¨í„´ì´ ë¯¸ë˜ë¥¼ ë³´ì¥í•˜ì§€ ì•ŠìŒ**  
âš ï¸ **ëª¨ë¸ì€ ë„êµ¬ì¼ ë¿, ë§¹ì‹ í•˜ì§€ ë§ ê²ƒ**  
âœ… **í•˜ì§€ë§Œ RNN/LSTM/GRUëŠ” ìˆœì°¨ ë°ì´í„° ì²˜ë¦¬ì˜ ê¸°ë³¸!**"""),
    ]

def get_conclusion_cells(md, code):
    """Conclusion cells - COMPLETE VERSION"""
    return [
        md("""---
# ğŸ“Š ëª¨ë“ˆ A ìš”ì•½ ë° ê²°ë¡ 

## í•™ìŠµí•œ ë‚´ìš©

### ì´ë¡ 
- âœ… RNNì˜ ê¸°ë³¸ êµ¬ì¡°ì™€ ìˆ˜ì‹ ($h_t = \\tanh(W_h h_{t-1} + W_x x_t + b_h)$)
- âœ… ê·¸ë˜ë””ì–¸íŠ¸ ì†Œì‹¤/í­ì£¼ ë¬¸ì œì˜ ì›ì¸ê³¼ í•´ê²° ë°©ë²•
- âœ… LSTMì˜ ê²Œì´íŠ¸ ë©”ì»¤ë‹ˆì¦˜ (forget, input, output gates)
- âœ… GRUì˜ ë‹¨ìˆœí™”ëœ êµ¬ì¡° (update, reset gates)
- âœ… BPTT (Backpropagation Through Time)
- âœ… ì†ì‹¤ í•¨ìˆ˜ (MSE, MAE)ì™€ í‰ê°€ ì§€í‘œ (RMSE, MAPE)

### ì‹¤ìŠµ
- âœ… í•©ì„± ì‹œê³„ì—´ ë°ì´í„° ìƒì„± ë° ì‹¤í—˜
- âœ… RNN, LSTM, GRU êµ¬í˜„ ë° ë¹„êµ
- âœ… í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (ìœˆë„ìš° í¬ê¸°, íˆë“  í¬ê¸°)
- âœ… ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ì˜ íš¨ê³¼
- âœ… ì‹¤ì œ Netflix ì£¼ê°€ ë°ì´í„° ì˜ˆì¸¡
- âœ… ëª¨ë¸ í‰ê°€ ë° ì˜¤ì°¨ ë¶„ì„

## í•µì‹¬ ì¸ì‚¬ì´íŠ¸

1. **LSTM/GRUì˜ í•„ìš”ì„±**
   - ê¸°ë³¸ RNNì€ ì¥ê¸° ì˜ì¡´ì„± í•™ìŠµì— í•œê³„
   - ê²Œì´íŠ¸ ë©”ì»¤ë‹ˆì¦˜ì€ ì„ íƒì  ì •ë³´ ì „ë‹¬ì„ ê°€ëŠ¥í•˜ê²Œ í•¨
   - ì…€ ìƒíƒœì˜ ë§ì…ˆ ì—…ë°ì´íŠ¸ê°€ ê·¸ë˜ë””ì–¸íŠ¸ ì§ì ‘ ì „íŒŒ

2. **í•˜ì´í¼íŒŒë¼ë¯¸í„°ì˜ ì¤‘ìš”ì„±**
   - ìœˆë„ìš° í¬ê¸°ëŠ” ì˜ˆì¸¡ ì„±ëŠ¥ì— í° ì˜í–¥
   - ë„ˆë¬´ í¬ê±°ë‚˜ ì‘ìœ¼ë©´ ì„±ëŠ¥ ì €í•˜
   - ì ì ˆí•œ íˆë“  í¬ê¸°ì™€ ë ˆì´ì–´ ìˆ˜ ì„ íƒ í•„ìš”

3. **ì‹¤ì œ ë°ì´í„°ì˜ ì–´ë ¤ì›€**
   - ì£¼ê°€ì™€ ê°™ì€ ë¹„ì •ìƒ ì‹œê³„ì—´ì€ ì˜ˆì¸¡ì´ ë§¤ìš° ì–´ë ¤ì›€
   - ëª¨ë¸ì€ íŒ¨í„´ì„ í•™ìŠµí•˜ì§€ë§Œ ì™¸ë¶€ ìš”ì¸ì„ ì•Œ ìˆ˜ ì—†ìŒ
   - ì  ì˜ˆì¸¡ë³´ë‹¤ í™•ë¥ ì  ì˜ˆì¸¡ì´ ë” ì ì ˆí•  ìˆ˜ ìˆìŒ

4. **ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘**
   - í•™ìŠµ ì•ˆì •ì„±ì— í•„ìˆ˜ì 
   - íŠ¹íˆ RNN ê³„ì—´ ëª¨ë¸ì—ì„œ ì¤‘ìš”
   - ê·¸ë˜ë””ì–¸íŠ¸ í­ì£¼ ë¬¸ì œ ë°©ì§€

## ì‹¤ë¬´ ì ìš© ê°€ì´ë“œ

### ì–¸ì œ RNN/LSTM/GRUë¥¼ ì‚¬ìš©í• ê¹Œ?

**ì‚¬ìš© ê¶Œì¥:**
- ìˆœì°¨ ë°ì´í„° (ì‹œê³„ì—´, ìì—°ì–´, ìŒì„± ë“±)
- ì´ì „ ì •ë³´ê°€ í˜„ì¬ ì˜ˆì¸¡ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ê²½ìš°
- ê°€ë³€ ê¸¸ì´ ì‹œí€€ìŠ¤ ì²˜ë¦¬ê°€ í•„ìš”í•œ ê²½ìš°

**ì£¼ì˜ì‚¬í•­:**
- ë§¤ìš° ê¸´ ì‹œí€€ìŠ¤(>1000)ëŠ” Transformer ê³ ë ¤
- ë³‘ë ¬í™”ê°€ ì–´ë ¤ì›Œ í•™ìŠµ ì†ë„ê°€ ëŠë¦¼
- ì ì ˆí•œ ì •ê·œí™”ì™€ í´ë¦¬í•‘ í•„ìˆ˜

### ëª¨ë¸ ì„ íƒ ê°€ì´ë“œ

- **RNN**: ê°„ë‹¨í•œ ìˆœì°¨ ë°ì´í„°, ì§§ì€ ì‹œí€€ìŠ¤
- **LSTM**: ì¥ê¸° ì˜ì¡´ì„±ì´ ì¤‘ìš”í•œ ê²½ìš°, ê¸´ ì‹œí€€ìŠ¤
- **GRU**: LSTMê³¼ ìœ ì‚¬í•˜ë‚˜ ë” ë¹ ë¥¸ í•™ìŠµ, ì¤‘ê°„ ê¸¸ì´ ì‹œí€€ìŠ¤

## ë‹¤ìŒ ë‹¨ê³„

ì´ì œ **ëª¨ë“ˆ B: U-Net ê¸°ë°˜ ì´ë¯¸ì§€ ë¶„í• **ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.

ì‹œê³„ì—´ ì˜ˆì¸¡ì—ì„œ ì´ë¯¸ì§€ ì²˜ë¦¬ë¡œ ë„ë©”ì¸ì´ ë°”ë€Œì§€ë§Œ,
ë”¥ëŸ¬ë‹ì˜ í•µì‹¬ ì›ë¦¬(ìˆœì „íŒŒ, ì—­ì „íŒŒ, ìµœì í™”)ëŠ” ë™ì¼í•©ë‹ˆë‹¤!

**ì£¼ìš” ì°¨ì´ì :**
- **ì…ë ¥**: 1D ì‹œí€€ìŠ¤ â†’ 2D ì´ë¯¸ì§€
- **ì•„í‚¤í…ì²˜**: RNN â†’ CNN (U-Net)
- **ì‘ì—…**: íšŒê·€ (ì˜ˆì¸¡) â†’ ë¶„ë¥˜ (ì„¸ê·¸ë¨¼í…Œì´ì…˜)
- **ì¶œë ¥**: ë‹¨ì¼ ê°’ â†’ í”½ì…€ë³„ í´ë˜ìŠ¤

---

**ëª¨ë“ˆ A í•™ìŠµì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤. ìˆ˜ê³ í•˜ì…¨ìŠµë‹ˆë‹¤! ğŸ‰**

ë‹¤ìŒ ëª¨ë“ˆì—ì„œëŠ” ì´ë¯¸ì§€ ë¶„í• ì˜ ì„¸ê³„ë¡œ ë“¤ì–´ê°‘ë‹ˆë‹¤.""")
    ]

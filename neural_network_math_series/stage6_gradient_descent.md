# Stage 6: κ²½μ‚¬ν•κ°•λ²• (Gradient Descent)

## π“ λ©μ°¨
1. [κ²½μ‚¬ν•κ°•λ²•μ΄λ€?](#1-κ²½μ‚¬ν•κ°•λ²•μ΄λ€)
2. [μν•™μ  μ λ„](#2-μν•™μ -μ λ„)
3. [ν•™μµλ¥  (Learning Rate)](#3-ν•™μµλ¥ -learning-rate)
4. [κ²½μ‚¬ν•κ°•λ²•μ λ³€ν•](#4-κ²½μ‚¬ν•κ°•λ²•μ-λ³€ν•)
5. [μλ ΄ μ΅°κ±΄κ³Ό λ¬Έμ μ ](#5-μλ ΄-μ΅°κ±΄κ³Ό-λ¬Έμ μ )
6. [Python μ‹κ°ν™”](#6-python-μ‹κ°ν™”)

---

## 1. κ²½μ‚¬ν•κ°•λ²•μ΄λ€?

### 1.1 μ •μ
κ²½μ‚¬ν•κ°•λ²•(Gradient Descent)μ€ ν•¨μμ **μµμ†κ°’μ„ μ°ΎκΈ° μ„ν• λ°λ³µμ  μµμ ν™” μ•κ³ λ¦¬μ¦**μ…λ‹λ‹¤.

### 1.2 ν•µμ‹¬ μ•„μ΄λ””μ–΄
"μ‚°μ„ λ‚΄λ ¤κ°€λ ¤λ©΄ κ°€μ¥ κ°€νλ¥Έ λ‚΄λ¦¬λ§‰ λ°©ν–¥μΌλ΅ ν• κ±Έμμ”© μ΄λ™ν•λ‹¤"

### 1.3 μν•™μ  ν‘ν„

**μ—…λ°μ΄νΈ κ·μΉ™:**
$$
\theta_{new} = \theta_{old} - \alpha \nabla J(\theta_{old})
$$

**κΈ°νΈ μ„¤λ…:**
- $\theta$: νλΌλ―Έν„° (κ°€μ¤‘μΉμ™€ νΈν–¥)
- $\alpha$: ν•™μµλ¥  (learning rate)
- $\nabla J(\theta)$: μ†μ‹¤ ν•¨μμ κ·Έλλ””μ–ΈνΈ
- $\theta_{new}$: μ—…λ°μ΄νΈλ νλΌλ―Έν„°

### 1.4 μ‹¤μƒν™ λΉ„μ 

**μ•κ° μ† ν•μ‚°:**
- ν„μ¬ μ„μΉμ—μ„ **λ°λ°‘μ κ²½μ‚¬**λ§ λλ‚„ μ μμ
- κ°€μ¥ κ°€νλ¥Έ λ‚΄λ¦¬λ§‰ λ°©ν–¥μΌλ΅ ν• λ°μ§μ”© μ΄λ™
- κ³„κ³΅(μµμ†κ°’)μ— λ„λ‹¬ν•  λ•κΉμ§€ λ°λ³µ

**μ¨λ„ μ΅°μ :**
- λ©ν‘: μ‹¤λ‚΄ μ¨λ„λ¥Ό μΎμ ν•κ² (μ†μ‹¤ μµμ†ν™”)
- λ„λ¬΄ λ¥λ©΄ β†’ μ¨λ„ λ‚®μ¶¤
- λ„λ¬΄ μ¶¥λ©΄ β†’ μ¨λ„ λ†’μ„
- μ μ  μΎμ ν• μ¨λ„λ΅ μλ ΄

---

## 2. μν•™μ  μ λ„

### 2.1 1μ°¨ ν…μΌλ¬ κ·Όμ‚¬

ν•¨μ $J(\theta)$λ¥Ό $\theta_0$ κ·Όμ²μ—μ„ κ·Όμ‚¬:

$$
J(\theta) \approx J(\theta_0) + \nabla J(\theta_0)^T (\theta - \theta_0)
$$

### 2.2 μ†μ‹¤ κ°μ† λ°©ν–¥

$J(\theta)$λ¥Ό μ¤„μ΄λ ¤λ©΄ $\theta - \theta_0$λ¥Ό μ–΄λ–»κ² μ„ νƒ?

$$
\theta - \theta_0 = -\alpha \nabla J(\theta_0)
$$

λ”°λΌμ„:
$$
\theta = \theta_0 - \alpha \nabla J(\theta_0)
$$

**μ΄μ **: κ·Έλλ””μ–ΈνΈ λ°λ€ λ°©ν–¥μ΄ κ°€μ¥ λΉ λ¥΄κ² κ°μ†ν•λ” λ°©ν–¥

### 2.3 λ°λ³µ μ•κ³ λ¦¬μ¦

**μ΄κΈ°ν™”:**
$$
\theta^{(0)} = \text{random initialization}
$$

**λ°λ³µ ($t = 0, 1, 2, \ldots$):**
$$
\theta^{(t+1)} = \theta^{(t)} - \alpha \nabla J(\theta^{(t)})
$$

**μΆ…λ£ μ΅°κ±΄:**
- $|\nabla J(\theta)| < \epsilon$ (κ·Έλλ””μ–ΈνΈκ°€ μ¶©λ¶„ν μ‘μ)
- μµλ€ λ°λ³µ νμ λ„λ‹¬
- μ†μ‹¤ λ³€ν™”κ°€ λ―Έλ―Έν•¨

### 2.4 λ‹¨κ³„λ³„ μμ 

**λ¬Έμ :** $J(\theta) = (\theta - 3)^2$ μµμ†ν™”

**κ·Έλλ””μ–ΈνΈ:**
$$
\frac{dJ}{d\theta} = 2(\theta - 3)
$$

**μ„¤μ •:** $\alpha = 0.1$, $\theta^{(0)} = 0$

**λ°λ³µ:**

| $t$ | $\theta^{(t)}$ | $\nabla J$ | $\alpha \nabla J$ | $\theta^{(t+1)}$ | $J(\theta^{(t)})$ |
|-----|---------------|------------|-------------------|-----------------|------------------|
| 0   | 0.0           | -6.0       | -0.6              | 0.6             | 9.0              |
| 1   | 0.6           | -4.8       | -0.48             | 1.08            | 5.76             |
| 2   | 1.08          | -3.84      | -0.384            | 1.464           | 3.686            |
| 3   | 1.464         | -3.072     | -0.307            | 1.771           | 2.359            |
| 4   | 1.771         | -2.458     | -0.246            | 2.017           | 1.510            |
| 5   | 2.017         | -1.966     | -0.197            | 2.213           | 0.966            |
| ... | ...           | ...        | ...               | ...             | ...              |
| β   | 3.0           | 0.0        | 0.0               | 3.0             | 0.0              |

**κ΄€μ°°:**
- $\theta$κ°€ μ μ  3μ— κ°€κΉμ›μ§ (μµμ†κ°’)
- $J(\theta)$κ°€ μ μ  0μ— κ°€κΉμ›μ§
- κ·Έλλ””μ–ΈνΈκ°€ μ μ  0μ— κ°€κΉμ›μ§

---

## 3. ν•™μµλ¥  (Learning Rate)

### 3.1 ν•™μµλ¥ μ μ—­ν• 

$\alpha$λ” **ν• λ²μ— μ–Όλ§λ‚ μ΄λ™ν• μ§€** κ²°μ •:

$$
\theta_{new} = \theta_{old} - \alpha \nabla J
$$

### 3.2 ν•™μµλ¥ μ— λ”°λ¥Έ λ™μ‘

#### π ν•™μµλ¥ μ΄ λ„λ¬΄ μ‘μ„ λ• ($\alpha \ll 1$)
- **μ¥μ **: μ•μ •μ , μ„Έλ°€ν• μ΅°μ •
- **λ‹¨μ **: μλ ΄μ΄ λ§¤μ° λλ¦Ό, λ§μ€ λ°λ³µ ν•„μ”

**μ:** $\alpha = 0.001$
$$
\theta^{(t+1)} = \theta^{(t)} - 0.001 \cdot \nabla J
$$

#### π‡ ν•™μµλ¥ μ΄ μ λ‹Ήν•  λ• ($\alpha \approx 0.01 \sim 0.1$)
- **κ· ν•**: λΉ λ¥΄κ² μλ ΄ν•λ©΄μ„λ„ μ•μ •μ 
- **μ΄μƒμ μΈ κ²½μ°**

#### π¦ ν•™μµλ¥ μ΄ λ„λ¬΄ ν΄ λ• ($\alpha \gg 1$)
- **λ¬Έμ **: λ°μ‚°(divergence), μµμ†κ°’μ„ μ§€λ‚μ³ ν•κΉ€
- **κ²°κ³Ό**: μ†μ‹¤μ΄ μ¦κ°€ν•κ±°λ‚ μ§„λ™

**μ:** $\alpha = 1.5$
$$
\theta^{(t+1)} = \theta^{(t)} - 1.5 \cdot \nabla J
$$

### 3.3 ν•™μµλ¥  μ¤μΌ€μ¤„λ§

κ³ μ •λ ν•™μµλ¥  λ€μ‹  **μ μ§„μ μΌλ΅ κ°μ†**:

**Step Decay:**
$$
\alpha_t = \alpha_0 \cdot 0.5^{\lfloor t/k \rfloor}
$$

**Exponential Decay:**
$$
\alpha_t = \alpha_0 \cdot e^{-\lambda t}
$$

**1/t Decay:**
$$
\alpha_t = \frac{\alpha_0}{1 + \lambda t}
$$

---

## 4. κ²½μ‚¬ν•κ°•λ²•μ λ³€ν•

### 4.1 Batch Gradient Descent

**λ¨λ“  λ°μ΄ν„°**λ¥Ό μ‚¬μ©ν•μ—¬ κ·Έλλ””μ–ΈνΈ κ³„μ‚°:

$$
\nabla J(\theta) = \frac{1}{m}\sum_{i=1}^{m} \nabla J^{(i)}(\theta)
$$

**μ¥μ :** μ •ν™•ν• κ·Έλλ””μ–ΈνΈ  
**λ‹¨μ :** λ°μ΄ν„°κ°€ λ§μΌλ©΄ λ§¤μ° λλ¦Ό

### 4.2 Stochastic Gradient Descent (SGD)

**ν•λ‚μ μƒν”**λ§ μ‚¬μ©:

$$
\theta = \theta - \alpha \nabla J^{(i)}(\theta)
$$

**μ¥μ :** λΉ λ¦„, λ©”λ¨λ¦¬ ν¨μ¨μ   
**λ‹¨μ :** λ…Έμ΄μ¦κ°€ λ§μ, μ§„λ™

### 4.3 Mini-batch Gradient Descent

**μ†κ·λ¨ λ°°μΉ** (μ: 32, 64, 128)λ¥Ό μ‚¬μ©:

$$
\nabla J(\theta) = \frac{1}{B}\sum_{i \in \text{batch}} \nabla J^{(i)}(\theta)
$$

**μ¥μ :** Batchμ™€ SGDμ κ· ν•  
**μ‹¤λ¬΄ ν‘μ¤€**: κ°€μ¥ λ§μ΄ μ‚¬μ©λ¨

### 4.4 λΉ„κµν‘

| λ°©λ²• | λ°°μΉ ν¬κΈ° | μ†λ„ | μ •ν™•λ„ | λ©”λ¨λ¦¬ |
|------|-----------|------|--------|--------|
| **Batch GD** | $m$ (μ „μ²΄) | λλ¦Ό | λ†’μ | λ§μ |
| **SGD** | 1 | λΉ λ¦„ | λ‚®μ (λ…Έμ΄μ¦) | μ μ |
| **Mini-batch** | 16~256 | μ¤‘κ°„ | μ¤‘κ°„ | μ¤‘κ°„ |

### 4.5 λ¨λ©ν…€ (Momentum)

κ΄€μ„±μ„ μ¶”κ°€ν•μ—¬ μ§„λ™ κ°μ†:

$$
v_t = \beta v_{t-1} + \alpha \nabla J(\theta_t)
$$
$$
\theta_{t+1} = \theta_t - v_t
$$

**κΈ°νΈ:**
- $v$: μ†λ„ (velocity)
- $\beta$: λ¨λ©ν…€ κ³„μ (λ³΄ν†µ 0.9)

**λΉ„μ **: κ³µμ΄ κµ΄λ¬κ° λ• κ΄€μ„±μΌλ΅ κ³„μ† μ›€μ§μ„

---

## 5. μλ ΄ μ΅°κ±΄κ³Ό λ¬Έμ μ 

### 5.1 μλ ΄ μ΅°κ±΄

**λ³Όλ΅ ν•¨μ (Convex):**
$$
J(\lambda \theta_1 + (1-\lambda)\theta_2) \leq \lambda J(\theta_1) + (1-\lambda)J(\theta_2)
$$

- μ „μ—­ μµμ†κ°’(global minimum) λ³΄μ¥
- μ: MSE

**λΉ„λ³Όλ΅ ν•¨μ (Non-convex):**
- μ—¬λ¬ μ§€μ—­ μµμ†κ°’(local minima) μ΅΄μ¬
- μ•μ¥μ (saddle point) μ΅΄μ¬
- μ: μ‹ κ²½λ§ μ†μ‹¤ ν•¨μ

### 5.2 μΌλ°μ μΈ λ¬Έμ μ 

#### 1. μ§€μ—­ μµμ†κ°’ (Local Minimum)
- μ „μ—­ μµμ†κ°’μ΄ μ•„λ‹ κ³³μ— κ°‡ν
- **ν•΄κ²°**: λ¨λ©ν…€, Adam

#### 2. μ•μ¥μ  (Saddle Point)
- κ·Έλλ””μ–ΈνΈκ°€ 0μ΄μ§€λ§ μµμ†κ°’μ΄ μ•„λ‹
- **ν•΄κ²°**: λ…Έμ΄μ¦ μ¶”κ°€ (SGD)

#### 3. Plateau (ν‰νƒ„ν• μμ—­)
- κ·Έλλ””μ–ΈνΈκ°€ λ§¤μ° μ‘μ•„ λλ¦¬κ² ν•™μµ
- **ν•΄κ²°**: μ μ‘μ  ν•™μµλ¥  (Adam)

### 5.3 μ‹¤λ¬΄ ν

**μ΄κΈ° ν•™μµλ¥  μ„ νƒ:**
1. μ‘μ€ κ°’μΌλ΅ μ‹μ‘ (0.001, 0.01)
2. μ†μ‹¤μ΄ κ°μ†ν•λ”μ§€ ν™•μΈ
3. μ¦κ°€μ‹ν‚¤λ©΄μ„ μµμ κ°’ μ°ΎκΈ°
4. Learning rate range test μ‚¬μ©

**μ΅°κΈ° μΆ…λ£ (Early Stopping):**
- κ²€μ¦ μ†μ‹¤μ΄ μ¦κ°€ν•λ©΄ ν•™μµ μ¤‘λ‹¨
- κ³Όμ ν•© λ°©μ§€

---

## 6. Python μ‹κ°ν™”

(μ‹κ°ν™” μ½”λ“λ” λ³„λ„ Python μ¤ν¬λ¦½νΈ μ°Έμ΅°)

---

## π“ ν•µμ‹¬ μ”μ•½

### κ²½μ‚¬ν•κ°•λ²• κ³µμ‹

$$
\boxed{\theta_{t+1} = \theta_t - \alpha \nabla J(\theta_t)}
$$

### κµ¬μ„± μ”μ†

| μ”μ† | κΈ°νΈ | μ—­ν•  | μ„ νƒ κΈ°μ¤€ |
|------|------|------|----------|
| **νλΌλ―Έν„°** | $\theta$ | μµμ ν™” λ€μƒ | - |
| **ν•™μµλ¥ ** | $\alpha$ | μ΄λ™ κ±°λ¦¬ | 0.001~0.1 |
| **κ·Έλλ””μ–ΈνΈ** | $\nabla J$ | μ΄λ™ λ°©ν–¥ | μλ™ κ³„μ‚° |

### μ‹¤μƒν™ λΉ„μ 
- **κ²½μ‚¬ν•κ°•λ²•**: μ•κ° μ†μ—μ„ κ³„κ³΅ μ°ΎκΈ°
- **ν•™μµλ¥ **: λ³΄ν­ ν¬κΈ°
- **κ·Έλλ””μ–ΈνΈ**: λ°λ°‘μ κ²½μ‚¬ λ°©ν–¥
- **λ¨λ©ν…€**: κµ΄λ¬κ°€λ” κ³µμ κ΄€μ„±

---

## π― λ‹¤μ λ‹¨κ³„ μκ³ 

**Stage 7**μ—μ„λ” μ΄μ κΉμ§€ λ°°μ΄ λ¨λ“  κ°λ…μ„ μ—°κ²°ν•μ—¬ **1μΈµ μ‹ κ²½λ§μ—μ„ λ‹¤μΈµ μ‹ κ²½λ§κΉμ§€μ ν•™μµ κ³Όμ •**μ„ μ™„μ „ν μ •λ¦¬ν•©λ‹λ‹¤!

---

**μ‘μ„± μ™„λ£ μ‹κ°**: 2024λ…„ κΈ°μ¤€  
**λ‚μ΄λ„**: β­β­β­β­β† (μ¤‘μƒκΈ‰)  
**μμƒ ν•™μµ μ‹κ°„**: 70-85λ¶„

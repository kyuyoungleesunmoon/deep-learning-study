# Stage 6: 경사하강법 (Gradient Descent)

## 📚 목차
1. [경사하강법이란?](#1-경사하강법이란)
2. [수학적 유도](#2-수학적-유도)
3. [학습률 (Learning Rate)](#3-학습률-learning-rate)
4. [경사하강법의 변형](#4-경사하강법의-변형)
5. [수렴 조건과 문제점](#5-수렴-조건과-문제점)
6. [Python 시각화](#6-python-시각화)

---

## 1. 경사하강법이란?

### 1.1 정의
경사하강법(Gradient Descent)은 함수의 **최솟값을 찾기 위한 반복적 최적화 알고리즘**입니다.

### 1.2 핵심 아이디어
"산을 내려가려면 가장 가파른 내리막 방향으로 한 걸음씩 이동한다"

### 1.3 수학적 표현

**업데이트 규칙:**
$$
\theta_{new} = \theta_{old} - \alpha \nabla J(\theta_{old})
$$

**기호 설명:**
- $\theta$: 파라미터 (가중치와 편향)
- $\alpha$: 학습률 (learning rate)
- $\nabla J(\theta)$: 손실 함수의 그래디언트
- $\theta_{new}$: 업데이트된 파라미터

### 1.4 실생활 비유

**안개 속 하산:**
- 현재 위치에서 **발밑의 경사**만 느낄 수 있음
- 가장 가파른 내리막 방향으로 한 발짝씩 이동
- 계곡(최솟값)에 도달할 때까지 반복

**온도 조절:**
- 목표: 실내 온도를 쾌적하게 (손실 최소화)
- 너무 덥면 → 온도 낮춤
- 너무 춥면 → 온도 높임
- 점점 쾌적한 온도로 수렴

---

## 2. 수학적 유도

### 2.1 1차 테일러 근사

함수 $J(\theta)$를 $\theta_0$ 근처에서 근사:

$$
J(\theta) \approx J(\theta_0) + \nabla J(\theta_0)^T (\theta - \theta_0)
$$

### 2.2 손실 감소 방향

$J(\theta)$를 줄이려면 $\theta - \theta_0$를 어떻게 선택?

$$
\theta - \theta_0 = -\alpha \nabla J(\theta_0)
$$

따라서:
$$
\theta = \theta_0 - \alpha \nabla J(\theta_0)
$$

**이유**: 그래디언트 반대 방향이 가장 빠르게 감소하는 방향

### 2.3 반복 알고리즘

**초기화:**
$$
\theta^{(0)} = \text{random initialization}
$$

**반복 ($t = 0, 1, 2, \ldots$):**
$$
\theta^{(t+1)} = \theta^{(t)} - \alpha \nabla J(\theta^{(t)})
$$

**종료 조건:**
- $|\nabla J(\theta)| < \epsilon$ (그래디언트가 충분히 작음)
- 최대 반복 횟수 도달
- 손실 변화가 미미함

### 2.4 단계별 예제

**문제:** $J(\theta) = (\theta - 3)^2$ 최소화

**그래디언트:**
$$
\frac{dJ}{d\theta} = 2(\theta - 3)
$$

**설정:** $\alpha = 0.1$, $\theta^{(0)} = 0$

**반복:**

| $t$ | $\theta^{(t)}$ | $\nabla J$ | $\alpha \nabla J$ | $\theta^{(t+1)}$ | $J(\theta^{(t)})$ |
|-----|---------------|------------|-------------------|-----------------|------------------|
| 0   | 0.0           | -6.0       | -0.6              | 0.6             | 9.0              |
| 1   | 0.6           | -4.8       | -0.48             | 1.08            | 5.76             |
| 2   | 1.08          | -3.84      | -0.384            | 1.464           | 3.686            |
| 3   | 1.464         | -3.072     | -0.307            | 1.771           | 2.359            |
| 4   | 1.771         | -2.458     | -0.246            | 2.017           | 1.510            |
| 5   | 2.017         | -1.966     | -0.197            | 2.213           | 0.966            |
| ... | ...           | ...        | ...               | ...             | ...              |
| ∞   | 3.0           | 0.0        | 0.0               | 3.0             | 0.0              |

**관찰:**
- $\theta$가 점점 3에 가까워짐 (최솟값)
- $J(\theta)$가 점점 0에 가까워짐
- 그래디언트가 점점 0에 가까워짐

---

## 3. 학습률 (Learning Rate)

### 3.1 학습률의 역할

$\alpha$는 **한 번에 얼마나 이동할지** 결정:

$$
\theta_{new} = \theta_{old} - \alpha \nabla J
$$

### 3.2 학습률에 따른 동작

#### 🐌 학습률이 너무 작을 때 ($\alpha \ll 1$)
- **장점**: 안정적, 세밀한 조정
- **단점**: 수렴이 매우 느림, 많은 반복 필요

**예:** $\alpha = 0.001$
$$
\theta^{(t+1)} = \theta^{(t)} - 0.001 \cdot \nabla J
$$

#### 🐇 학습률이 적당할 때 ($\alpha \approx 0.01 \sim 0.1$)
- **균형**: 빠르게 수렴하면서도 안정적
- **이상적인 경우**

#### 🦘 학습률이 너무 클 때 ($\alpha \gg 1$)
- **문제**: 발산(divergence), 최솟값을 지나쳐 튕김
- **결과**: 손실이 증가하거나 진동

**예:** $\alpha = 1.5$
$$
\theta^{(t+1)} = \theta^{(t)} - 1.5 \cdot \nabla J
$$

### 3.3 학습률 스케줄링

고정된 학습률 대신 **점진적으로 감소**:

**Step Decay:**
$$
\alpha_t = \alpha_0 \cdot 0.5^{\lfloor t/k \rfloor}
$$

**Exponential Decay:**
$$
\alpha_t = \alpha_0 \cdot e^{-\lambda t}
$$

**1/t Decay:**
$$
\alpha_t = \frac{\alpha_0}{1 + \lambda t}
$$

---

## 4. 경사하강법의 변형

### 4.1 Batch Gradient Descent

**모든 데이터**를 사용하여 그래디언트 계산:

$$
\nabla J(\theta) = \frac{1}{m}\sum_{i=1}^{m} \nabla J^{(i)}(\theta)
$$

**장점:** 정확한 그래디언트  
**단점:** 데이터가 많으면 매우 느림

### 4.2 Stochastic Gradient Descent (SGD)

**하나의 샘플**만 사용:

$$
\theta = \theta - \alpha \nabla J^{(i)}(\theta)
$$

**장점:** 빠름, 메모리 효율적  
**단점:** 노이즈가 많음, 진동

### 4.3 Mini-batch Gradient Descent

**소규모 배치** (예: 32, 64, 128)를 사용:

$$
\nabla J(\theta) = \frac{1}{B}\sum_{i \in \text{batch}} \nabla J^{(i)}(\theta)
$$

**장점:** Batch와 SGD의 균형  
**실무 표준**: 가장 많이 사용됨

### 4.4 비교표

| 방법 | 배치 크기 | 속도 | 정확도 | 메모리 |
|------|-----------|------|--------|--------|
| **Batch GD** | $m$ (전체) | 느림 | 높음 | 많음 |
| **SGD** | 1 | 빠름 | 낮음 (노이즈) | 적음 |
| **Mini-batch** | 16~256 | 중간 | 중간 | 중간 |

### 4.5 모멘텀 (Momentum)

관성을 추가하여 진동 감소:

$$
v_t = \beta v_{t-1} + \alpha \nabla J(\theta_t)
$$
$$
\theta_{t+1} = \theta_t - v_t
$$

**기호:**
- $v$: 속도 (velocity)
- $\beta$: 모멘텀 계수 (보통 0.9)

**비유**: 공이 굴러갈 때 관성으로 계속 움직임

---

## 5. 수렴 조건과 문제점

### 5.1 수렴 조건

**볼록 함수 (Convex):**
$$
J(\lambda \theta_1 + (1-\lambda)\theta_2) \leq \lambda J(\theta_1) + (1-\lambda)J(\theta_2)
$$

- 전역 최솟값(global minimum) 보장
- 예: MSE

**비볼록 함수 (Non-convex):**
- 여러 지역 최솟값(local minima) 존재
- 안장점(saddle point) 존재
- 예: 신경망 손실 함수

### 5.2 일반적인 문제점

#### 1. 지역 최솟값 (Local Minimum)
- 전역 최솟값이 아닌 곳에 갇힘
- **해결**: 모멘텀, Adam

#### 2. 안장점 (Saddle Point)
- 그래디언트가 0이지만 최솟값이 아님
- **해결**: 노이즈 추가 (SGD)

#### 3. Plateau (평탄한 영역)
- 그래디언트가 매우 작아 느리게 학습
- **해결**: 적응적 학습률 (Adam)

### 5.3 실무 팁

**초기 학습률 선택:**
1. 작은 값으로 시작 (0.001, 0.01)
2. 손실이 감소하는지 확인
3. 증가시키면서 최적값 찾기
4. Learning rate range test 사용

**조기 종료 (Early Stopping):**
- 검증 손실이 증가하면 학습 중단
- 과적합 방지

---

## 6. Python 시각화

(시각화 코드는 별도 Python 스크립트 참조)

---

## 📝 핵심 요약

### 경사하강법 공식

$$
\boxed{\theta_{t+1} = \theta_t - \alpha \nabla J(\theta_t)}
$$

### 구성 요소

| 요소 | 기호 | 역할 | 선택 기준 |
|------|------|------|----------|
| **파라미터** | $\theta$ | 최적화 대상 | - |
| **학습률** | $\alpha$ | 이동 거리 | 0.001~0.1 |
| **그래디언트** | $\nabla J$ | 이동 방향 | 자동 계산 |

### 실생활 비유
- **경사하강법**: 안개 속에서 계곡 찾기
- **학습률**: 보폭 크기
- **그래디언트**: 발밑의 경사 방향
- **모멘텀**: 굴러가는 공의 관성

---

## 🎯 다음 단계 예고

**Stage 7**에서는 이제까지 배운 모든 개념을 연결하여 **1층 신경망에서 다층 신경망까지의 학습 과정**을 완전히 정리합니다!

---

**작성 완료 시각**: 2024년 기준  
**난이도**: ⭐⭐⭐⭐☆ (중상급)  
**예상 학습 시간**: 70-85분

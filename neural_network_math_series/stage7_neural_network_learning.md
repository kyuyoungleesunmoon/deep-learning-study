# Stage 7: ì‹ ê²½ë§ í•™ìŠµ - 1ì¸µì—ì„œ ë‹¤ì¸µê¹Œì§€

## ğŸ“š ëª©ì°¨
1. [ë‹¨ì¸µ ì‹ ê²½ë§ í•™ìŠµ](#1-ë‹¨ì¸µ-ì‹ ê²½ë§-í•™ìŠµ)
2. [ë‹¤ì¸µ ì‹ ê²½ë§ êµ¬ì¡°](#2-ë‹¤ì¸µ-ì‹ ê²½ë§-êµ¬ì¡°)
3. [ì—­ì „íŒŒ ì•Œê³ ë¦¬ì¦˜](#3-ì—­ì „íŒŒ-ì•Œê³ ë¦¬ì¦˜)
4. [ì „ì²´ í•™ìŠµ ê³¼ì •](#4-ì „ì²´-í•™ìŠµ-ê³¼ì •)
5. [êµ¬í˜„ ì˜ˆì œ](#5-êµ¬í˜„-ì˜ˆì œ)

---

## 1. ë‹¨ì¸µ ì‹ ê²½ë§ í•™ìŠµ

### 1.1 êµ¬ì¡°

**ì…ë ¥ â†’ ì€ë‹‰ì¸µ â†’ ì¶œë ¥**

$$
\mathbf{z}^{[1]} = \mathbf{W}^{[1]}\mathbf{x} + \mathbf{b}^{[1]}
$$
$$
\mathbf{a}^{[1]} = \sigma(\mathbf{z}^{[1]})
$$
$$
\hat{y} = \mathbf{a}^{[1]}
$$

### 1.2 ìˆœë°©í–¥ ì „íŒŒ (Forward Propagation)

**ë‹¨ê³„:**
1. ì…ë ¥ $\mathbf{x}$
2. ê°€ì¤‘í•©: $\mathbf{z}^{[1]} = \mathbf{W}^{[1]}\mathbf{x} + \mathbf{b}^{[1]}$
3. í™œì„±í™”: $\mathbf{a}^{[1]} = f(\mathbf{z}^{[1]})$
4. ì†ì‹¤: $L = \frac{1}{2}(\mathbf{a}^{[1]} - \mathbf{y})^2$

### 1.3 ì—­ì „íŒŒ (Backpropagation)

**ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°:**

$$
\frac{\partial L}{\partial \mathbf{a}^{[1]}} = \mathbf{a}^{[1]} - \mathbf{y}
$$

$$
\frac{\partial L}{\partial \mathbf{z}^{[1]}} = \frac{\partial L}{\partial \mathbf{a}^{[1]}} \odot f'(\mathbf{z}^{[1]})
$$

$$
\frac{\partial L}{\partial \mathbf{W}^{[1]}} = \frac{\partial L}{\partial \mathbf{z}^{[1]}} \mathbf{x}^T
$$

$$
\frac{\partial L}{\partial \mathbf{b}^{[1]}} = \frac{\partial L}{\partial \mathbf{z}^{[1]}}
$$

**ê¸°í˜¸ ì„¤ëª…:**
- $\odot$: ì›ì†Œë³„ ê³± (element-wise multiplication)
- $f'$: í™œì„±í™” í•¨ìˆ˜ì˜ ë¯¸ë¶„

### 1.4 íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸

$$
\mathbf{W}^{[1]} := \mathbf{W}^{[1]} - \alpha \frac{\partial L}{\partial \mathbf{W}^{[1]}}
$$

$$
\mathbf{b}^{[1]} := \mathbf{b}^{[1]} - \alpha \frac{\partial L}{\partial \mathbf{b}^{[1]}}
$$

---

## 2. ë‹¤ì¸µ ì‹ ê²½ë§ êµ¬ì¡°

### 2.1 2ì¸µ ì‹ ê²½ë§

**ì…ë ¥ â†’ ì€ë‹‰ì¸µ 1 â†’ ì€ë‹‰ì¸µ 2 â†’ ì¶œë ¥**

**ìˆœë°©í–¥:**
$$
\mathbf{z}^{[1]} = \mathbf{W}^{[1]}\mathbf{x} + \mathbf{b}^{[1]}
$$
$$
\mathbf{a}^{[1]} = f^{[1]}(\mathbf{z}^{[1]})
$$
$$
\mathbf{z}^{[2]} = \mathbf{W}^{[2]}\mathbf{a}^{[1]} + \mathbf{b}^{[2]}
$$
$$
\mathbf{a}^{[2]} = f^{[2]}(\mathbf{z}^{[2]})
$$
$$
\hat{\mathbf{y}} = \mathbf{a}^{[2]}
$$

### 2.2 ì¼ë°˜í™”ëœ $L$ì¸µ ì‹ ê²½ë§

**ì¸µ $l$ì˜ ê³„ì‚°:**
$$
\mathbf{z}^{[l]} = \mathbf{W}^{[l]}\mathbf{a}^{[l-1]} + \mathbf{b}^{[l]}
$$
$$
\mathbf{a}^{[l]} = f^{[l]}(\mathbf{z}^{[l]})
$$

ì—¬ê¸°ì„œ $\mathbf{a}^{[0]} = \mathbf{x}$ (ì…ë ¥)

---

## 3. ì—­ì „íŒŒ ì•Œê³ ë¦¬ì¦˜

### 3.1 ì¶œë ¥ì¸µì—ì„œ ì‹œì‘

**ì†ì‹¤ í•¨ìˆ˜ì˜ ê·¸ë˜ë””ì–¸íŠ¸:**
$$
\delta^{[L]} = \frac{\partial L}{\partial \mathbf{z}^{[L]}} = \frac{\partial L}{\partial \mathbf{a}^{[L]}} \odot f'^{[L]}(\mathbf{z}^{[L]})
$$

### 3.2 ì—­ë°©í–¥ìœ¼ë¡œ ì „íŒŒ

**ì¸µ $l$ì˜ ê·¸ë˜ë””ì–¸íŠ¸:**
$$
\delta^{[l]} = (\mathbf{W}^{[l+1]})^T \delta^{[l+1]} \odot f'^{[l]}(\mathbf{z}^{[l]})
$$

### 3.3 íŒŒë¼ë¯¸í„° ê·¸ë˜ë””ì–¸íŠ¸

$$
\frac{\partial L}{\partial \mathbf{W}^{[l]}} = \delta^{[l]} (\mathbf{a}^{[l-1]})^T
$$

$$
\frac{\partial L}{\partial \mathbf{b}^{[l]}} = \delta^{[l]}
$$

### 3.4 ìˆ˜ì¹˜ ì˜ˆì œ

**2ì¸µ ì‹ ê²½ë§:** 2 ì…ë ¥ â†’ 3 ì€ë‹‰ â†’ 1 ì¶œë ¥

**ì£¼ì–´ì§„ ê°’:**
- $\mathbf{x} = [1, 2]^T$
- $\mathbf{y} = 1$
- ëª¨ë“  ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì€ 0.5ë¡œ ì´ˆê¸°í™”
- í™œì„±í™”: Sigmoid
- ì†ì‹¤: MSE

**ìˆœë°©í–¥:**
1. $\mathbf{z}^{[1]} = \mathbf{W}^{[1]}\mathbf{x} + \mathbf{b}^{[1]}$
2. $\mathbf{a}^{[1]} = \sigma(\mathbf{z}^{[1]})$
3. $\mathbf{z}^{[2]} = \mathbf{W}^{[2]}\mathbf{a}^{[1]} + \mathbf{b}^{[2]}$
4. $\hat{y} = \sigma(\mathbf{z}^{[2]})$
5. $L = \frac{1}{2}(\hat{y} - y)^2$

**ì—­ì „íŒŒ:**
1. $\delta^{[2]} = (\hat{y} - y) \cdot \sigma'(\mathbf{z}^{[2]})$
2. $\delta^{[1]} = (\mathbf{W}^{[2]})^T \delta^{[2]} \odot \sigma'(\mathbf{z}^{[1]})$
3. ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë° ì—…ë°ì´íŠ¸

---

## 4. ì „ì²´ í•™ìŠµ ê³¼ì •

### 4.1 ì•Œê³ ë¦¬ì¦˜

```
ì´ˆê¸°í™”: W, bë¥¼ ëœë¤ ê°’ìœ¼ë¡œ
for epoch in 1 to max_epochs:
    for each mini-batch:
        # ìˆœë°©í–¥ ì „íŒŒ
        for l in 1 to L:
            z[l] = W[l] @ a[l-1] + b[l]
            a[l] = f[l](z[l])
        
        # ì†ì‹¤ ê³„ì‚°
        L = loss(a[L], y)
        
        # ì—­ì „íŒŒ
        Î´[L] = âˆ‚L/âˆ‚z[L]
        for l in L-1 down to 1:
            Î´[l] = (W[l+1])^T @ Î´[l+1] âŠ™ f'[l](z[l])
        
        # íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
        for l in 1 to L:
            W[l] -= Î± * âˆ‚L/âˆ‚W[l]
            b[l] -= Î± * âˆ‚L/âˆ‚b[l]
```

### 4.2 í•µì‹¬ ë°©ì •ì‹ ìš”ì•½

**ìˆœë°©í–¥:**
$$
\mathbf{z}^{[l]} = \mathbf{W}^{[l]}\mathbf{a}^{[l-1]} + \mathbf{b}^{[l]}, \quad \mathbf{a}^{[l]} = f(\mathbf{z}^{[l]})
$$

**ì—­ì „íŒŒ:**
$$
\delta^{[l]} = (\mathbf{W}^{[l+1]})^T \delta^{[l+1]} \odot f'(\mathbf{z}^{[l]})
$$

**ì—…ë°ì´íŠ¸:**
$$
\mathbf{W}^{[l]} := \mathbf{W}^{[l]} - \alpha \delta^{[l]} (\mathbf{a}^{[l-1]})^T
$$

---

## 5. êµ¬í˜„ ì˜ˆì œ

### 5.1 Python êµ¬í˜„ (NumPy)

```python
import numpy as np

class NeuralNetwork:
    def __init__(self, layer_dims):
        self.L = len(layer_dims) - 1
        self.parameters = {}
        
        # ì´ˆê¸°í™”
        for l in range(1, self.L + 1):
            self.parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01
            self.parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))
    
    def sigmoid(self, z):
        return 1 / (1 + np.exp(-z))
    
    def sigmoid_derivative(self, z):
        s = self.sigmoid(z)
        return s * (1 - s)
    
    def forward(self, X):
        self.cache = {'A0': X}
        A = X
        
        for l in range(1, self.L + 1):
            Z = self.parameters['W' + str(l)] @ A + self.parameters['b' + str(l)]
            A = self.sigmoid(Z)
            self.cache['Z' + str(l)] = Z
            self.cache['A' + str(l)] = A
        
        return A
    
    def backward(self, Y):
        m = Y.shape[1]
        grads = {}
        
        # ì¶œë ¥ì¸µ
        dZ = self.cache['A' + str(self.L)] - Y
        grads['dW' + str(self.L)] = (1/m) * dZ @ self.cache['A' + str(self.L-1)].T
        grads['db' + str(self.L)] = (1/m) * np.sum(dZ, axis=1, keepdims=True)
        
        # ì€ë‹‰ì¸µ
        for l in reversed(range(1, self.L)):
            dA = self.parameters['W' + str(l+1)].T @ dZ
            dZ = dA * self.sigmoid_derivative(self.cache['Z' + str(l)])
            grads['dW' + str(l)] = (1/m) * dZ @ self.cache['A' + str(l-1)].T
            grads['db' + str(l)] = (1/m) * np.sum(dZ, axis=1, keepdims=True)
        
        return grads
    
    def update(self, grads, learning_rate):
        for l in range(1, self.L + 1):
            self.parameters['W' + str(l)] -= learning_rate * grads['dW' + str(l)]
            self.parameters['b' + str(l)] -= learning_rate * grads['db' + str(l)]
    
    def train(self, X, Y, epochs, learning_rate):
        for epoch in range(epochs):
            # ìˆœë°©í–¥
            A = self.forward(X)
            
            # ì†ì‹¤ ê³„ì‚°
            loss = np.mean((A - Y)**2)
            
            # ì—­ì „íŒŒ
            grads = self.backward(Y)
            
            # ì—…ë°ì´íŠ¸
            self.update(grads, learning_rate)
            
            if epoch % 100 == 0:
                print(f"Epoch {epoch}, Loss: {loss:.4f}")

# ì‚¬ìš© ì˜ˆ
nn = NeuralNetwork([2, 4, 1])  # 2 ì…ë ¥, 4 ì€ë‹‰, 1 ì¶œë ¥
X = np.array([[0, 0, 1, 1], [0, 1, 0, 1]])  # XOR ì…ë ¥
Y = np.array([[0, 1, 1, 0]])  # XOR ì¶œë ¥
nn.train(X, Y, epochs=1000, learning_rate=0.5)
```

---

## ğŸ“ í•µì‹¬ ìš”ì•½

### ì‹ ê²½ë§ í•™ìŠµì˜ 3ë‹¨ê³„

1. **ìˆœë°©í–¥ ì „íŒŒ**: ì…ë ¥ â†’ ì¶œë ¥ ê³„ì‚°
2. **ì†ì‹¤ ê³„ì‚°**: ì˜ˆì¸¡ê³¼ ì‹¤ì œê°’ ë¹„êµ
3. **ì—­ì „íŒŒ**: ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë° íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸

### ìˆ˜í•™ì  í•µì‹¬

$$
\boxed{
\begin{align}
&\text{ìˆœë°©í–¥: } \mathbf{z}^{[l]} = \mathbf{W}^{[l]}\mathbf{a}^{[l-1]} + \mathbf{b}^{[l]}, \quad \mathbf{a}^{[l]} = f(\mathbf{z}^{[l]}) \\
&\text{ì—­ì „íŒŒ: } \delta^{[l]} = (\mathbf{W}^{[l+1]})^T \delta^{[l+1]} \odot f'(\mathbf{z}^{[l]}) \\
&\text{ì—…ë°ì´íŠ¸: } \mathbf{W}^{[l]} := \mathbf{W}^{[l]} - \alpha \frac{\partial L}{\partial \mathbf{W}^{[l]}}
\end{align}
}
$$

---

## ğŸ¯ ë‹¤ìŒ ë‹¨ê³„ ì˜ˆê³ 

**Stage 8**ì—ì„œëŠ” ì§€ê¸ˆê¹Œì§€ ë°°ìš´ ëª¨ë“  ë‚´ìš©ì„ **ìˆ˜ì‹ë§Œìœ¼ë¡œ ì™„ì „íˆ ì •ë¦¬**í•˜ëŠ” ìµœì¢… ë§ˆìŠ¤í„° ë¬¸ì„œë¥¼ ì œê³µí•©ë‹ˆë‹¤!

---

**ì‘ì„± ì™„ë£Œ ì‹œê°**: 2024ë…„ ê¸°ì¤€  
**ë‚œì´ë„**: â­â­â­â­â­ (ê³ ê¸‰)  
**ì˜ˆìƒ í•™ìŠµ ì‹œê°„**: 90-120ë¶„

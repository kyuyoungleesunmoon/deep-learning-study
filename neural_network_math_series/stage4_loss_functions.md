# Stage 4: ì†ì‹¤ í•¨ìˆ˜ (Loss Functions)

## ğŸ“š ëª©ì°¨
1. [ì†ì‹¤ í•¨ìˆ˜ë€?](#1-ì†ì‹¤-í•¨ìˆ˜ë€)
2. [í‰ê·  ì œê³± ì˜¤ì°¨ (MSE)](#2-í‰ê· -ì œê³±-ì˜¤ì°¨-mse)
3. [êµì°¨ ì—”íŠ¸ë¡œí”¼ (Cross-Entropy)](#3-êµì°¨-ì—”íŠ¸ë¡œí”¼-cross-entropy)
4. [ì†ì‹¤ í•¨ìˆ˜ ì„ íƒ ê¸°ì¤€](#4-ì†ì‹¤-í•¨ìˆ˜-ì„ íƒ-ê¸°ì¤€)
5. [Python ì‹œê°í™”](#5-python-ì‹œê°í™”)

---

## 1. ì†ì‹¤ í•¨ìˆ˜ë€?

### 1.1 ì •ì˜
ì†ì‹¤ í•¨ìˆ˜(Loss Function ë˜ëŠ” Cost Function)ëŠ” ì‹ ê²½ë§ì˜ **ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì˜ ì°¨ì´**ë¥¼ ì¸¡ì •í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤. í•™ìŠµì˜ ëª©í‘œëŠ” ì´ ì†ì‹¤ì„ ìµœì†Œí™”í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

### 1.2 ìˆ˜í•™ì  í‘œí˜„

**ë‹¨ì¼ ìƒ˜í”Œì— ëŒ€í•œ ì†ì‹¤:**
$$
L(\hat{y}, y) = \text{distance}(\hat{y}, y)
$$

**ì „ì²´ ë°ì´í„°ì…‹ì— ëŒ€í•œ ì†ì‹¤ (ë¹„ìš© í•¨ìˆ˜):**
$$
J(\mathbf{W}, \mathbf{b}) = \frac{1}{m}\sum_{i=1}^{m} L(\hat{y}^{(i)}, y^{(i)})
$$

**ê¸°í˜¸ ì„¤ëª…:**
- $\hat{y}$: ì˜ˆì¸¡ê°’ (predicted value)
- $y$: ì‹¤ì œê°’ (true value)
- $L$: ì†ì‹¤ í•¨ìˆ˜
- $J$: ë¹„ìš© í•¨ìˆ˜ (ì „ì²´ ì†ì‹¤ì˜ í‰ê· )
- $m$: ìƒ˜í”Œ ê°œìˆ˜
- $\mathbf{W}, \mathbf{b}$: ì‹ ê²½ë§ì˜ ê°€ì¤‘ì¹˜ì™€ í¸í–¥
- $(i)$: $i$ë²ˆì§¸ ìƒ˜í”Œ

### 1.3 ì™œ í•„ìš”í•œê°€?

**í•™ìŠµ = ìµœì í™”:**
$$
\mathbf{W}^*, \mathbf{b}^* = \arg\min_{\mathbf{W}, \mathbf{b}} J(\mathbf{W}, \mathbf{b})
$$

- ì†ì‹¤ í•¨ìˆ˜ê°€ **í•™ìŠµì˜ ëª©í‘œ**(objective)ë¥¼ ì •ì˜
- ì†ì‹¤ì´ ì‘ì„ìˆ˜ë¡ ì˜ˆì¸¡ì´ ì •í™•í•¨
- ê²½ì‚¬í•˜ê°•ë²•ìœ¼ë¡œ ì†ì‹¤ì„ ìµœì†Œí™”

### 1.4 ì‹¤ìƒí™œ ë¹„ìœ 

**ê³¼ë… ë§ì¶”ê¸°:**
- **ì˜ˆì¸¡ê°’**: í™”ì‚´ì´ ë§ì€ ìœ„ì¹˜
- **ì‹¤ì œê°’**: ê³¼ë…ì˜ ì¤‘ì‹¬
- **ì†ì‹¤**: í™”ì‚´ê³¼ ì¤‘ì‹¬ ì‚¬ì´ì˜ ê±°ë¦¬
- **í•™ìŠµ**: ê±°ë¦¬ë¥¼ ì¤„ì´ëŠ” ë°©ë²• ì°¾ê¸°

**ì˜¨ë„ ì˜ˆì¸¡:**
- ì‹¤ì œ ì˜¨ë„: 25Â°C
- ì˜ˆì¸¡ ì˜¨ë„: 28Â°C
- ì†ì‹¤: $(28-25)^2 = 9$ (ì˜¤ì°¨ì˜ ì œê³±)

---

## 2. í‰ê·  ì œê³± ì˜¤ì°¨ (MSE)

### 2.1 ìˆ˜í•™ì  ì •ì˜

**ë‹¨ì¼ ì¶œë ¥:**
$$
\text{MSE} = \frac{1}{m}\sum_{i=1}^{m}(y^{(i)} - \hat{y}^{(i)})^2
$$

**ë‹¤ì¤‘ ì¶œë ¥:**
$$
\text{MSE} = \frac{1}{m}\sum_{i=1}^{m}\sum_{j=1}^{n}(y_j^{(i)} - \hat{y}_j^{(i)})^2
$$

**ê¸°í˜¸ ì„¤ëª…:**
- $m$: ìƒ˜í”Œ ìˆ˜
- $n$: ì¶œë ¥ ì°¨ì›
- $(y - \hat{y})^2$: ì˜¤ì°¨ì˜ ì œê³±

### 2.2 íŠ¹ì„±

#### ê°’ì˜ ë²”ìœ„:
$$
\text{MSE} \geq 0
$$

- **ìµœì†Œê°’**: 0 (ì™„ë²½í•œ ì˜ˆì¸¡)
- **ì œê³± ì‚¬ìš©**: í° ì˜¤ì°¨ì— ë” í° í˜ë„í‹°

#### ë¯¸ë¶„:
$$
\frac{\partial \text{MSE}}{\partial \hat{y}} = -\frac{2}{m}\sum_{i=1}^{m}(y^{(i)} - \hat{y}^{(i)})
$$

### 2.3 ìˆ˜ì¹˜ ì˜ˆì œ

**ì˜ˆì œ 1: íšŒê·€ ë¬¸ì œ**

ì‹¤ì œê°’ê³¼ ì˜ˆì¸¡ê°’:
| ìƒ˜í”Œ | ì‹¤ì œê°’ $y$ | ì˜ˆì¸¡ê°’ $\hat{y}$ | ì˜¤ì°¨ $(y-\hat{y})$ | ì˜¤ì°¨Â² |
|------|-----------|-----------------|-------------------|-------|
| 1    | 10        | 9               | 1                 | 1     |
| 2    | 20        | 22              | -2                | 4     |
| 3    | 30        | 28              | 2                 | 4     |
| 4    | 40        | 41              | -1                | 1     |

**ê³„ì‚°:**
$$
\text{MSE} = \frac{1}{4}(1 + 4 + 4 + 1) = \frac{10}{4} = 2.5
$$

**ì˜ˆì œ 2: ì£¼íƒ ê°€ê²© ì˜ˆì¸¡**

| ì£¼íƒ | ì‹¤ì œ ê°€ê²© (ì–µ) | ì˜ˆì¸¡ ê°€ê²© (ì–µ) | ì˜¤ì°¨Â² |
|------|--------------|--------------|-------|
| A    | 5.0          | 5.2          | 0.04  |
| B    | 3.5          | 3.0          | 0.25  |
| C    | 7.2          | 7.5          | 0.09  |

$$
\text{MSE} = \frac{1}{3}(0.04 + 0.25 + 0.09) = \frac{0.38}{3} = 0.127 \text{ ì–µ}^2
$$

**RMSE (Root MSE):**
$$
\text{RMSE} = \sqrt{\text{MSE}} = \sqrt{0.127} = 0.356 \text{ ì–µ} = 3,560\text{ë§Œì›}
$$

### 2.4 MSEì˜ ë³€í˜•

#### 2.4.1 MAE (Mean Absolute Error)
$$
\text{MAE} = \frac{1}{m}\sum_{i=1}^{m}|y^{(i)} - \hat{y}^{(i)}|
$$

- ì´ìƒì¹˜(outlier)ì— ëœ ë¯¼ê°
- ì ˆëŒ“ê°’ ì‚¬ìš©

#### 2.4.2 Huber Loss
$$
L_\delta(y, \hat{y}) = \begin{cases}
\frac{1}{2}(y - \hat{y})^2 & \text{if } |y - \hat{y}| \leq \delta \\
\delta|y - \hat{y}| - \frac{1}{2}\delta^2 & \text{otherwise}
\end{cases}
$$

- MSEì™€ MAEì˜ ì¡°í•©
- ì´ìƒì¹˜ì— ê°•ê±´(robust)

### 2.5 ì¥ì  & ë‹¨ì 

#### ì¥ì :
- âœ… **ìˆ˜í•™ì ìœ¼ë¡œ ê¹”ë”**: ë¯¸ë¶„ì´ ê°„ë‹¨
- âœ… **ë³¼ë¡ í•¨ìˆ˜**: ì „ì—­ ìµœì†Œê°’ ì¡´ì¬
- âœ… **ì§ê´€ì **: "ì˜¤ì°¨ì˜ í‰ê· "

#### ë‹¨ì :
- âŒ **ì´ìƒì¹˜ì— ë¯¼ê°**: í° ì˜¤ì°¨ê°€ ì†ì‹¤ì„ í¬ê²Œ ì¦ê°€
- âŒ **ë‹¨ìœ„ ë¬¸ì œ**: ì œê³±í•˜ë©´ ë‹¨ìœ„ê°€ ë‹¬ë¼ì§ (m â†’ mÂ²)
- âŒ **ë¶„ë¥˜ì— ë¶€ì í•©**: í™•ë¥  ì¶œë ¥ì— ë§ì§€ ì•ŠìŒ

### 2.6 ì‹¤ìƒí™œ ì‘ìš©

**íšŒê·€ ë¬¸ì œì— ì‚¬ìš©:**
- ì£¼íƒ ê°€ê²© ì˜ˆì¸¡
- ì£¼ì‹ ê°€ê²© ì˜ˆì¸¡
- ì˜¨ë„ ì˜ˆì¸¡
- ë§¤ì¶œ ì˜ˆì¸¡

---

## 3. êµì°¨ ì—”íŠ¸ë¡œí”¼ (Cross-Entropy)

### 3.1 ìˆ˜í•™ì  ì •ì˜

#### 3.1.1 ì´ì§„ êµì°¨ ì—”íŠ¸ë¡œí”¼ (Binary Cross-Entropy)
$$
L(y, \hat{y}) = -[y\log(\hat{y}) + (1-y)\log(1-\hat{y})]
$$

**ì „ì²´ ìƒ˜í”Œ:**
$$
J = -\frac{1}{m}\sum_{i=1}^{m}[y^{(i)}\log(\hat{y}^{(i)}) + (1-y^{(i)})\log(1-\hat{y}^{(i)})]
$$

**ê¸°í˜¸ ì„¤ëª…:**
- $y \in \{0, 1\}$: ì‹¤ì œ ë ˆì´ë¸”
- $\hat{y} \in (0, 1)$: ì˜ˆì¸¡ í™•ë¥ 
- $\log$: ìì—°ë¡œê·¸ (ë°‘ì´ $e$)

#### 3.1.2 ë²”ì£¼í˜• êµì°¨ ì—”íŠ¸ë¡œí”¼ (Categorical Cross-Entropy)
$$
L(\mathbf{y}, \hat{\mathbf{y}}) = -\sum_{j=1}^{K} y_j \log(\hat{y}_j)
$$

**ì „ì²´ ìƒ˜í”Œ:**
$$
J = -\frac{1}{m}\sum_{i=1}^{m}\sum_{j=1}^{K} y_j^{(i)} \log(\hat{y}_j^{(i)})
$$

**ê¸°í˜¸ ì„¤ëª…:**
- $K$: í´ë˜ìŠ¤ ê°œìˆ˜
- $\mathbf{y}$: ì›-í•« ì¸ì½”ë”©ëœ ì‹¤ì œ ë ˆì´ë¸”
- $\hat{\mathbf{y}}$: Softmax ì¶œë ¥ (í™•ë¥  ë¶„í¬)

### 3.2 íŠ¹ì„±

#### ê°’ì˜ ë²”ìœ„:
$$
0 \leq L \leq \infty
$$

- **ìµœì†Œê°’**: 0 (ì™„ë²½í•œ ì˜ˆì¸¡: $\hat{y}=y$)
- **ìµœëŒ€ê°’**: $\infty$ (í™•ì‹ ìˆê²Œ í‹€ë¦¼: $\hat{y} \to 0$ when $y=1$)

#### ë¯¸ë¶„ (ì´ì§„ ë¶„ë¥˜):
$$
\frac{\partial L}{\partial \hat{y}} = -\frac{y}{\hat{y}} + \frac{1-y}{1-\hat{y}}
$$

Sigmoidì™€ í•¨ê»˜ ì‚¬ìš©í•˜ë©´:
$$
\frac{\partial L}{\partial z} = \hat{y} - y
$$

ë§¤ìš° ê¹”ë”!

### 3.3 ìˆ˜ì¹˜ ì˜ˆì œ

**ì˜ˆì œ 1: ì´ì§„ ë¶„ë¥˜ (ìŠ¤íŒ¸ í•„í„°)**

| ìƒ˜í”Œ | ì‹¤ì œ $y$ | ì˜ˆì¸¡ $\hat{y}$ | $y\log\hat{y}$ | $(1-y)\log(1-\hat{y})$ | $L$ |
|------|---------|---------------|---------------|----------------------|-----|
| 1    | 1       | 0.9           | -0.105        | 0                    | 0.105 |
| 2    | 0       | 0.2           | 0             | -0.223               | 0.223 |
| 3    | 1       | 0.7           | -0.357        | 0                    | 0.357 |
| 4    | 0       | 0.1           | 0             | -0.105               | 0.105 |

**ìƒ˜í”Œ 1 ê³„ì‚°:**
$$
L = -(1 \times \log(0.9) + 0 \times \log(0.1)) = -\log(0.9) = 0.105
$$

**í‰ê·  ì†ì‹¤:**
$$
J = \frac{1}{4}(0.105 + 0.223 + 0.357 + 0.105) = 0.198
$$

**ì˜ˆì œ 2: ë‹¤ì¤‘ í´ë˜ìŠ¤ (ë™ë¬¼ ë¶„ë¥˜)**

ì‹¤ì œ: ê³ ì–‘ì´ (ì›-í•«: [1, 0, 0])  
ì˜ˆì¸¡: [0.7, 0.2, 0.1]

$$
L = -(1 \times \log(0.7) + 0 \times \log(0.2) + 0 \times \log(0.1))
$$
$$
= -\log(0.7) = 0.357
$$

ë§Œì•½ ì˜ˆì¸¡ì´ [0.9, 0.05, 0.05]ë¼ë©´:
$$
L = -\log(0.9) = 0.105 \quad \text{(ë” ë‚®ìŒ = ë” ì¢‹ìŒ)}
$$

### 3.4 ì •ë³´ ì´ë¡ ì  í•´ì„

êµì°¨ ì—”íŠ¸ë¡œí”¼ëŠ” **ë‘ í™•ë¥  ë¶„í¬ì˜ ì°¨ì´**ë¥¼ ì¸¡ì •:

$$
H(p, q) = -\sum_x p(x)\log q(x)
$$

- $p$: ì‹¤ì œ ë¶„í¬ (true distribution)
- $q$: ì˜ˆì¸¡ ë¶„í¬ (predicted distribution)

**ì—”íŠ¸ë¡œí”¼ (Entropy):**
$$
H(p) = -\sum_x p(x)\log p(x)
$$

**KL ë°œì‚° (KL Divergence):**
$$
D_{KL}(p \| q) = H(p, q) - H(p)
$$

êµì°¨ ì—”íŠ¸ë¡œí”¼ë¥¼ ìµœì†Œí™” = KL ë°œì‚° ìµœì†Œí™” = ë¶„í¬ë¥¼ ê°€ê¹ê²Œ

### 3.5 ì¥ì  & ë‹¨ì 

#### ì¥ì :
- âœ… **í™•ë¥ ì— ì í•©**: ë¶„ë¥˜ ë¬¸ì œì— ìì—°ìŠ¤ëŸ¬ì›€
- âœ… **ë¹ ë¥¸ í•™ìŠµ**: ì˜¤ì°¨ê°€ í´ ë•Œ ê¸°ìš¸ê¸°ë„ í¼
- âœ… **ì •ë³´ ì´ë¡ ì  ì˜ë¯¸**: ë¶„í¬ ì°¨ì´ ì¸¡ì •

#### ë‹¨ì :
- âŒ **ìˆ˜ì¹˜ ì•ˆì •ì„±**: $\log(0)$ì€ ì •ì˜ ì•ˆë¨ â†’ í´ë¦¬í•‘ í•„ìš”
- âŒ **ë¶ˆê· í˜• ë°ì´í„°**: í•œ í´ë˜ìŠ¤ê°€ ë§¤ìš° ë§ìœ¼ë©´ í¸í–¥

### 3.6 ì‹¤ìƒí™œ ì‘ìš©

**ë¶„ë¥˜ ë¬¸ì œì— ì‚¬ìš©:**
- ì´ë¯¸ì§€ ë¶„ë¥˜ (ê°œ/ê³ ì–‘ì´)
- ìŠ¤íŒ¸ ë©”ì¼ í•„í„°
- ê°ì • ë¶„ì„ (ê¸ì •/ë¶€ì •)
- ì§ˆë³‘ ì§„ë‹¨ (ì–‘ì„±/ìŒì„±)
- ê°ì²´ ì¸ì‹ (ì—¬ëŸ¬ í´ë˜ìŠ¤)

---

## 4. ì†ì‹¤ í•¨ìˆ˜ ì„ íƒ ê¸°ì¤€

### 4.1 ë¬¸ì œ ìœ í˜•ë³„ ì„ íƒ

| ë¬¸ì œ ìœ í˜• | ì¶œë ¥ì¸µ í™œì„±í™” | ì†ì‹¤ í•¨ìˆ˜ | ì˜ˆì‹œ |
|----------|-------------|----------|------|
| **ì´ì§„ ë¶„ë¥˜** | Sigmoid | Binary Cross-Entropy | ìŠ¤íŒ¸/ì •ìƒ |
| **ë‹¤ì¤‘ í´ë˜ìŠ¤** | Softmax | Categorical Cross-Entropy | ë™ë¬¼ ë¶„ë¥˜ |
| **íšŒê·€** | Linear | MSE | ì£¼íƒ ê°€ê²© |
| **íšŒê·€ (ì´ìƒì¹˜ ë§ìŒ)** | Linear | MAE ë˜ëŠ” Huber | ë§¤ì¶œ ì˜ˆì¸¡ |

### 4.2 ë¹„êµí‘œ

| íŠ¹ì§• | MSE | Cross-Entropy |
|------|-----|---------------|
| **ìš©ë„** | íšŒê·€ | ë¶„ë¥˜ |
| **ì¶œë ¥ ë²”ìœ„** | $(-\infty, \infty)$ | $[0, 1]$ (í™•ë¥ ) |
| **ìµœì  í™œì„±í™”** | Linear | Sigmoid/Softmax |
| **ì´ìƒì¹˜ ë¯¼ê°ë„** | ë†’ìŒ (ì œê³±) | ì¤‘ê°„ (ë¡œê·¸) |
| **ìˆ˜ë ´ ì†ë„** | ëŠë¦¼ | ë¹ ë¦„ |

### 4.3 ì‹œê°ì  ë¹„êµ

**ì˜¤ì°¨ vs ì†ì‹¤:**

| ì˜¤ì°¨ | MSE | BCE (y=1) |
|------|-----|-----------|
| 0.1  | 0.01 | 0.105 |
| 0.3  | 0.09 | 0.357 |
| 0.5  | 0.25 | 0.693 |
| 0.9  | 0.81 | 2.303 |

**ê´€ì°°:**
- MSE: ì˜¤ì°¨ê°€ ì»¤ì§€ë©´ ì œê³±ìœ¼ë¡œ ì¦ê°€
- BCE: ì˜¤ì°¨ê°€ ì»¤ì§€ë©´ ë¡œê·¸ë¡œ ê¸‰ê²©íˆ ì¦ê°€ (íŠ¹íˆ í™•ì‹ ìˆê²Œ í‹€ë¦´ ë•Œ)

---

## 5. Python ì‹œê°í™”

```python
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import rcParams

rcParams['font.family'] = 'DejaVu Sans'

# ========== ê·¸ë¦¼ 1: ì†ì‹¤ í•¨ìˆ˜ ë¹„êµ ==========
fig, axes = plt.subplots(2, 2, figsize=(14, 12))

# 1. MSE
ax1 = axes[0, 0]
y_true = 10
y_pred = np.linspace(5, 15, 100)
mse = (y_true - y_pred)**2

ax1.plot(y_pred, mse, 'b-', linewidth=2.5)
ax1.axvline(x=y_true, color='red', linestyle='--', linewidth=2, label=f'True value = {y_true}')
ax1.scatter([y_true], [0], color='red', s=100, zorder=5)
ax1.grid(True, alpha=0.3)
ax1.set_xlabel('Predicted Value', fontsize=13)
ax1.set_ylabel('MSE Loss', fontsize=13)
ax1.set_title('Mean Squared Error', fontsize=15, fontweight='bold')
ax1.legend(fontsize=11)
ax1.text(12, 15, r'$L = (y - \hat{y})^2$', fontsize=12,
        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))

# 2. MAE
ax2 = axes[0, 1]
mae = np.abs(y_true - y_pred)

ax2.plot(y_pred, mae, 'g-', linewidth=2.5)
ax2.axvline(x=y_true, color='red', linestyle='--', linewidth=2, label=f'True value = {y_true}')
ax2.scatter([y_true], [0], color='red', s=100, zorder=5)
ax2.grid(True, alpha=0.3)
ax2.set_xlabel('Predicted Value', fontsize=13)
ax2.set_ylabel('MAE Loss', fontsize=13)
ax2.set_title('Mean Absolute Error', fontsize=15, fontweight='bold')
ax2.legend(fontsize=11)
ax2.text(12, 3, r'$L = |y - \hat{y}|$', fontsize=12,
        bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8))

# 3. Binary Cross-Entropy (y=1)
ax3 = axes[1, 0]
y_true_bce = 1
y_pred_bce = np.linspace(0.01, 0.99, 100)
bce = -np.log(y_pred_bce)

ax3.plot(y_pred_bce, bce, 'r-', linewidth=2.5)
ax3.grid(True, alpha=0.3)
ax3.set_xlabel('Predicted Probability', fontsize=13)
ax3.set_ylabel('BCE Loss', fontsize=13)
ax3.set_title('Binary Cross-Entropy (y=1)', fontsize=15, fontweight='bold')
ax3.set_ylim(0, 5)
ax3.text(0.2, 3, r'$L = -\log(\hat{y})$ when $y=1$', fontsize=12,
        bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.8))
ax3.axvline(x=1, color='red', linestyle='--', alpha=0.5)

# 4. Binary Cross-Entropy (y=0)
ax4 = axes[1, 1]
y_true_bce = 0
bce_neg = -np.log(1 - y_pred_bce)

ax4.plot(y_pred_bce, bce_neg, 'purple', linewidth=2.5)
ax4.grid(True, alpha=0.3)
ax4.set_xlabel('Predicted Probability', fontsize=13)
ax4.set_ylabel('BCE Loss', fontsize=13)
ax4.set_title('Binary Cross-Entropy (y=0)', fontsize=15, fontweight='bold')
ax4.set_ylim(0, 5)
ax4.text(0.6, 3, r'$L = -\log(1-\hat{y})$ when $y=0$', fontsize=12,
        bbox=dict(boxstyle='round', facecolor='plum', alpha=0.8))
ax4.axvline(x=0, color='red', linestyle='--', alpha=0.5)

plt.tight_layout()
plt.savefig('/home/runner/work/deep-learning-study/deep-learning-study/neural_network_math_series/stage4_loss_functions.png', dpi=300, bbox_inches='tight')
print("âœ… Loss functions visualization saved!")
plt.close()

# ========== ê·¸ë¦¼ 2: ì‹¤ì œ ë°ì´í„° ì˜ˆì œ ==========
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# 1. íšŒê·€ ì˜ˆì œ
ax1 = axes[0]
np.random.seed(42)
x = np.linspace(0, 10, 50)
y_true_reg = 2 * x + 1 + np.random.randn(50) * 2
y_pred_reg = 2 * x + 1

ax1.scatter(x, y_true_reg, alpha=0.5, label='True values', s=50)
ax1.plot(x, y_pred_reg, 'r-', linewidth=2, label='Predictions')

# ì˜¤ì°¨ ì„  í‘œì‹œ
for i in [5, 15, 25, 35]:
    ax1.plot([x[i], x[i]], [y_true_reg[i], y_pred_reg[i]], 'g--', alpha=0.5, linewidth=1.5)
    
ax1.grid(True, alpha=0.3)
ax1.set_xlabel('x', fontsize=13)
ax1.set_ylabel('y', fontsize=13)
ax1.set_title('Regression with MSE Loss', fontsize=15, fontweight='bold')
ax1.legend(fontsize=11)

mse_value = np.mean((y_true_reg - y_pred_reg)**2)
ax1.text(2, 20, f'MSE = {mse_value:.2f}', fontsize=12,
        bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.8))

# 2. ë¶„ë¥˜ ì˜ˆì œ
ax2 = axes[1]
# ë°ì´í„° ìƒì„±
np.random.seed(42)
class_0 = np.random.randn(50, 2) + np.array([-2, -2])
class_1 = np.random.randn(50, 2) + np.array([2, 2])

ax2.scatter(class_0[:, 0], class_0[:, 1], c='blue', marker='o', s=50, alpha=0.6, label='Class 0')
ax2.scatter(class_1[:, 0], class_1[:, 1], c='red', marker='x', s=50, alpha=0.6, label='Class 1')

# ê²°ì • ê²½ê³„
xx = np.linspace(-5, 5, 100)
yy = xx
ax2.plot(xx, yy, 'k-', linewidth=2, label='Decision boundary')

ax2.grid(True, alpha=0.3)
ax2.set_xlabel('Feature 1', fontsize=13)
ax2.set_ylabel('Feature 2', fontsize=13)
ax2.set_title('Binary Classification with BCE Loss', fontsize=15, fontweight='bold')
ax2.legend(fontsize=11)
ax2.set_xlim(-5, 5)
ax2.set_ylim(-5, 5)

plt.tight_layout()
plt.savefig('/home/runner/work/deep-learning-study/deep-learning-study/neural_network_math_series/stage4_examples.png', dpi=300, bbox_inches='tight')
print("âœ… Examples visualization saved!")
plt.close()

print("\nğŸ‰ All Stage 4 visualizations completed successfully!")
```

### 5.1 ì‹œê°í™” ê²°ê³¼ í•´ì„¤

#### ê·¸ë¦¼ 1: ì†ì‹¤ í•¨ìˆ˜ í˜•íƒœ
1. **ì¢Œìƒë‹¨ - MSE**: í¬ë¬¼ì„  í˜•íƒœ, ì‹¤ì œê°’ì—ì„œ ë©€ì–´ì§ˆìˆ˜ë¡ ì œê³±ìœ¼ë¡œ ì¦ê°€
2. **ìš°ìƒë‹¨ - MAE**: Vì í˜•íƒœ, ì‹¤ì œê°’ì—ì„œ ë©€ì–´ì§ˆìˆ˜ë¡ ì„ í˜• ì¦ê°€
3. **ì¢Œí•˜ë‹¨ - BCE (y=1)**: ì˜ˆì¸¡ í™•ë¥ ì´ 0ì— ê°€ê¹Œìš°ë©´ ì†ì‹¤ì´ ë¬´í•œëŒ€ë¡œ
4. **ìš°í•˜ë‹¨ - BCE (y=0)**: ì˜ˆì¸¡ í™•ë¥ ì´ 1ì— ê°€ê¹Œìš°ë©´ ì†ì‹¤ì´ ë¬´í•œëŒ€ë¡œ

#### ê·¸ë¦¼ 2: ì‹¤ì œ ì‘ìš©
1. **ì¢Œì¸¡ - íšŒê·€**: ì˜ˆì¸¡ ì§ì„ ê³¼ ì‹¤ì œ ì ë“¤ ì‚¬ì´ì˜ ìˆ˜ì§ ê±°ë¦¬ê°€ ì˜¤ì°¨
2. **ìš°ì¸¡ - ë¶„ë¥˜**: ê²°ì • ê²½ê³„ë¡œ ë‘ í´ë˜ìŠ¤ë¥¼ ë¶„ë¦¬

---

## ğŸ“ í•µì‹¬ ìš”ì•½

### ì†ì‹¤ í•¨ìˆ˜ í•œëˆˆì— ë³´ê¸°

| ì†ì‹¤ í•¨ìˆ˜ | ìˆ˜ì‹ | ìš©ë„ | íŠ¹ì§• |
|----------|------|------|------|
| **MSE** | $\frac{1}{m}\sum(y-\hat{y})^2$ | íšŒê·€ | ì œê³± í˜ë„í‹° |
| **MAE** | $\frac{1}{m}\sum\|y-\hat{y}\|$ | íšŒê·€ | ì´ìƒì¹˜ì— ê°•ê±´ |
| **BCE** | $-[y\log\hat{y}+(1-y)\log(1-\hat{y})]$ | ì´ì§„ ë¶„ë¥˜ | í™•ë¥  ê¸°ë°˜ |
| **CCE** | $-\sum y_j\log\hat{y}_j$ | ë‹¤ì¤‘ ë¶„ë¥˜ | ì›-í•« ì¸ì½”ë”© |

### ì„ íƒ ê°€ì´ë“œ

**ê°„ë‹¨í•œ ê·œì¹™:**
1. **íšŒê·€ â†’ MSE** (ê¸°ë³¸)
2. **ì´ì§„ ë¶„ë¥˜ â†’ Binary Cross-Entropy**
3. **ë‹¤ì¤‘ í´ë˜ìŠ¤ â†’ Categorical Cross-Entropy**
4. **ì´ìƒì¹˜ ë§ìŒ â†’ MAE ë˜ëŠ” Huber**

### ì‹¤ìƒí™œ ë¹„ìœ 
- **MSE**: ê³¼ë… ë§ì¶”ê¸°ì—ì„œ ì¤‘ì‹¬ê¹Œì§€ì˜ ê±°ë¦¬ì˜ ì œê³±
- **MAE**: ê³¼ë… ë§ì¶”ê¸°ì—ì„œ ì¤‘ì‹¬ê¹Œì§€ì˜ ì‹¤ì œ ê±°ë¦¬
- **Cross-Entropy**: í™•ë¥  ì˜ˆë³´ì˜ ì •í™•ë„ (ë‚ ì”¨ ì˜ˆë³´)

---

## ğŸ¯ ë‹¤ìŒ ë‹¨ê³„ ì˜ˆê³ 

**Stage 5**ì—ì„œëŠ” ì†ì‹¤ì„ ìµœì†Œí™”í•˜ê¸° ìœ„í•´ í•„ìš”í•œ **ë¯¸ë¶„ê³¼ í¸ë¯¸ë¶„**ì„ ë°°ì›ë‹ˆë‹¤:
- ë¯¸ë¶„ì˜ ê¸°í•˜í•™ì  ì˜ë¯¸
- í¸ë¯¸ë¶„ê³¼ ê·¸ë˜ë””ì–¸íŠ¸
- ì—°ì‡„ ë²•ì¹™

ë¯¸ë¶„ì„ ì´í•´í•´ì•¼ ì—­ì „íŒŒ(backpropagation)ì™€ ê²½ì‚¬í•˜ê°•ë²•ì„ ì™„ì „íˆ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!

---

**ì‘ì„± ì™„ë£Œ ì‹œê°**: 2024ë…„ ê¸°ì¤€  
**ë‚œì´ë„**: â­â­â­â˜†â˜† (ì¤‘ê¸‰)  
**ì˜ˆìƒ í•™ìŠµ ì‹œê°„**: 60-75ë¶„
